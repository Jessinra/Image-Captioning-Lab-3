{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git add \"Keras.ipynb\"\n",
    "!git commit -m \"test pred using pplm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"rnn_units\": 256,\n",
    "    \"rnn_type\": \"BiLSTM\",\n",
    "    \"tokenizer\": \"BERT\",\n",
    "    \"word_embedding\": \"BERT\",\n",
    "    \"vocab_size\": 3000,\n",
    "    \"combine_strategy\": \"merge\",\n",
    "    \"combine_layer\": \"concat\",\n",
    "    \"image_context_size\": 256,\n",
    "    \"word_embedding_dim\": 256,\n",
    "    \"batch_size\": 32,\n",
    "    \"data_size\": 5000,\n",
    "    \"use_mapping\": True,\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"max_caption_length\": 25, # use <int> or None\n",
    "    \"image_feature_extractor\": \"xception\",\n",
    "    \"use_sequence\": True,\n",
    "    \"epoch\": 20,\n",
    "    \"use_mask\": True,\n",
    "    \"version\": \"v5 : positional embedding\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "\n",
    "# for TFBertModel\n",
    "PROXIES = {\n",
    "  \"http\": \"http://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "  \"https\": \"https://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"[CLS] \" + annot['caption'] + \" [SEP]\"\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "\n",
    "stopper = -1 if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]\n",
    "train_captions = all_captions[:stopper]\n",
    "train_img_paths = all_img_paths[:stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 5000\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = len(train_captions) if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature_extractor(model_type=\"xception\"):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.compat.v1.keras.applications.xception\n",
    "        cnn_model = tf.compat.v1.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"CNN encoder model not supported yet\")\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "def get_image_feature_shape(model_type):\n",
    "    \n",
    "    if model_type == \"xception\":\n",
    "        return (100, 2048)\n",
    "    elif model_type == \"inception_v3\":\n",
    "        return (64, 2048)\n",
    "    else:\n",
    "        raise Exception (\"model unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = extractor_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FEATURE_SHAPE = get_image_feature_shape(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extractor, extractor_preprocessor = get_image_feature_extractor(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image feature\n",
    "\n",
    "this step mostly skipped since the results are cached already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 157.25\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = PARAMS[\"batch_size\"]\n",
    "\n",
    "estimated_batch_count = DATA_SIZE / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique images\n",
    "# unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# # Prepare dataset\n",
    "# image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "# image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "# image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessed image (batch)\n",
    "\n",
    "# for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "#     # get context vector of batch images\n",
    "#     batch_features = extractor(batch_imgs)\n",
    "    \n",
    "#     # flatten 2D cnn result into 1D for RNN decoder input\n",
    "#     # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "#     # image_feature = 100 (Xception)\n",
    "#     # image_feature = 64 (Inception V3)\n",
    "#     batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "#     # Cache preprocessed image\n",
    "#     for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "#         image_path = image_path.numpy().decode(\"utf-8\")\n",
    "#         np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Caption dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 11:51:18.165058 140637096286016 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0330 11:51:18.166154 140637096286016 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = PARAMS[\"tokenizer\"]\n",
    "VOCAB_SIZE = PARAMS[\"vocab_size\"]  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "    \n",
    "    def use_custom_mapping(self, use_mapping=True, vocab_size=3000):\n",
    "        \n",
    "        self.use_mapping = use_mapping\n",
    "        self.cust_vocab_size = vocab_size\n",
    "        self.mapping_initialized = False\n",
    "\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into custom indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        bert_ids = [self.convert_tokens_to_ids(self.tokenize(x)) for x in tqdm(texts)]\n",
    "        \n",
    "        if not self.use_mapping:\n",
    "            return bert_ids\n",
    "        \n",
    "        if not self.mapping_initialized:\n",
    "            self._initialize_custom_mapping(bert_ids)\n",
    "            return [self._convert_bert_id_to_custom_id(x) for x in tqdm(bert_ids)]\n",
    "        \n",
    "        return bert_ids\n",
    "    \n",
    "        \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \n",
    "        bert_ids = super().convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            return self._convert_bert_id_to_custom_id(bert_ids)\n",
    "        else:\n",
    "            return bert_ids\n",
    "        \n",
    "        \n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            bert_ids = self._convert_custom_id_to_bert_id(token_ids)\n",
    "        else:\n",
    "            bert_ids = token_ids\n",
    "            \n",
    "        bert_tokens = super().convert_ids_to_tokens(bert_ids)\n",
    "        return bert_tokens\n",
    "    \n",
    "    \n",
    "    def _initialize_custom_mapping(self, corpus_bert_ids):\n",
    "        \n",
    "        print(\"    > constructing custom mapping < \\n\")\n",
    "        self._build_occurence_table(corpus_bert_ids)\n",
    "        self._build_custom_mapping_table()\n",
    "        self.mapping_initialized = True\n",
    "        \n",
    "        \n",
    "    def _build_occurence_table(self, tokenized_captions):\n",
    "        \"\"\"\n",
    "        build dict of token frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        self.occurence_table = {}\n",
    "        for caption in tqdm(tokenized_captions):\n",
    "            for token in caption:\n",
    "                if token not in self.occurence_table:\n",
    "                    self.occurence_table[token] = 0\n",
    "                self.occurence_table[token] += 1\n",
    "                \n",
    "    \n",
    "    def _build_custom_mapping_table(self):\n",
    "        \n",
    "        _special_token = ['[UNK]', '[PAD]']\n",
    "        _actual_vocab_size = self.cust_vocab_size - len(_special_token)\n",
    "        \n",
    "        sorted_occurence = {k: v for k, v in sorted(\n",
    "            self.occurence_table.items(), reverse=True, key=lambda item: item[1]\n",
    "        )}\n",
    "        \n",
    "        used_tokens = sorted(list(sorted_occurence)[:_actual_vocab_size])\n",
    "        mapping_size = min(len(used_tokens), _actual_vocab_size)\n",
    "        \n",
    "        _bert_pad = 0\n",
    "        _bert_oov = 100\n",
    "        self._custom_pad = 0\n",
    "        self._custom_oov = mapping_size + 1\n",
    "        \n",
    "        self.bert_id_to_custom_id = {\n",
    "            _bert_pad: self._custom_pad, \n",
    "            _bert_oov: self._custom_oov\n",
    "        }\n",
    "        self.custom_id_to_bert_id = {\n",
    "            self._custom_pad: _bert_pad, \n",
    "            self._custom_oov: _bert_oov\n",
    "        }\n",
    "        \n",
    "        for i in range(0, mapping_size):\n",
    "            bert_token = used_tokens[i]\n",
    "            self.bert_id_to_custom_id[bert_token] = i + 1    \n",
    "            self.custom_id_to_bert_id[i + 1] = bert_token\n",
    "            \n",
    "        print(\"Vocab contains {0} / {1} unique tokens ({2:.2f} %)\".format(\n",
    "            len(used_tokens) + 2,\\\n",
    "            len(sorted_occurence),\\\n",
    "            (len(used_tokens) / len(sorted_occurence) * 100)\n",
    "        ))\n",
    "        \n",
    "        sorted_occurence_count = list(sorted_occurence.values())\n",
    "        used_tokens_count = sum(sorted_occurence_count[:_actual_vocab_size])\n",
    "        total_tokens_count = sum(sorted_occurence_count)\n",
    "        \n",
    "        print(\"Using {0} / {1} tokens available ({2:.2f} %)\".format(\n",
    "            used_tokens_count,\\\n",
    "            total_tokens_count,\\\n",
    "            (used_tokens_count / total_tokens_count * 100)\n",
    "        ))        \n",
    "        \n",
    "    def _convert_bert_id_to_custom_id(self, token_ids):\n",
    "        return [self.bert_id_to_custom_id[x] if x in self.bert_id_to_custom_id else self._custom_oov for x in token_ids]\n",
    "    \n",
    "    def _convert_custom_id_to_bert_id(self, token_ids):\n",
    "        return [self.custom_id_to_bert_id[x] for x in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_type, use_mapping, vocab_size):\n",
    "    \n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    if tokenizer_type == \"BERT\" :\n",
    "        tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "        tokenizer.use_custom_mapping(use_mapping, vocab_size)\n",
    "\n",
    "    # use default keras tokenizer\n",
    "    else : \n",
    "        tokenizer = TokenizerWrapper(num_words=vocab_size, oov_token=\"[UNK]\")\n",
    "        tokenizer.fit_on_texts(train_captions)    \n",
    "        tokenizer.word_index['[PAD]'] = 0\n",
    "        tokenizer.index_word[0] = '[PAD]'\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 11:51:20.008800 140637096286016 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0330 11:51:21.265700 140637096286016 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "caption_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=False,\n",
    "    vocab_size=0\n",
    ")\n",
    "\n",
    "target_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=PARAMS[\"use_mapping\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4107.91it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4224.81it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 390182.33it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 378438.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > constructing custom mapping < \n",
      "\n",
      "Vocab contains 3000 / 3584 unique tokens (83.65 %)\n",
      "Using 68179 / 68765 tokens available (99.15 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "caption_tokens = caption_tokenizer.texts_to_sequences(train_captions)\n",
    "target_tokens = target_tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parallel dataset\n",
    "\n",
    "prepare captions and target to support parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parallel_dataset(img_paths, caption_tokens, target_tokens):\n",
    "    \n",
    "    dataset_img_paths = []\n",
    "    dataset_captions = []\n",
    "    dataset_target = []\n",
    "    dataset_target_position = []\n",
    "\n",
    "    for i in tqdm(range(0, len(train_img_paths))):\n",
    "        img = img_paths[i]\n",
    "        cap = caption_tokens[i]\n",
    "        tar = target_tokens[i]\n",
    "\n",
    "        for j in range(1, len(cap)):\n",
    "            dataset_img_paths.append(img)\n",
    "            dataset_captions.append(cap[:j])\n",
    "            dataset_target.append(tar[j])\n",
    "            dataset_target_position.append(j)\n",
    "\n",
    "    dataset_captions = pad_sequences(dataset_captions, maxlen=PARAMS[\"max_caption_length\"], padding='post')\n",
    "    \n",
    "    return dataset_img_paths, dataset_captions, dataset_target, dataset_target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 25406.78it/s]\n"
     ]
    }
   ],
   "source": [
    "parallel_img_paths, parallel_captions, parallel_target, parallel_target_position = preprocess_parallel_dataset(train_img_paths, caption_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = parallel_captions.shape[0]\n",
    "MAX_CAPTION_LENGTH = parallel_captions.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_target_position = tf.one_hot(parallel_target_position, depth=MAX_CAPTION_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "\n",
    "def load_dataset(img_name, caption, target, target_pos):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, caption, target, target_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((parallel_img_paths, parallel_captions, parallel_target, parallel_target_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "\n",
    "dataset = dataset.map(lambda item1, item2, item3, item4: tf.numpy_function(\n",
    "          load_dataset, [item1, item2, item3, item4], [tf.float32, tf.int32, tf.int32, tf.float32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(DATA_SIZE / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 16, image_feature = 100, 2048)\n",
    "# caption => (batch_size = 16, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1395 batches, (total : 44640)\n",
      "eval : 298 batches, (total : 9536)\n",
      "test : 300 batches, (total : 9600 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captioning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=PARAMS[\"learning_rate\"])\n",
    "# loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    real  => (batch_size,)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate loss\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # create mask to filter out padding token \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    # Ignore loss_ if real token is padding\n",
    "    loss_ *= mask\n",
    "    \n",
    "    # Get mean of curren batch's loss (somewhat batch norm)\n",
    "    result_loss = tf.reduce_mean(loss_)\n",
    "    \n",
    "    return result_loss\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    loss_  => (batch_size, 1)\n",
    "    mask   => (batch_size, 1)  : indicate is padding or not\n",
    "\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim)\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # x => (batch_size, 100, 2048)\n",
    "        x = self.fc(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, image_feature_size, image_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, img_feature_size, image_context_size)\n",
    "        hidden                        => (batch_size, embedding_size)\n",
    "        \n",
    "        note : \n",
    "        img_feature_size ==  64 for Inception V3,\n",
    "        img_feature_size == 100 for Xception,\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        _w1 = self.W1(features)\n",
    "        _w2 = self.W2(hidden_with_time_axis)\n",
    "        score = tf.nn.tanh(_w1 + _w2)\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_with_time_axis      => (batch_size, 1, embedding_size)\n",
    "        score                      => (batch_size, img_feature_size, units)\n",
    "        attention_weights          => (batch_size, img_feature_size, 1)\n",
    "        context_vector (after sum) => (batch_size, img_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=256, \n",
    "                 embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 combine_strategy=\"merge\", combine_layer=\"concat\",\n",
    "                 vocab_size=3000, batch_size=32):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # when to use context_vector [\"inject_init\", \"inject_pre\", \"inject_par\", \"merge\"]\n",
    "        self.combine_strategy = combine_strategy\n",
    "        \n",
    "        # how to use context_vector [\"add\", \"concat\"]\n",
    "        self.combine_layer = combine_layer\n",
    "        \n",
    "        # =====================================================\n",
    "        \n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "        \n",
    "#         self.mask = Masking(mask_value=0, dtype=\"int32\")\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "        self.positional_embedding = Embedding(\n",
    "            input_dim=MAX_CAPTION_LENGTH,\n",
    "            output_dim=16,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "        \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\") # same size as vocab\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "           \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\") # same size as vocab\n",
    "        \n",
    "        \n",
    "    def _init_embedding(self):\n",
    "        \n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.bert_embedding = TFBertModel.from_pretrained('bert-base-uncased', proxies=PROXIES)\n",
    "            self.bert_embedding.trainable = False\n",
    "            self.embedding_dim = self.bert_embedding.config.hidden_size\n",
    "            \n",
    "        else:\n",
    "            self.default_embedding = Embedding(\n",
    "                input_dim=self.vocab_size, \n",
    "                output_dim=self.embedding_dim, \n",
    "                mask_zero=True,\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def _init_rnn(self):\n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector'\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            self.bilstm = Bidirectional(LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        \n",
    "    def embedding(self, tokens, as_sentence=False):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embedding_type == \"BERT\": \n",
    "            embedding, sentence_embedding = self._bert_embedding(tokens)\n",
    "        \n",
    "        else:\n",
    "            embedding = self._default_embedding(tokens)\n",
    "            sentence_embedding = tf.reduce_mean(embedding, 1)\n",
    "            \n",
    "\n",
    "        if as_sentence:\n",
    "            # embedding => (batch_size, embedding_dim)\n",
    "            return sentence_embedding\n",
    "        \n",
    "        else:\n",
    "            # embedding => (batch_size, tokens_length, embedding_dim)\n",
    "            return embedding\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        embedding (tokens)   => (batch_size, tokens_length, embedding_dim)\n",
    "        embedding (sentence) => (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def _bert_embedding(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        # mask out attention from padding\n",
    "        # mask => (batch_size, sequence_length)\n",
    "        attention_mask = x == 0\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, attention_mask=attention_mask)\n",
    "        \"\"\"\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        token_type_ids = tf.cast((x == 0), tf.int32)\n",
    "        \n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, token_type_ids=token_type_ids)\n",
    "        return hidden_states, sentence_embedding\n",
    "    \n",
    "        \"\"\"\n",
    "        hidden states contains hidden state for each word\n",
    "        sentence_embedding is general embedding for whole sentences\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def _default_embedding(self, x):\n",
    "        \n",
    "        # embedding => (batch_size, sequence_length, embedding_size)\n",
    "        embedding = self.default_embedding(x)\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "    def apply_strategy(self, x, context_vector, curr_iter=-1):\n",
    "        \"\"\"\n",
    "        context_vector : image's vector\n",
    "        x              : rnn input (word embedding)\n",
    "        strategy       : \n",
    "        curr_iter      : current iteration number\n",
    "        \n",
    "        No longer support inject_init & inject_pre, since training become fully parallel\n",
    "        \"\"\"\n",
    "        \n",
    "#         if self.combine_strategy == \"inject_init\":\n",
    "#             initial_state = tf.squeeze(context_vector) if curr_iter == 1 else None\n",
    "#             output, state = self.rnn_model(x, initial_state=initial_state)  \n",
    "            \n",
    "#         elif self.combine_strategy == \"inject_pre\":\n",
    "#             x = context_vector if curr_iter == 1 else x\n",
    "#             output, state = self.rnn_model(x)  \n",
    "            \n",
    "        if self.combine_strategy == \"inject_par\":\n",
    "            \n",
    "            context_vector = tf.expand_dims(context_vector, 1)\n",
    "            context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "                        \n",
    "            x = self.custom_combine_layer(context_vector, x)\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output, state = self.rnn_model(x)              \n",
    "            \n",
    "        else: # merge (as default)\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output, state = self.rnn_model(x) \n",
    "            \n",
    "            context_vector = tf.expand_dims(context_vector, 1)\n",
    "            context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output = self.custom_combine_layer(context_vector, output)\n",
    "            \n",
    "        return output, state\n",
    "    \n",
    "    \n",
    "    def rnn_model(self, x, initial_state=None):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            \n",
    "            # adjust initial state, LSTM has 2 hidden states (h and c)\n",
    "            if initial_state is not None:\n",
    "                init_h = initial_state\n",
    "                init_c = tf.zeros(initial_state.shape)\n",
    "                initial_state = [init_h, init_c]\n",
    "            \n",
    "            output, h_state, c_state = self.lstm(x, initial_state=initial_state)\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            output, _, h_state, _, _ = self.bilstm(x)\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, h_state = self.gru(x, initial_state=initial_state)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, h_state\n",
    "    \n",
    "        \"\"\"\n",
    "        output => (batch_size, rnn_size)\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def custom_combine_layer(self, x, y):\n",
    "        if self.combine_layer == \"add\":\n",
    "            return self._add_layer(x, y)\n",
    "        else:\n",
    "            return self._concat_layer(x, y)\n",
    "\n",
    "        \n",
    "    def _add_layer(self, x, y):\n",
    "        \n",
    "        if x.shape[1] != y.shape[1] :\n",
    "            exception = \"Cannot combine using 'add' strategy, both tensor has different shape {} & {}\"\n",
    "            raise Exception(exception.format(x.shape, y.shape))\n",
    "            \n",
    "        return tf.keras.layers.add([x, y])\n",
    "            \n",
    "        \n",
    "    def _concat_layer(self, x, y):\n",
    "        return tf.concat([x, y], axis=-1)\n",
    "        \n",
    "    \n",
    "    def call(self, decoder_input, context_vector, position):\n",
    "        \"\"\" \n",
    "        decoder_input  : last predicted word => (batch_size, sequence len)\n",
    "        context_vector : image's vector      => (batch_size, img_context_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Mask out padding (0)\n",
    "#         decoder_input = self.mask(decoder_input)\n",
    "        \n",
    "        # x1 => (batch_size, input_sentence_len, embedding_dim + positional_dim)\n",
    "        x1 = self.embedding(decoder_input)\n",
    "#         x1_1 = self.embedding(decoder_input)\n",
    "#         x1_2 = self.positional_embedding(position)\n",
    "        \n",
    "#         x1_2 = tf.reduce_mean(x1_2, axis=1)\n",
    "#         x1_2 = tf.expand_dims(x1_2, axis=1)\n",
    "#         x1_2 = tf.tile(x1_2, multiples=[1, x1_1.shape[1], 1])\n",
    "        \n",
    "#         x1 = tf.concat([x1_1, x1_2], axis=-1)\n",
    "        \n",
    "        # x2 (concat) => (batch_size, input_sentence_len, embedding_dim + image_context_size)\n",
    "        # x2 (add) => (batch_size, embedding_dim)\n",
    "        x2, rnn_state = self.apply_strategy(x1, context_vector)\n",
    "        \n",
    "        ## ============================================\n",
    "        ## TODO: add another attention layer ? \n",
    "        ## ============================================\n",
    "        \n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        # x3 => (batch_size, sequence_len, rnn_units)\n",
    "        x3 = self.fc1(x2)   # how important is every sequence \n",
    "        x3 = self.batchnorm(x3)\n",
    "        x3 = self.leakyrelu(x3)\n",
    "        \n",
    "        # x4 => (batch_size, sequence_len * rnn_units) || was (batch_size * sequence_len=1, rnn_units)\n",
    "        x4 = tf.reshape(x3, (x3.shape[0], -1))\n",
    "\n",
    "        x4 = self.dropout(x4)\n",
    "        \n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x4)\n",
    "        \n",
    "        return word_predictions, rnn_state\n",
    "    \n",
    "\n",
    "    def reset_state(self, batch_size=None):\n",
    "        \n",
    "        if batch_size is not None:\n",
    "            return tf.zeros((batch_size, self.rnn_units))\n",
    "        \n",
    "        return tf.zeros((self.batch_size, self.rnn_units))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rnn_units': 256,\n",
       " 'rnn_type': 'BiLSTM',\n",
       " 'tokenizer': 'BERT',\n",
       " 'word_embedding': 'BERT',\n",
       " 'vocab_size': 3000,\n",
       " 'combine_strategy': 'merge',\n",
       " 'combine_layer': 'concat',\n",
       " 'image_context_size': 256,\n",
       " 'word_embedding_dim': 256,\n",
       " 'batch_size': 32,\n",
       " 'data_size': 5000,\n",
       " 'use_mapping': True,\n",
       " 'learning_rate': 0.0003,\n",
       " 'max_caption_length': 25,\n",
       " 'image_feature_extractor': 'xception',\n",
       " 'use_sequence': True,\n",
       " 'epoch': 20,\n",
       " 'use_mask': True,\n",
       " 'version': 'v5 : positional embedding'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 11:51:27.215607 140637096286016 configuration_utils.py:256] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/m13516112/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0330 11:51:27.217345 140637096286016 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0330 11:51:28.399942 140637096286016 modeling_tf_utils.py:333] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5 from cache at /home/m13516112/.cache/torch/transformers/d667df51ec24c20190f01fb4c20a21debc4c4fc12f7e2f5441ac0a99690e3ee9.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5\n",
      "I0330 11:51:29.637956 140637096286016 modeling_tf_utils.py:375] Layers from pretrained model not used in TFBertModel: ['nsp___cls', 'mlm___cls']\n"
     ]
    }
   ],
   "source": [
    "encoder = CNN_Encoder(\n",
    "    output_dim=PARAMS[\"image_context_size\"]\n",
    ")\n",
    "\n",
    "attention = BahdanauAttention(\n",
    "    units=PARAMS[\"rnn_units\"]\n",
    ")\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    rnn_type=PARAMS[\"rnn_type\"], \n",
    "    rnn_units=PARAMS[\"rnn_units\"],\n",
    "    embedding_type=PARAMS[\"word_embedding\"], \n",
    "    embedding_dim=PARAMS[\"word_embedding_dim\"],  \n",
    "    combine_strategy=PARAMS[\"combine_strategy\"], \n",
    "    combine_layer=PARAMS[\"combine_layer\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    "    batch_size=PARAMS[\"batch_size\"]\n",
    ")\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Requirements\n",
    "\n",
    "# combine_strategy = \"inject_init\" : IMAGE_CONTEXT_SIZE == UNITS\n",
    "# combine_strategy = \"inject_pre\"  : IMAGE_CONTEXT_SIZE == WORD_EMBEDDING_DIM\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default feed forward function\n",
    "\n",
    "@tf.function\n",
    "def feed_forward(img_tensor, caption, position):\n",
    "    \"\"\"\n",
    "    img_tensor => (batch_size, image_feature_size, 2048)\n",
    "    caption => (batch_size, max_caption_length)\n",
    "    \"\"\"\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    # note : used to be decoder hidden state, \n",
    "    # but changed into current word embedding for paralel training\n",
    "    \n",
    "    # hidden => (batch_size, embedding_size)\n",
    "    hidden = decoder.embedding(caption, as_sentence=True)\n",
    "\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = encoder(img_tensor)\n",
    "\n",
    "    # context_vector => (batch_size, image_context_size)\n",
    "    context_vector, _ = attention(features, hidden)\n",
    "\n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions, _ = decoder(caption, context_vector, position)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, caption, target, position):\n",
    "    \"\"\"\n",
    "    target => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training model\n",
    "    with tf.GradientTape() as gradient_tape:\n",
    "        \n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions = feed_forward(img_tensor, caption, position)\n",
    "        \n",
    "        # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "        loss = loss_function(target, predictions)\n",
    "\n",
    "        \n",
    "    # Apply gradient\n",
    "    trainable_variables = encoder.trainable_variables + \\\n",
    "                          decoder.trainable_variables + \\\n",
    "                          attention.trainable_variables\n",
    "    \n",
    "    gradients = gradient_tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def eval_step(img_tensor, caption, target):\n",
    "    \n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions = feed_forward(img_tensor, caption)\n",
    "    \n",
    "    # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "    loss = loss_function(target, predictions)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def choose_predicted_id(predictions, strategy=\"max\", sampling_k=10):\n",
    "    \"\"\"\n",
    "    predictions : encoder word prediction => (batch_size, vocab_size)\n",
    "    strategy    : how to choose word [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sampling method (categorical dist)\n",
    "    if strategy == \"sample\":\n",
    "        \n",
    "        # sampled_proba & sampled_ids => (batch_size, sampling_k)\n",
    "        sampled_proba, sampled_ids = tf.math.top_k(predictions, sampling_k)\n",
    "        \n",
    "        # chosen_sampled_col => (batch_size, )\n",
    "        chosen_sampled_col = tf.squeeze(tf.random.categorical(sampled_proba, 1))\n",
    "        \n",
    "        # create row idx to zip with chosen_sampled_col\n",
    "        row_idx = tf.range(predictions.shape[0], dtype=chosen_sampled_col.dtype)\n",
    "        row_col_idx = tf.stack([row_idx, chosen_sampled_col], axis=1)\n",
    "        \n",
    "        # predicted_ids => (batch_size, )\n",
    "        predicted_ids = tf.gather_nd(sampled_ids, row_col_idx)\n",
    "\n",
    "    # Max index method\n",
    "    else:\n",
    "        predicted_ids = tf.argmax(predictions, 1)\n",
    "    \n",
    "    # predicted_ids => (batch_size, )\n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(images_paths):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [load_image(x)[0] for x in images_paths]\n",
    "    \n",
    "    # x => (batch_size, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, 10, 10, 2048)\n",
    "    x = extractor(x)\n",
    "    \n",
    "    # x  => (batch_size, img_feature_size, 2048)\n",
    "    x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = encoder(x)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def get_supporting_features(images_paths, strategy=\"mean\"):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, img_count, 1)\n",
    "    strategy : strategy to aggregate multiple supporting image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [[load_image(x)[0] for x in images_set] for images_set in images_paths]\n",
    "    \n",
    "    # x => (batch_size, img_count, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, img_count, 10, 10, 2048)\n",
    "    x = [extractor(image_set) for image_set in x]\n",
    "    \n",
    "    # features => (batch_size, img_count, img_feature_size, image_context_size)\n",
    "    features = encoder(x)\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    if strategy == \"logsumexp\":\n",
    "        features = tf.reduce_logsumexp(features, 1)\n",
    "    elif strategy == \"max\":\n",
    "        features = tf.reduce_max(features, 1)\n",
    "    elif strategy == \"min\":\n",
    "        features = tf.reduce_min(features, 1)\n",
    "    else:\n",
    "        features = tf.reduce_mean(features, 1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_indices(support_text):\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(0, len(support_text)):\n",
    "        context_token = target_tokenizer.tokenize(support_text[i])\n",
    "        context_token_id = target_tokenizer.convert_tokens_to_ids(context_token)\n",
    "        context_token_id = set(context_token_id)\n",
    "        context_token_id.discard(0)\n",
    "        for x in sorted(context_token_id):\n",
    "            indices.append([i, x])\n",
    "\n",
    "    # return => (word_count, 2)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_supporting_text_vector(support_text, vocab_size):\n",
    "    \"\"\"\n",
    "    support_text : list of text describing main image context => (batch_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(support_text)\n",
    "    \n",
    "    # indices => ( sum(batch_size * ?word_count), 2)\n",
    "    indices = get_one_hot_indices(support_text)\n",
    "    values = tf.ones(len(indices))\n",
    "    sparse_one_hot = tf.sparse.SparseTensor(indices, values, dense_shape=[batch_size, vocab_size])\n",
    "    \n",
    "    # sparse_one_hot => (batch_size, vocab_size)\n",
    "    return sparse_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluate(images_paths,\n",
    "                    support_text=None,\n",
    "                    support_imgs=None, \n",
    "                    support_aggregate_strategy=\"mean\",\n",
    "                    pplm_iteration=3,\n",
    "                    pplm_weight=0.03,\n",
    "                    pplm_gm_weight=0.8,\n",
    "                    choose_word_strategy=\"sample\",\n",
    "                   ):\n",
    "    \n",
    "    \"\"\"\n",
    "    images_paths : list of image_path                           => (batch_size, 1)\n",
    "    support_text : list of text describing main image context   => (batch_size)\n",
    "    support_imgs : list of list of image_path                   => (batch_size, image_count, 1)\n",
    "    support_aggregate_strategy : how to aggregate support image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    pplm_iteration : number of pplm step done for every decoding step\n",
    "    pplm_weight    : weight of pplm loss\n",
    "    pplm_gm_weight : geometric mean fusion weight (0 means use only original prediction, 1 means use only pplm prediction)\n",
    "    choose_word_strategy : how to choose word from prediction distribution [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(images_paths)\n",
    "    \n",
    "    # initialize captions placeholder\n",
    "    start_token = caption_tokenizer.convert_tokens_to_ids(['[CLS]']) # use bert id's not custom id\n",
    "    result_captions = tf.tile(tf.expand_dims(start_token, 1), [batch_size, 1])\n",
    "    attention_plot = tf.reshape([], shape=(batch_size, 0, IMAGE_FEATURE_SHAPE[0]))\n",
    "    \n",
    "    # Extract features from main images\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = get_image_features(images_paths)\n",
    "    \n",
    "    \n",
    "    if support_text is not None:\n",
    "        # support_text_vector => (batch_size, vocab_size)\n",
    "        support_text_vector = get_supporting_text_vector(support_text, decoder.vocab_size) \n",
    "    else:\n",
    "        # set all pplm related variable to 0\n",
    "        pplm_iteration = 0\n",
    "        pplm_weight = 0\n",
    "        pplm_gm_weight = 0\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(MAX_CAPTION_LENGTH)):\n",
    "        \n",
    "        # decoder_input => (batch_size, ~MAX_CAPTION_LENGTH)\n",
    "        caption = pad_sequences(result_captions, maxlen=MAX_CAPTION_LENGTH, padding=\"post\")\n",
    "        \n",
    "        # hidden => (batch_size, embedding_size)\n",
    "        hidden = decoder.embedding(caption, as_sentence=True)  ## BOTTLE\n",
    "\n",
    "        # context_vector => (batch_size, image_context_size)\n",
    "        # attention_weights => (batch_size, img_feature_size, 1)\n",
    "        context_vector, attention_weights = attention(features, hidden)\n",
    "\n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions, _ = decoder(caption, context_vector, position=None)  \n",
    "        \n",
    "        # ======================== PPLM section ========================\n",
    "        ori_prediction = predictions\n",
    "        \n",
    "        curr_pertubation = tf.Variable(tf.zeros((batch_size, decoder.embedding_dim)), name=\"curr_pertubation\", trainable=True)\n",
    "        \n",
    "        \n",
    "        for j in range(pplm_iteration):\n",
    "            \n",
    "            with tf.GradientTape() as pplm_tape: \n",
    "                \n",
    "                hidden += curr_pertubation\n",
    "                \n",
    "                context_vector, attention_weights = attention(features, hidden)\n",
    "                predictions, _ = decoder(caption, context_vector)\n",
    "                pplm_loss = pplm_loss_function(support_text_vector, predictions, pplm_weight=pplm_weight)\n",
    "                \n",
    "            \"\"\"\n",
    "            most impactfull layer to train = last dense layer\n",
    "            \"\"\"\n",
    "            \n",
    "            trainable_variables = [curr_pertubation]\n",
    "            gradients = pplm_tape.gradient(pplm_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "            \n",
    "            predictions, _ = decoder(caption, context_vector)\n",
    "        \n",
    "        # fuse final pplm_prediction and original prediction\n",
    "        fused_predictions = (predictions * pplm_gm_weight) + (ori_prediction * (1 - pplm_gm_weight)) \n",
    "        \n",
    "        # predicted_ids => (batch_size,)\n",
    "        predicted_ids = choose_predicted_id(fused_predictions, strategy=choose_word_strategy)\n",
    "        \n",
    "        # convert custom id mapping to bert's id\n",
    "        # predicted_bert_ids => (batch_size, 1)\n",
    "        predicted_bert_ids = target_tokenizer._convert_custom_id_to_bert_id(predicted_ids.numpy())\n",
    "        predicted_bert_ids = tf.expand_dims(predicted_bert_ids, 1)\n",
    "        \n",
    "        # store result\n",
    "        result_captions = tf.concat([result_captions, predicted_bert_ids], axis=1)\n",
    "        \n",
    "        # attention_weights => (batch_size, 1, img_feature_size)\n",
    "        attention_weights = tf.reshape(attention_weights, shape=(batch_size, 1, -1))\n",
    "        \n",
    "        # assign attention weights to respective generated word\n",
    "        # attention_plot => (batch_size, ~max_caption_len, feature_size)\n",
    "        attention_plot = tf.concat([attention_plot, attention_weights], axis=1)\n",
    "        \n",
    "    # remove start token & revert to tokens\n",
    "    result_captions = [caption_tokenizer.convert_ids_to_tokens(x[1:]) for x in result_captions]\n",
    "    \n",
    "    return result_captions, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy, MeanSquaredError\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def pplm_loss_function(real, pred, pplm_weight=0.03):\n",
    "    \"\"\"\n",
    "    real  => (batch_size, vocab_size)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "\n",
    "#     mm = tf.sparse.sparse_dense_matmul(real, tf.transpose(pred))\n",
    "#     mm = tf.reduce_sum(tf.abs(mm), 1)\n",
    "#     loss = tf.reduce_sum(mm, 0)\n",
    "    \n",
    "    real = tf.sparse.to_dense(real, default_value=0)\n",
    "    pplm_loss = CategoricalCrossentropy(from_logits=True)\n",
    "    loss = pplm_loss(real, pred, pplm_weight)\n",
    "    \n",
    "#     print(loss)\n",
    "    return loss\n",
    "\n",
    "    \"\"\"\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1395it [02:48,  8.27it/s]\n",
      "1395it [02:33,  9.11it/s]\n",
      "1395it [02:24,  9.68it/s]\n",
      "1395it [02:26,  9.52it/s]\n",
      "1395it [02:25,  9.62it/s]\n",
      "1395it [02:48,  8.30it/s]\n",
      "1395it [02:39,  8.74it/s]\n",
      "1395it [04:14,  5.47it/s]\n",
      "1395it [04:12,  5.52it/s]\n",
      "1395it [04:14,  5.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe4bfb5cfd0>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    for (img_tensor, captions, target, target_position) in tqdm(train_dataset):\n",
    "        batch_loss = train_step(img_tensor, captions, target, target_position)\n",
    "        loss.append(batch_loss.numpy())\n",
    "        \n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe5947d67b8>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.055691041585803"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-5000:]) / len(loss[-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(loss[-500:]) / len(loss[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:00<00:00, 505015.31it/s]\n"
     ]
    }
   ],
   "source": [
    "true_captions = {}\n",
    "for annot in tqdm(annotations[\"annotations\"]):\n",
    "    image_id = annot[\"image_id\"]\n",
    "\n",
    "    if image_id not in true_captions:\n",
    "        true_captions[image_id] = []\n",
    "    \n",
    "    true_captions[image_id].append(annot['caption'])\n",
    "    \n",
    "    \n",
    "def get_true_captions(image_paths):\n",
    "    return [true_captions[int(x[-10:-4])] for x in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:13<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "person has their green [UNK] to the middle to it bright ball ball ball and\n",
      "\n",
      "true caps : \n",
      "some people are standing near some public toilets\n",
      "A group of people are standing out in front of portable toilets.\n",
      "people are standing around some porta potties and mess\n",
      "a couple of people stand in front of some toilets \n",
      "groups of people standing around the toilet area\n",
      "=====================================\n",
      "predicted : \n",
      "two long horn up in two street with [UNK] , white , refrigerator in it\n",
      "\n",
      "true caps : \n",
      "A person is holding onto a cellphone somewhere.\n",
      "there is someone holding a small phone with a small screen\n",
      "A picture of a cell phone showing the time as 8:56 PM.\n",
      "a hand is holding a black and silver cellphone\n",
      "This is a verizon LG Envy 2 cell phone.\n",
      "=====================================\n",
      "predicted : \n",
      "a woman on people background with her desk\n",
      "\n",
      "true caps : \n",
      "People working on laptop computers in a student dining room\n",
      "People sitting in a room at tables with laptops \n",
      "A group of people that are sitting in front of laptops.\n",
      "Many people have signed up for the computer seminar.\n",
      "A group of people at tables with laptops having conversation.\n",
      "=====================================\n",
      "predicted : \n",
      "one three [UNK]s birds low the grass . are the young young with ball dressed together a person and young empty television empty in\n",
      "\n",
      "true caps : \n",
      "two birds near one another in a field \n",
      "Two black colored birds sitting in a field together. \n",
      "Two birds that are sitting on the ground.\n",
      "2 birds standing on the ground in a field \n",
      "Two birds that are standing next to each other on the grass. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[5:9]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images)\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:12<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "a couple of people standing around with frisbees .\n",
      "\n",
      "true caps : \n",
      "some people are standing near some public toilets\n",
      "A group of people are standing out in front of portable toilets.\n",
      "people are standing around some porta potties and mess\n",
      "a couple of people stand in front of some toilets \n",
      "groups of people standing around the toilet area\n",
      "=====================================\n",
      "predicted : \n",
      "someone holding a cell pone phone on the phone phone phone and phone\n",
      "\n",
      "true caps : \n",
      "A person is holding onto a cellphone somewhere.\n",
      "there is someone holding a small phone with a small screen\n",
      "A picture of a cell phone showing the time as 8:56 PM.\n",
      "a hand is holding a black and silver cellphone\n",
      "This is a verizon LG Envy 2 cell phone.\n",
      "=====================================\n",
      "predicted : \n",
      "a group of people sitting at chairs with laptops .\n",
      "\n",
      "true caps : \n",
      "People working on laptop computers in a student dining room\n",
      "People sitting in a room at tables with laptops \n",
      "A group of people that are sitting in front of laptops.\n",
      "Many people have signed up for the computer seminar.\n",
      "A group of people at tables with laptops having conversation.\n",
      "=====================================\n",
      "predicted : \n",
      "a couple of black birds that are eating a tree .\n",
      "\n",
      "true caps : \n",
      "two birds near one another in a field \n",
      "Two black colored birds sitting in a field together. \n",
      "Two birds that are sitting on the ground.\n",
      "2 birds standing on the ground in a field \n",
      "Two birds that are standing next to each other on the grass. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[5:9]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_strategy=\"max\")\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "stop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-5981adc29e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: stop"
     ]
    }
   ],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train/{}\".format(str(datetime.now())[:-10])\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           attention=attention,\n",
    "                           optimizer=optimizer\n",
    "                          )\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------\")\n",
    "print(checkpoint_path)\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(checkpoint_path)\n",
    "with open(checkpoint_path + \"/config.txt\", \"w\") as f:\n",
    "    f.write(str(PARAMS))\n",
    "    \n",
    "log_file = open(checkpoint_path + \"/log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, PARAMS[\"epoch\"]):\n",
    "    \n",
    "    start = time.time()\n",
    "    loss = 0\n",
    "    batch = 1\n",
    "\n",
    "    for img_tensor, captions, target in tqdm(train_dataset):\n",
    "        \n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "\n",
    "            with open(checkpoint_path + \"/log.txt\", \"a\") as f:\n",
    "                log_message = ' {} Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                    checkpoint_path, epoch + 1, batch, batch_loss.numpy())\n",
    "                f.write(str(log_message + \"\\n\"))\n",
    "                print(log_message)   \n",
    "\n",
    "        batch += 1\n",
    "        \n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(batch_loss.numpy())\n",
    "\n",
    "#     ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = []\n",
    "\n",
    "# for (img_tensor, captions, target) in tqdm(train_dataset):\n",
    "#     batch_loss = train_step(img_tensor, captions, target)\n",
    "#     loss.append(batch_loss.numpy())\n",
    "    \n",
    "# plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PPLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "# text = [\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "# ]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images, support_text=text, pplm_iteration=5, pplm_weight=1)\n",
    "# result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
