{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git add \"Keras.ipynb\"\n",
    "!git commit -m \"test pred using pplm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"rnn_units\": 256,\n",
    "    \"rnn_type\": \"BiLSTM\",\n",
    "    \"tokenizer\": \"BERT\",\n",
    "    \"word_embedding\": \"BERT\",\n",
    "    \"vocab_size\": 3000,\n",
    "    \"combine_strategy\": \"merge\",\n",
    "    \"combine_layer\": \"concat\",\n",
    "    \"image_context_size\": 256,\n",
    "    \"word_embedding_dim\": 256,\n",
    "    \"batch_size\": 32,\n",
    "    \"data_size\": 5000,\n",
    "    \"use_mapping\": True,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_caption_length\": 25, # use <int> or None\n",
    "    \"image_feature_extractor\": \"xception\",\n",
    "    \"use_sequence\": True,\n",
    "    \"epoch\": 20,\n",
    "    \"use_mask\": True,\n",
    "    \"version\": \"v5 : positional embedding\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "\n",
    "# for TFBertModel\n",
    "PROXIES = {\n",
    "  \"http\": \"http://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "  \"https\": \"https://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"[CLS] \" + annot['caption'] + \" [SEP]\"\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "\n",
    "stopper = -1 if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]\n",
    "train_captions = all_captions[:stopper]\n",
    "train_img_paths = all_img_paths[:stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 5000\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = len(train_captions) if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature_extractor(model_type=\"xception\"):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.compat.v1.keras.applications.xception\n",
    "        cnn_model = tf.compat.v1.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"CNN encoder model not supported yet\")\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "def get_image_feature_shape(model_type):\n",
    "    \n",
    "    if model_type == \"xception\":\n",
    "        return (100, 2048)\n",
    "    elif model_type == \"inception_v3\":\n",
    "        return (64, 2048)\n",
    "    else:\n",
    "        raise Exception (\"model unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = extractor_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FEATURE_SHAPE = get_image_feature_shape(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extractor, extractor_preprocessor = get_image_feature_extractor(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image feature\n",
    "\n",
    "this step mostly skipped since the results are cached already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 157.25\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = PARAMS[\"batch_size\"]\n",
    "\n",
    "estimated_batch_count = DATA_SIZE / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique images\n",
    "# unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# # Prepare dataset\n",
    "# image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "# image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "# image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessed image (batch)\n",
    "\n",
    "# for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "#     # get context vector of batch images\n",
    "#     batch_features = extractor(batch_imgs)\n",
    "    \n",
    "#     # flatten 2D cnn result into 1D for RNN decoder input\n",
    "#     # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "#     # image_feature = 100 (Xception)\n",
    "#     # image_feature = 64 (Inception V3)\n",
    "#     batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "#     # Cache preprocessed image\n",
    "#     for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "#         image_path = image_path.numpy().decode(\"utf-8\")\n",
    "#         np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Caption dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 16:20:05.144181 139869419927360 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0330 16:20:05.145640 139869419927360 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = PARAMS[\"tokenizer\"]\n",
    "VOCAB_SIZE = PARAMS[\"vocab_size\"]  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "    \n",
    "    def use_custom_mapping(self, use_mapping=True, vocab_size=3000):\n",
    "        \n",
    "        self.use_mapping = use_mapping\n",
    "        self.cust_vocab_size = vocab_size\n",
    "        self.mapping_initialized = False\n",
    "\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into custom indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        bert_ids = [self.convert_tokens_to_ids(self.tokenize(x)) for x in tqdm(texts)]\n",
    "        \n",
    "        if not self.use_mapping:\n",
    "            return bert_ids\n",
    "        \n",
    "        if not self.mapping_initialized:\n",
    "            self._initialize_custom_mapping(bert_ids)\n",
    "            return [self._convert_bert_id_to_custom_id(x) for x in tqdm(bert_ids)]\n",
    "        \n",
    "        return bert_ids\n",
    "    \n",
    "        \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \n",
    "        bert_ids = super().convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            return self._convert_bert_id_to_custom_id(bert_ids)\n",
    "        else:\n",
    "            return bert_ids\n",
    "        \n",
    "        \n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            bert_ids = self._convert_custom_id_to_bert_id(token_ids)\n",
    "        else:\n",
    "            bert_ids = token_ids\n",
    "            \n",
    "        bert_tokens = super().convert_ids_to_tokens(bert_ids)\n",
    "        return bert_tokens\n",
    "    \n",
    "    \n",
    "    def _initialize_custom_mapping(self, corpus_bert_ids):\n",
    "        \n",
    "        print(\"    > constructing custom mapping < \\n\")\n",
    "        self._build_occurence_table(corpus_bert_ids)\n",
    "        self._build_custom_mapping_table()\n",
    "        self.mapping_initialized = True\n",
    "        \n",
    "        \n",
    "    def _build_occurence_table(self, tokenized_captions):\n",
    "        \"\"\"\n",
    "        build dict of token frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        self.occurence_table = {}\n",
    "        for caption in tqdm(tokenized_captions):\n",
    "            for token in caption:\n",
    "                if token not in self.occurence_table:\n",
    "                    self.occurence_table[token] = 0\n",
    "                self.occurence_table[token] += 1\n",
    "                \n",
    "    \n",
    "    def _build_custom_mapping_table(self):\n",
    "        \n",
    "        _special_token = ['[UNK]', '[PAD]']\n",
    "        _actual_vocab_size = self.cust_vocab_size - len(_special_token)\n",
    "        \n",
    "        sorted_occurence = {k: v for k, v in sorted(\n",
    "            self.occurence_table.items(), reverse=True, key=lambda item: item[1]\n",
    "        )}\n",
    "        \n",
    "        used_tokens = sorted(list(sorted_occurence)[:_actual_vocab_size])\n",
    "        mapping_size = min(len(used_tokens), _actual_vocab_size)\n",
    "        \n",
    "        _bert_pad = 0\n",
    "        _bert_oov = 100\n",
    "        self._custom_pad = 0\n",
    "        self._custom_oov = mapping_size + 1\n",
    "        \n",
    "        self.bert_id_to_custom_id = {\n",
    "            _bert_pad: self._custom_pad, \n",
    "            _bert_oov: self._custom_oov\n",
    "        }\n",
    "        self.custom_id_to_bert_id = {\n",
    "            self._custom_pad: _bert_pad, \n",
    "            self._custom_oov: _bert_oov\n",
    "        }\n",
    "        \n",
    "        for i in range(0, mapping_size):\n",
    "            bert_token = used_tokens[i]\n",
    "            self.bert_id_to_custom_id[bert_token] = i + 1    \n",
    "            self.custom_id_to_bert_id[i + 1] = bert_token\n",
    "            \n",
    "        print(\"Vocab contains {0} / {1} unique tokens ({2:.2f} %)\".format(\n",
    "            len(used_tokens) + 2,\\\n",
    "            len(sorted_occurence),\\\n",
    "            (len(used_tokens) / len(sorted_occurence) * 100)\n",
    "        ))\n",
    "        \n",
    "        sorted_occurence_count = list(sorted_occurence.values())\n",
    "        used_tokens_count = sum(sorted_occurence_count[:_actual_vocab_size])\n",
    "        total_tokens_count = sum(sorted_occurence_count)\n",
    "        \n",
    "        print(\"Using {0} / {1} tokens available ({2:.2f} %)\".format(\n",
    "            used_tokens_count,\\\n",
    "            total_tokens_count,\\\n",
    "            (used_tokens_count / total_tokens_count * 100)\n",
    "        ))        \n",
    "        \n",
    "    def _convert_bert_id_to_custom_id(self, token_ids):\n",
    "        return [self.bert_id_to_custom_id[x] if x in self.bert_id_to_custom_id else self._custom_oov for x in token_ids]\n",
    "    \n",
    "    def _convert_custom_id_to_bert_id(self, token_ids):\n",
    "        return [self.custom_id_to_bert_id[x] for x in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_type, use_mapping, vocab_size):\n",
    "    \n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    if tokenizer_type == \"BERT\" :\n",
    "        tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "        tokenizer.use_custom_mapping(use_mapping, vocab_size)\n",
    "\n",
    "    # use default keras tokenizer\n",
    "    else : \n",
    "        tokenizer = TokenizerWrapper(num_words=vocab_size, oov_token=\"[UNK]\")\n",
    "        tokenizer.fit_on_texts(train_captions)    \n",
    "        tokenizer.word_index['[PAD]'] = 0\n",
    "        tokenizer.index_word[0] = '[PAD]'\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 16:20:07.026720 139869419927360 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0330 16:20:08.253211 139869419927360 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "caption_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=False,\n",
    "    vocab_size=0\n",
    ")\n",
    "\n",
    "target_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=PARAMS[\"use_mapping\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 3138.62it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2874.90it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 262383.42it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 231241.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > constructing custom mapping < \n",
      "\n",
      "Vocab contains 3000 / 3584 unique tokens (83.65 %)\n",
      "Using 68179 / 68765 tokens available (99.15 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "caption_tokens = caption_tokenizer.texts_to_sequences(train_captions)\n",
    "target_tokens = target_tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parallel dataset\n",
    "\n",
    "prepare captions and target to support parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parallel_dataset(img_paths, caption_tokens, target_tokens):\n",
    "    \n",
    "    dataset_img_paths = []\n",
    "    dataset_captions = []\n",
    "    dataset_target = []\n",
    "    dataset_target_position = []\n",
    "\n",
    "    for i in tqdm(range(0, len(train_img_paths))):\n",
    "        img = img_paths[i]\n",
    "        cap = caption_tokens[i]\n",
    "        tar = target_tokens[i]\n",
    "\n",
    "        for j in range(1, len(cap)):\n",
    "            dataset_img_paths.append(img)\n",
    "            dataset_captions.append(cap[:j])\n",
    "            dataset_target.append(tar[j])\n",
    "            dataset_target_position.append(j)\n",
    "\n",
    "    dataset_captions = pad_sequences(dataset_captions, maxlen=PARAMS[\"max_caption_length\"], padding='post')\n",
    "    \n",
    "    return dataset_img_paths, dataset_captions, dataset_target, dataset_target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 21446.56it/s]\n"
     ]
    }
   ],
   "source": [
    "parallel_img_paths, parallel_captions, parallel_target, parallel_target_position = preprocess_parallel_dataset(train_img_paths, caption_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = parallel_captions.shape[0]\n",
    "MAX_CAPTION_LENGTH = parallel_captions.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_target_position = tf.one_hot(parallel_target_position, depth=MAX_CAPTION_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "\n",
    "def load_dataset(img_name, caption, target, target_pos):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, caption, target, target_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((parallel_img_paths, parallel_captions, parallel_target, parallel_target_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "\n",
    "dataset = dataset.map(lambda item1, item2, item3, item4: tf.numpy_function(\n",
    "          load_dataset, [item1, item2, item3, item4], [tf.float32, tf.int32, tf.int32, tf.float32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(DATA_SIZE / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 16, image_feature = 100, 2048)\n",
    "# caption => (batch_size = 16, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1395 batches, (total : 44640)\n",
      "eval : 298 batches, (total : 9536)\n",
      "test : 300 batches, (total : 9600 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captioning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=PARAMS[\"learning_rate\"])\n",
    "# loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    real  => (batch_size,)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate loss\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # create mask to filter out padding token \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    # Ignore loss_ if real token is padding\n",
    "    loss_ *= mask\n",
    "    \n",
    "    # Get mean of curren batch's loss (somewhat batch norm)\n",
    "    result_loss = tf.reduce_mean(loss_)\n",
    "    \n",
    "    return result_loss\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    loss_  => (batch_size, 1)\n",
    "    mask   => (batch_size, 1)  : indicate is padding or not\n",
    "\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim)\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # x => (batch_size, 100, 2048)\n",
    "        x = self.fc(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, image_feature_size, image_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, img_feature_size, image_context_size)\n",
    "        hidden                        => (batch_size, embedding_size)\n",
    "        \n",
    "        note : \n",
    "        img_feature_size ==  64 for Inception V3,\n",
    "        img_feature_size == 100 for Xception,\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        _w1 = self.W1(features)\n",
    "        _w2 = self.W2(hidden_with_time_axis)\n",
    "        score = tf.nn.tanh(_w1 + _w2)\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_with_time_axis      => (batch_size, 1, embedding_size)\n",
    "        score                      => (batch_size, img_feature_size, units)\n",
    "        attention_weights          => (batch_size, img_feature_size, 1)\n",
    "        context_vector (after sum) => (batch_size, img_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=256, \n",
    "                 embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 combine_strategy=\"merge\", combine_layer=\"concat\",\n",
    "                 vocab_size=3000, batch_size=32):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # when to use context_vector [\"inject_init\", \"inject_pre\", \"inject_par\", \"merge\"]\n",
    "        self.combine_strategy = combine_strategy\n",
    "        \n",
    "        # how to use context_vector [\"add\", \"concat\"]\n",
    "        self.combine_layer = combine_layer\n",
    "        \n",
    "        # =====================================================\n",
    "        \n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "        \n",
    "#         self.mask = Masking(mask_value=0, dtype=\"int32\")\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "        self.positional_embedding = Embedding(\n",
    "            input_dim=MAX_CAPTION_LENGTH,\n",
    "            output_dim=16,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "        \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\") # same size as vocab\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "           \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\") # same size as vocab\n",
    "        \n",
    "        \n",
    "    def _init_embedding(self):\n",
    "        \n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.bert_embedding = TFBertModel.from_pretrained('bert-base-uncased', proxies=PROXIES)\n",
    "            self.bert_embedding.trainable = False\n",
    "            self.embedding_dim = self.bert_embedding.config.hidden_size\n",
    "            \n",
    "        else:\n",
    "            self.default_embedding = Embedding(\n",
    "                input_dim=self.vocab_size, \n",
    "                output_dim=self.embedding_dim, \n",
    "                mask_zero=True,\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def _init_rnn(self):\n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector'\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            self.bilstm = Bidirectional(LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        \n",
    "    def embedding(self, tokens, as_sentence=False):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embedding_type == \"BERT\": \n",
    "            embedding, sentence_embedding = self._bert_embedding(tokens)\n",
    "        \n",
    "        else:\n",
    "            embedding = self._default_embedding(tokens)\n",
    "            sentence_embedding = tf.reduce_mean(embedding, 1)\n",
    "            \n",
    "\n",
    "        if as_sentence:\n",
    "            # embedding => (batch_size, embedding_dim)\n",
    "            return sentence_embedding\n",
    "        \n",
    "        else:\n",
    "            # embedding => (batch_size, tokens_length, embedding_dim)\n",
    "            return embedding\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        embedding (tokens)   => (batch_size, tokens_length, embedding_dim)\n",
    "        embedding (sentence) => (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def _bert_embedding(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        # mask out attention from padding\n",
    "        # mask => (batch_size, sequence_length)\n",
    "        attention_mask = x == 0\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, attention_mask=attention_mask)\n",
    "        \"\"\"\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        token_type_ids = tf.cast((x == 0), tf.int32)\n",
    "        \n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, token_type_ids=token_type_ids)\n",
    "        return hidden_states, sentence_embedding\n",
    "    \n",
    "        \"\"\"\n",
    "        hidden states contains hidden state for each word\n",
    "        sentence_embedding is general embedding for whole sentences\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def _default_embedding(self, x):\n",
    "        \n",
    "        # embedding => (batch_size, sequence_length, embedding_size)\n",
    "        embedding = self.default_embedding(x)\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "    def apply_strategy(self, x, context_vector, curr_iter=-1):\n",
    "        \"\"\"\n",
    "        context_vector : image's vector\n",
    "        x              : rnn input (word embedding)\n",
    "        strategy       : \n",
    "        curr_iter      : current iteration number\n",
    "        \n",
    "        No longer support inject_init & inject_pre, since training become fully parallel\n",
    "        \"\"\"\n",
    "        \n",
    "#         if self.combine_strategy == \"inject_init\":\n",
    "#             initial_state = tf.squeeze(context_vector) if curr_iter == 1 else None\n",
    "#             output, state = self.rnn_model(x, initial_state=initial_state)  \n",
    "            \n",
    "#         elif self.combine_strategy == \"inject_pre\":\n",
    "#             x = context_vector if curr_iter == 1 else x\n",
    "#             output, state = self.rnn_model(x)  \n",
    "            \n",
    "        if self.combine_strategy == \"inject_par\":\n",
    "            \n",
    "            context_vector = tf.expand_dims(context_vector, 1)\n",
    "            context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "                        \n",
    "            x = self.custom_combine_layer(context_vector, x)\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output, state = self.rnn_model(x)              \n",
    "            \n",
    "        else: # merge (as default)\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output, state = self.rnn_model(x) \n",
    "            \n",
    "            context_vector = tf.expand_dims(context_vector, 1)\n",
    "            context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output = self.custom_combine_layer(context_vector, output)\n",
    "            \n",
    "        return output, state\n",
    "    \n",
    "    \n",
    "    def rnn_model(self, x, initial_state=None):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            \n",
    "            # adjust initial state, LSTM has 2 hidden states (h and c)\n",
    "            if initial_state is not None:\n",
    "                init_h = initial_state\n",
    "                init_c = tf.zeros(initial_state.shape)\n",
    "                initial_state = [init_h, init_c]\n",
    "            \n",
    "            output, h_state, c_state = self.lstm(x, initial_state=initial_state)\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            output, _, h_state, _, _ = self.bilstm(x)\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, h_state = self.gru(x, initial_state=initial_state)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, h_state\n",
    "    \n",
    "        \"\"\"\n",
    "        output => (batch_size, rnn_size)\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def custom_combine_layer(self, x, y):\n",
    "        if self.combine_layer == \"add\":\n",
    "            return self._add_layer(x, y)\n",
    "        else:\n",
    "            return self._concat_layer(x, y)\n",
    "\n",
    "        \n",
    "    def _add_layer(self, x, y):\n",
    "        \n",
    "        if x.shape[1] != y.shape[1] :\n",
    "            exception = \"Cannot combine using 'add' strategy, both tensor has different shape {} & {}\"\n",
    "            raise Exception(exception.format(x.shape, y.shape))\n",
    "            \n",
    "        return tf.keras.layers.add([x, y])\n",
    "            \n",
    "        \n",
    "    def _concat_layer(self, x, y):\n",
    "        return tf.concat([x, y], axis=-1)\n",
    "        \n",
    "    \n",
    "    def call(self, decoder_input, context_vector, position):\n",
    "        \"\"\" \n",
    "        decoder_input  : last predicted word => (batch_size, sequence len)\n",
    "        context_vector : image's vector      => (batch_size, img_context_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Mask out padding (0)\n",
    "#         decoder_input = self.mask(decoder_input)\n",
    "        \n",
    "        # x1 => (batch_size, input_sentence_len, embedding_dim + positional_dim)\n",
    "        x1 = self.embedding(decoder_input)\n",
    "#         x1_1 = self.embedding(decoder_input)\n",
    "#         x1_2 = self.positional_embedding(position)\n",
    "        \n",
    "#         x1_2 = tf.reduce_mean(x1_2, axis=1)\n",
    "#         x1_2 = tf.expand_dims(x1_2, axis=1)\n",
    "#         x1_2 = tf.tile(x1_2, multiples=[1, x1_1.shape[1], 1])\n",
    "        \n",
    "#         x1 = tf.concat([x1_1, x1_2], axis=-1)\n",
    "        \n",
    "        # x2 (concat) => (batch_size, input_sentence_len, embedding_dim + image_context_size)\n",
    "        # x2 (add) => (batch_size, embedding_dim)\n",
    "        x2, rnn_state = self.apply_strategy(x1, context_vector)\n",
    "        \n",
    "        ## ============================================\n",
    "        ## TODO: add another attention layer ? \n",
    "        ## ============================================\n",
    "        \n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        # x3 => (batch_size, sequence_len, rnn_units)\n",
    "        x3 = self.fc1(x2)   # how important is every sequence \n",
    "        x3 = self.batchnorm(x3)\n",
    "        x3 = self.leakyrelu(x3)\n",
    "        \n",
    "        # x4 => (batch_size, sequence_len * rnn_units) || was (batch_size * sequence_len=1, rnn_units)\n",
    "        x4 = tf.reshape(x3, (x3.shape[0], -1))\n",
    "\n",
    "        x4 = self.dropout(x4)\n",
    "        \n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x4)\n",
    "        \n",
    "        return word_predictions, rnn_state\n",
    "    \n",
    "\n",
    "    def reset_state(self, batch_size=None):\n",
    "        \n",
    "        if batch_size is not None:\n",
    "            return tf.zeros((batch_size, self.rnn_units))\n",
    "        \n",
    "        return tf.zeros((self.batch_size, self.rnn_units))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rnn_units': 256,\n",
       " 'rnn_type': 'BiLSTM',\n",
       " 'tokenizer': 'BERT',\n",
       " 'word_embedding': 'BERT',\n",
       " 'vocab_size': 3000,\n",
       " 'combine_strategy': 'merge',\n",
       " 'combine_layer': 'concat',\n",
       " 'image_context_size': 256,\n",
       " 'word_embedding_dim': 256,\n",
       " 'batch_size': 32,\n",
       " 'data_size': 5000,\n",
       " 'use_mapping': True,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_caption_length': 25,\n",
       " 'image_feature_extractor': 'xception',\n",
       " 'use_sequence': True,\n",
       " 'epoch': 20,\n",
       " 'use_mask': True,\n",
       " 'version': 'v5 : positional embedding'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 16:20:15.604147 139869419927360 configuration_utils.py:256] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/m13516112/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0330 16:20:15.605631 139869419927360 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0330 16:20:16.817837 139869419927360 modeling_tf_utils.py:333] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5 from cache at /home/m13516112/.cache/torch/transformers/d667df51ec24c20190f01fb4c20a21debc4c4fc12f7e2f5441ac0a99690e3ee9.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5\n",
      "I0330 16:20:19.645560 139869419927360 modeling_tf_utils.py:375] Layers from pretrained model not used in TFBertModel: ['mlm___cls', 'nsp___cls']\n"
     ]
    }
   ],
   "source": [
    "encoder = CNN_Encoder(\n",
    "    output_dim=PARAMS[\"image_context_size\"]\n",
    ")\n",
    "\n",
    "attention = BahdanauAttention(\n",
    "    units=PARAMS[\"rnn_units\"]\n",
    ")\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    rnn_type=PARAMS[\"rnn_type\"], \n",
    "    rnn_units=PARAMS[\"rnn_units\"],\n",
    "    embedding_type=PARAMS[\"word_embedding\"], \n",
    "    embedding_dim=PARAMS[\"word_embedding_dim\"],  \n",
    "    combine_strategy=PARAMS[\"combine_strategy\"], \n",
    "    combine_layer=PARAMS[\"combine_layer\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    "    batch_size=PARAMS[\"batch_size\"]\n",
    ")\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Requirements\n",
    "\n",
    "# combine_strategy = \"inject_init\" : IMAGE_CONTEXT_SIZE == UNITS\n",
    "# combine_strategy = \"inject_pre\"  : IMAGE_CONTEXT_SIZE == WORD_EMBEDDING_DIM\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default feed forward function\n",
    "\n",
    "@tf.function\n",
    "def feed_forward(img_tensor, caption, position):\n",
    "    \"\"\"\n",
    "    img_tensor => (batch_size, image_feature_size, 2048)\n",
    "    caption => (batch_size, max_caption_length)\n",
    "    \"\"\"\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    # note : used to be decoder hidden state, \n",
    "    # but changed into current word embedding for paralel training\n",
    "    \n",
    "    # hidden => (batch_size, embedding_size)\n",
    "    hidden = decoder.embedding(caption, as_sentence=True)\n",
    "\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = encoder(img_tensor)\n",
    "\n",
    "    # context_vector => (batch_size, image_context_size)\n",
    "    context_vector, _ = attention(features, hidden)\n",
    "\n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions, _ = decoder(caption, context_vector, position)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, caption, target, position):\n",
    "    \"\"\"\n",
    "    target => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training model\n",
    "    with tf.GradientTape() as gradient_tape:\n",
    "        \n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions = feed_forward(img_tensor, caption, position)\n",
    "        \n",
    "        # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "        loss = loss_function(target, predictions)\n",
    "\n",
    "        \n",
    "    # Apply gradient\n",
    "    trainable_variables = encoder.trainable_variables + \\\n",
    "                          decoder.trainable_variables + \\\n",
    "                          attention.trainable_variables\n",
    "    \n",
    "    gradients = gradient_tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def eval_step(img_tensor, caption, target):\n",
    "    \n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions = feed_forward(img_tensor, caption)\n",
    "    \n",
    "    # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "    loss = loss_function(target, predictions)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def choose_predicted_id(predictions, strategy=\"max\", sampling_k=10):\n",
    "    \"\"\"\n",
    "    predictions : encoder word prediction => (batch_size, vocab_size)\n",
    "    strategy    : how to choose word [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sampling method (categorical dist)\n",
    "    if strategy == \"sample\":\n",
    "        \n",
    "        # sampled_proba & sampled_ids => (batch_size, sampling_k)\n",
    "        sampled_proba, sampled_ids = tf.math.top_k(predictions, sampling_k)\n",
    "        \n",
    "        # chosen_sampled_col => (batch_size, )\n",
    "        chosen_sampled_col = tf.squeeze(tf.random.categorical(sampled_proba, 1))\n",
    "        \n",
    "        # create row idx to zip with chosen_sampled_col\n",
    "        row_idx = tf.range(predictions.shape[0], dtype=chosen_sampled_col.dtype)\n",
    "        row_col_idx = tf.stack([row_idx, chosen_sampled_col], axis=1)\n",
    "        \n",
    "        # predicted_ids => (batch_size, )\n",
    "        predicted_ids = tf.gather_nd(sampled_ids, row_col_idx)\n",
    "\n",
    "    # Max index method\n",
    "    else:\n",
    "        predicted_ids = tf.argmax(predictions, 1)\n",
    "    \n",
    "    # predicted_ids => (batch_size, )\n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(images_paths):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [load_image(x)[0] for x in images_paths]\n",
    "    \n",
    "    # x => (batch_size, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, 10, 10, 2048)\n",
    "    x = extractor(x)\n",
    "    \n",
    "    # x  => (batch_size, img_feature_size, 2048)\n",
    "    x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = encoder(x)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def get_supporting_features(images_paths, strategy=\"mean\"):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, img_count, 1)\n",
    "    strategy : strategy to aggregate multiple supporting image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [[load_image(x)[0] for x in images_set] for images_set in images_paths]\n",
    "    \n",
    "    # x => (batch_size, img_count, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, img_count, 10, 10, 2048)\n",
    "    x = [extractor(image_set) for image_set in x]\n",
    "    \n",
    "    # features => (batch_size, img_count, img_feature_size, image_context_size)\n",
    "    features = encoder(x)\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    if strategy == \"logsumexp\":\n",
    "        features = tf.reduce_logsumexp(features, 1)\n",
    "    elif strategy == \"max\":\n",
    "        features = tf.reduce_max(features, 1)\n",
    "    elif strategy == \"min\":\n",
    "        features = tf.reduce_min(features, 1)\n",
    "    else:\n",
    "        features = tf.reduce_mean(features, 1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_indices(support_text):\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(0, len(support_text)):\n",
    "        context_token = target_tokenizer.tokenize(support_text[i])\n",
    "        context_token_id = target_tokenizer.convert_tokens_to_ids(context_token)\n",
    "        context_token_id = set(context_token_id)\n",
    "        context_token_id.discard(0)\n",
    "        for x in sorted(context_token_id):\n",
    "            indices.append([i, x])\n",
    "\n",
    "    # return => (word_count, 2)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_supporting_text_vector(support_text, vocab_size):\n",
    "    \"\"\"\n",
    "    support_text : list of text describing main image context => (batch_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(support_text)\n",
    "    \n",
    "    # indices => ( sum(batch_size * ?word_count), 2)\n",
    "    indices = get_one_hot_indices(support_text)\n",
    "    values = tf.ones(len(indices))\n",
    "    sparse_one_hot = tf.sparse.SparseTensor(indices, values, dense_shape=[batch_size, vocab_size])\n",
    "    \n",
    "    # sparse_one_hot => (batch_size, vocab_size)\n",
    "    return sparse_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluate(images_paths,\n",
    "                    support_text=None,\n",
    "                    support_imgs=None, \n",
    "                    support_aggregate_strategy=\"mean\",\n",
    "                    pplm_iteration=3,\n",
    "                    pplm_weight=0.03,\n",
    "                    pplm_gm_weight=0.8,\n",
    "                    choose_word_strategy=\"sample\",\n",
    "                   ):\n",
    "    \n",
    "    \"\"\"\n",
    "    images_paths : list of image_path                           => (batch_size, 1)\n",
    "    support_text : list of text describing main image context   => (batch_size)\n",
    "    support_imgs : list of list of image_path                   => (batch_size, image_count, 1)\n",
    "    support_aggregate_strategy : how to aggregate support image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    pplm_iteration : number of pplm step done for every decoding step\n",
    "    pplm_weight    : weight of pplm loss\n",
    "    pplm_gm_weight : geometric mean fusion weight (0 means use only original prediction, 1 means use only pplm prediction)\n",
    "    choose_word_strategy : how to choose word from prediction distribution [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(images_paths)\n",
    "    \n",
    "    # initialize captions placeholder\n",
    "    start_token = caption_tokenizer.convert_tokens_to_ids(['[CLS]']) # use bert id's not custom id\n",
    "    result_captions = tf.tile(tf.expand_dims(start_token, 1), [batch_size, 1])\n",
    "    attention_plot = tf.reshape([], shape=(batch_size, 0, IMAGE_FEATURE_SHAPE[0]))\n",
    "    \n",
    "    # Extract features from main images\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = get_image_features(images_paths)\n",
    "    \n",
    "    \n",
    "    if support_text is not None:\n",
    "        # support_text_vector => (batch_size, vocab_size)\n",
    "        support_text_vector = get_supporting_text_vector(support_text, decoder.vocab_size) \n",
    "    else:\n",
    "        # set all pplm related variable to 0\n",
    "        pplm_iteration = 0\n",
    "        pplm_weight = 0\n",
    "        pplm_gm_weight = 0\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(MAX_CAPTION_LENGTH)):\n",
    "        \n",
    "        # decoder_input => (batch_size, ~MAX_CAPTION_LENGTH)\n",
    "        caption = pad_sequences(result_captions, maxlen=MAX_CAPTION_LENGTH, padding=\"post\")\n",
    "        \n",
    "        # hidden => (batch_size, embedding_size)\n",
    "        hidden = decoder.embedding(caption, as_sentence=True)  ## BOTTLE\n",
    "\n",
    "        # context_vector => (batch_size, image_context_size)\n",
    "        # attention_weights => (batch_size, img_feature_size, 1)\n",
    "        context_vector, attention_weights = attention(features, hidden)\n",
    "\n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions, _ = decoder(caption, context_vector, position=None)  \n",
    "        \n",
    "        # ======================== PPLM section ========================\n",
    "        ori_prediction = predictions\n",
    "        \n",
    "        curr_pertubation = tf.Variable(tf.zeros((batch_size, decoder.embedding_dim)), name=\"curr_pertubation\", trainable=True)\n",
    "        \n",
    "        \n",
    "        for j in range(pplm_iteration):\n",
    "            \n",
    "            with tf.GradientTape() as pplm_tape: \n",
    "                \n",
    "                hidden += curr_pertubation\n",
    "                \n",
    "                context_vector, attention_weights = attention(features, hidden)\n",
    "                predictions, _ = decoder(caption, context_vector)\n",
    "                pplm_loss = pplm_loss_function(support_text_vector, predictions, pplm_weight=pplm_weight)\n",
    "                \n",
    "            \"\"\"\n",
    "            most impactfull layer to train = last dense layer\n",
    "            \"\"\"\n",
    "            \n",
    "            trainable_variables = [curr_pertubation]\n",
    "            gradients = pplm_tape.gradient(pplm_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "            \n",
    "            predictions, _ = decoder(caption, context_vector)\n",
    "        \n",
    "        # fuse final pplm_prediction and original prediction\n",
    "        fused_predictions = (predictions * pplm_gm_weight) + (ori_prediction * (1 - pplm_gm_weight)) \n",
    "        \n",
    "        # predicted_ids => (batch_size,)\n",
    "        predicted_ids = choose_predicted_id(fused_predictions, strategy=choose_word_strategy)\n",
    "        \n",
    "        # convert custom id mapping to bert's id\n",
    "        # predicted_bert_ids => (batch_size, 1)\n",
    "        predicted_bert_ids = target_tokenizer._convert_custom_id_to_bert_id(predicted_ids.numpy())\n",
    "        predicted_bert_ids = tf.expand_dims(predicted_bert_ids, 1)\n",
    "        \n",
    "        # store result\n",
    "        result_captions = tf.concat([result_captions, predicted_bert_ids], axis=1)\n",
    "        \n",
    "        # attention_weights => (batch_size, 1, img_feature_size)\n",
    "        attention_weights = tf.reshape(attention_weights, shape=(batch_size, 1, -1))\n",
    "        \n",
    "        # assign attention weights to respective generated word\n",
    "        # attention_plot => (batch_size, ~max_caption_len, feature_size)\n",
    "        attention_plot = tf.concat([attention_plot, attention_weights], axis=1)\n",
    "        \n",
    "    # remove start token & revert to tokens\n",
    "    result_captions = [caption_tokenizer.convert_ids_to_tokens(x[1:]) for x in result_captions]\n",
    "    \n",
    "    return result_captions, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy, MeanSquaredError\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def pplm_loss_function(real, pred, pplm_weight=0.03):\n",
    "    \"\"\"\n",
    "    real  => (batch_size, vocab_size)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "\n",
    "#     mm = tf.sparse.sparse_dense_matmul(real, tf.transpose(pred))\n",
    "#     mm = tf.reduce_sum(tf.abs(mm), 1)\n",
    "#     loss = tf.reduce_sum(mm, 0)\n",
    "    \n",
    "    real = tf.sparse.to_dense(real, default_value=0)\n",
    "    pplm_loss = CategoricalCrossentropy(from_logits=True)\n",
    "    loss = pplm_loss(real, pred, pplm_weight)\n",
    "    \n",
    "#     print(loss)\n",
    "    return loss\n",
    "\n",
    "    \"\"\"\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1395it [05:01,  4.63it/s]\n",
      "1395it [04:18,  5.40it/s]\n",
      "1395it [04:16,  5.44it/s]\n",
      "1395it [04:21,  5.34it/s]\n",
      "1395it [04:18,  5.39it/s]\n",
      "1395it [04:19,  5.38it/s]\n",
      "1395it [04:16,  5.43it/s]\n",
      "1395it [04:20,  5.36it/s]\n",
      "1395it [04:18,  5.39it/s]\n",
      "1395it [04:18,  5.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f32048afa20>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    for (img_tensor, captions, target, target_position) in tqdm(train_dataset):\n",
    "        batch_loss = train_step(img_tensor, captions, target, target_position)\n",
    "        loss.append(batch_loss.numpy())\n",
    "        \n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f32048305c0>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.991021341174841"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-500:]) / len(loss[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0908419034689665"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-5000:]) / len(loss[-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:00<00:00, 536422.54it/s]\n"
     ]
    }
   ],
   "source": [
    "true_captions = {}\n",
    "for annot in tqdm(annotations[\"annotations\"]):\n",
    "    image_id = annot[\"image_id\"]\n",
    "\n",
    "    if image_id not in true_captions:\n",
    "        true_captions[image_id] = []\n",
    "    \n",
    "    true_captions[image_id].append(annot['caption'])\n",
    "    \n",
    "    \n",
    "def get_true_captions(image_paths):\n",
    "    return [true_captions[int(x[-10:-4])] for x in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:13<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "a man on a suit case in its air skate slopes ' sides standing next .\n",
      "\n",
      "true caps : \n",
      "some people are standing near some public toilets\n",
      "A group of people are standing out in front of portable toilets.\n",
      "people are standing around some porta potties and mess\n",
      "a couple of people stand in front of some toilets \n",
      "groups of people standing around the toilet area\n",
      "=====================================\n",
      "predicted : \n",
      "a small new colorful colored with some cell [UNK] of a and a and it\n",
      "\n",
      "true caps : \n",
      "A person is holding onto a cellphone somewhere.\n",
      "there is someone holding a small phone with a small screen\n",
      "A picture of a cell phone showing the time as 8:56 PM.\n",
      "a hand is holding a black and silver cellphone\n",
      "This is a verizon LG Envy 2 cell phone.\n",
      "=====================================\n",
      "predicted : \n",
      "some party people gathered a people that to a room [UNK] , the big large group , and sink walking mask at women playing room\n",
      "\n",
      "true caps : \n",
      "People working on laptop computers in a student dining room\n",
      "People sitting in a room at tables with laptops \n",
      "A group of people that are sitting in front of laptops.\n",
      "Many people have signed up for the computer seminar.\n",
      "A group of people at tables with laptops having conversation.\n",
      "=====================================\n",
      "predicted : \n",
      "an very brown airplane taking [UNK] into large white of black trees in grass background airline\n",
      "\n",
      "true caps : \n",
      "two birds near one another in a field \n",
      "Two black colored birds sitting in a field together. \n",
      "Two birds that are sitting on the ground.\n",
      "2 birds standing on the ground in a field \n",
      "Two birds that are standing next to each other on the grass. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[5:9]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images)\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:13<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "a man standing in a grassy covered area .\n",
      "\n",
      "true caps : \n",
      "some people are standing near some public toilets\n",
      "A group of people are standing out in front of portable toilets.\n",
      "people are standing around some porta potties and mess\n",
      "a couple of people stand in front of some toilets \n",
      "groups of people standing around the toilet area\n",
      "=====================================\n",
      "predicted : \n",
      "a close up of a person holding a new hand\n",
      "\n",
      "true caps : \n",
      "A person is holding onto a cellphone somewhere.\n",
      "there is someone holding a small phone with a small screen\n",
      "A picture of a cell phone showing the time as 8:56 PM.\n",
      "a hand is holding a black and silver cellphone\n",
      "This is a verizon LG Envy 2 cell phone.\n",
      "=====================================\n",
      "predicted : \n",
      "people working on a large large large room\n",
      "\n",
      "true caps : \n",
      "People working on laptop computers in a student dining room\n",
      "People sitting in a room at tables with laptops \n",
      "A group of people that are sitting in front of laptops.\n",
      "Many people have signed up for the computer seminar.\n",
      "A group of people at tables with laptops having conversation.\n",
      "=====================================\n",
      "predicted : \n",
      "a bird sits in a grassy and red area and a tree\n",
      "\n",
      "true caps : \n",
      "two birds near one another in a field \n",
      "Two black colored birds sitting in a field together. \n",
      "Two birds that are sitting on the ground.\n",
      "2 birds standing on the ground in a field \n",
      "Two birds that are standing next to each other on the grass. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[5:9]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_strategy=\"max\")\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "stop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-5981adc29e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: stop"
     ]
    }
   ],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train/{}\".format(str(datetime.now())[:-10])\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           attention=attention,\n",
    "                           optimizer=optimizer\n",
    "                          )\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------\")\n",
    "print(checkpoint_path)\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(checkpoint_path)\n",
    "with open(checkpoint_path + \"/config.txt\", \"w\") as f:\n",
    "    f.write(str(PARAMS))\n",
    "    \n",
    "log_file = open(checkpoint_path + \"/log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, PARAMS[\"epoch\"]):\n",
    "    \n",
    "    start = time.time()\n",
    "    loss = 0\n",
    "    batch = 1\n",
    "\n",
    "    for img_tensor, captions, target in tqdm(train_dataset):\n",
    "        \n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "\n",
    "            with open(checkpoint_path + \"/log.txt\", \"a\") as f:\n",
    "                log_message = ' {} Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                    checkpoint_path, epoch + 1, batch, batch_loss.numpy())\n",
    "                f.write(str(log_message + \"\\n\"))\n",
    "                print(log_message)   \n",
    "\n",
    "        batch += 1\n",
    "        \n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(batch_loss.numpy())\n",
    "\n",
    "#     ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = []\n",
    "\n",
    "# for (img_tensor, captions, target) in tqdm(train_dataset):\n",
    "#     batch_loss = train_step(img_tensor, captions, target)\n",
    "#     loss.append(batch_loss.numpy())\n",
    "    \n",
    "# plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PPLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "# text = [\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "# ]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images, support_text=text, pplm_iteration=5, pplm_weight=1)\n",
    "# result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
