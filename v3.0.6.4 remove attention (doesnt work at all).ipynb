{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git add \"Keras.ipynb\"\n",
    "!git commit -m \"test pred using pplm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"rnn_units\": 256,\n",
    "    \"rnn_type\": \"BiLSTM\",\n",
    "    \"tokenizer\": \"BERT\",\n",
    "    \"word_embedding\": \"BERT\",\n",
    "    \"vocab_size\": 3000,\n",
    "    \"combine_strategy\": \"merge\",\n",
    "    \"combine_layer\": \"concat\",\n",
    "    \"image_context_size\": 256,\n",
    "    \"word_embedding_dim\": 256,\n",
    "    \"batch_size\": 32,\n",
    "    \"data_size\": 5000,\n",
    "    \"use_mapping\": True,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_caption_length\": 25, # use <int> or None\n",
    "    \"image_feature_extractor\": \"xception\",\n",
    "    \"use_sequence\": True,\n",
    "    \"epoch\": 20,\n",
    "    \"use_mask\": True,\n",
    "    \n",
    "    \"beam_search_k\": 5,\n",
    "    \"version\": \"v5 : positional embedding\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "\n",
    "# for TFBertModel\n",
    "PROXIES = {\n",
    "  \"http\": \"http://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "  \"https\": \"https://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"[CLS] \" + annot['caption'] + \" [SEP]\"\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "\n",
    "stopper = -1 if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]\n",
    "train_captions = all_captions[:stopper]\n",
    "train_img_paths = all_img_paths[:stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 5000\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = len(train_captions) if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature_extractor(model_type=\"xception\"):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.compat.v1.keras.applications.xception\n",
    "        cnn_model = tf.compat.v1.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"CNN encoder model not supported yet\")\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "def get_image_feature_shape(model_type):\n",
    "    \n",
    "    if model_type == \"xception\":\n",
    "        return (100, 2048)\n",
    "    elif model_type == \"inception_v3\":\n",
    "        return (64, 2048)\n",
    "    else:\n",
    "        raise Exception (\"model unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = extractor_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FEATURE_SHAPE = get_image_feature_shape(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extractor, extractor_preprocessor = get_image_feature_extractor(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image feature\n",
    "\n",
    "this step mostly skipped since the results are cached already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 157.25\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = PARAMS[\"batch_size\"]\n",
    "\n",
    "estimated_batch_count = DATA_SIZE / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique images\n",
    "# unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# # Prepare dataset\n",
    "# image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "# image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "# image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessed image (batch)\n",
    "\n",
    "# for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "#     # get context vector of batch images\n",
    "#     batch_features = extractor(batch_imgs)\n",
    "    \n",
    "#     # flatten 2D cnn result into 1D for RNN decoder input\n",
    "#     # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "#     # image_feature = 100 (Xception)\n",
    "#     # image_feature = 64 (Inception V3)\n",
    "#     batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "#     # Cache preprocessed image\n",
    "#     for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "#         image_path = image_path.numpy().decode(\"utf-8\")\n",
    "#         np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Caption dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0401 12:45:35.710252 140465195878208 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0401 12:45:35.711478 140465195878208 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = PARAMS[\"tokenizer\"]\n",
    "VOCAB_SIZE = PARAMS[\"vocab_size\"]  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "    \n",
    "    def use_custom_mapping(self, use_mapping=True, vocab_size=3000):\n",
    "        \n",
    "        self.use_mapping = use_mapping\n",
    "        self.cust_vocab_size = vocab_size\n",
    "        self.mapping_initialized = False\n",
    "\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into custom indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        bert_ids = [self.convert_tokens_to_ids(self.tokenize(x)) for x in tqdm(texts)]\n",
    "        \n",
    "        if not self.use_mapping:\n",
    "            return bert_ids\n",
    "        \n",
    "        if not self.mapping_initialized:\n",
    "            self._initialize_custom_mapping(bert_ids)\n",
    "            return [self._convert_bert_id_to_custom_id(x) for x in tqdm(bert_ids)]\n",
    "        \n",
    "        return bert_ids\n",
    "    \n",
    "        \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \n",
    "        bert_ids = super().convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            return self._convert_bert_id_to_custom_id(bert_ids)\n",
    "        else:\n",
    "            return bert_ids\n",
    "        \n",
    "        \n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            bert_ids = self._convert_custom_id_to_bert_id(token_ids)\n",
    "        else:\n",
    "            bert_ids = token_ids\n",
    "            \n",
    "        bert_tokens = super().convert_ids_to_tokens(bert_ids)\n",
    "        return bert_tokens\n",
    "    \n",
    "    \n",
    "    def _initialize_custom_mapping(self, corpus_bert_ids):\n",
    "        \n",
    "        print(\"    > constructing custom mapping < \\n\")\n",
    "        self._build_occurence_table(corpus_bert_ids)\n",
    "        self._build_custom_mapping_table()\n",
    "        self.mapping_initialized = True\n",
    "        \n",
    "        \n",
    "    def _build_occurence_table(self, tokenized_captions):\n",
    "        \"\"\"\n",
    "        build dict of token frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        self.occurence_table = {}\n",
    "        for caption in tqdm(tokenized_captions):\n",
    "            for token in caption:\n",
    "                if token not in self.occurence_table:\n",
    "                    self.occurence_table[token] = 0\n",
    "                self.occurence_table[token] += 1\n",
    "                \n",
    "    \n",
    "    def _build_custom_mapping_table(self):\n",
    "        \n",
    "        _special_token = ['[UNK]', '[PAD]']\n",
    "        _actual_vocab_size = self.cust_vocab_size - len(_special_token)\n",
    "        \n",
    "        sorted_occurence = {k: v for k, v in sorted(\n",
    "            self.occurence_table.items(), reverse=True, key=lambda item: item[1]\n",
    "        )}\n",
    "        \n",
    "        used_tokens = sorted(list(sorted_occurence)[:_actual_vocab_size])\n",
    "        mapping_size = min(len(used_tokens), _actual_vocab_size)\n",
    "        \n",
    "        _bert_pad = 0\n",
    "        _bert_oov = 100\n",
    "        self._custom_pad = 0\n",
    "        self._custom_oov = mapping_size + 1\n",
    "        \n",
    "        self.bert_id_to_custom_id = {\n",
    "            _bert_pad: self._custom_pad, \n",
    "            _bert_oov: self._custom_oov\n",
    "        }\n",
    "        self.custom_id_to_bert_id = {\n",
    "            self._custom_pad: _bert_pad, \n",
    "            self._custom_oov: _bert_oov\n",
    "        }\n",
    "        \n",
    "        for i in range(0, mapping_size):\n",
    "            bert_token = used_tokens[i]\n",
    "            self.bert_id_to_custom_id[bert_token] = i + 1    \n",
    "            self.custom_id_to_bert_id[i + 1] = bert_token\n",
    "            \n",
    "        print(\"Vocab contains {0} / {1} unique tokens ({2:.2f} %)\".format(\n",
    "            len(used_tokens) + 2,\\\n",
    "            len(sorted_occurence),\\\n",
    "            (len(used_tokens) / len(sorted_occurence) * 100)\n",
    "        ))\n",
    "        \n",
    "        sorted_occurence_count = list(sorted_occurence.values())\n",
    "        used_tokens_count = sum(sorted_occurence_count[:_actual_vocab_size])\n",
    "        total_tokens_count = sum(sorted_occurence_count)\n",
    "        \n",
    "        print(\"Using {0} / {1} tokens available ({2:.2f} %)\".format(\n",
    "            used_tokens_count,\\\n",
    "            total_tokens_count,\\\n",
    "            (used_tokens_count / total_tokens_count * 100)\n",
    "        ))        \n",
    "        \n",
    "    def _convert_bert_id_to_custom_id(self, token_ids):\n",
    "        return [self.bert_id_to_custom_id[x] if x in self.bert_id_to_custom_id else self._custom_oov for x in token_ids]\n",
    "    \n",
    "    def _convert_custom_id_to_bert_id(self, token_ids):\n",
    "        return [self.custom_id_to_bert_id[x] for x in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_type, use_mapping, vocab_size):\n",
    "    \n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    if tokenizer_type == \"BERT\" :\n",
    "        tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "        tokenizer.use_custom_mapping(use_mapping, vocab_size)\n",
    "\n",
    "    # use default keras tokenizer\n",
    "    else : \n",
    "        tokenizer = TokenizerWrapper(num_words=vocab_size, oov_token=\"[UNK]\")\n",
    "        tokenizer.fit_on_texts(train_captions)    \n",
    "        tokenizer.word_index['[PAD]'] = 0\n",
    "        tokenizer.index_word[0] = '[PAD]'\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0401 12:45:38.193645 140465195878208 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0401 12:45:39.413329 140465195878208 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "caption_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=False,\n",
    "    vocab_size=0\n",
    ")\n",
    "\n",
    "target_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=PARAMS[\"use_mapping\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 2702.71it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2756.64it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 238836.54it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 261529.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > constructing custom mapping < \n",
      "\n",
      "Vocab contains 3000 / 3584 unique tokens (83.65 %)\n",
      "Using 68179 / 68765 tokens available (99.15 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "caption_tokens = caption_tokenizer.texts_to_sequences(train_captions)\n",
    "target_tokens = target_tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parallel dataset\n",
    "\n",
    "prepare captions and target to support parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parallel_dataset(img_paths, caption_tokens, target_tokens):\n",
    "    \n",
    "    dataset_img_paths = []\n",
    "    dataset_captions = []\n",
    "    dataset_target = []\n",
    "    dataset_target_position = []\n",
    "\n",
    "    for i in tqdm(range(0, len(train_img_paths))):\n",
    "        img = img_paths[i]\n",
    "        cap = caption_tokens[i]\n",
    "        tar = target_tokens[i]\n",
    "\n",
    "        for j in range(1, len(cap)):\n",
    "            dataset_img_paths.append(img)\n",
    "            dataset_captions.append(cap[:j])\n",
    "            dataset_target.append(tar[j])\n",
    "            dataset_target_position.append(j)\n",
    "\n",
    "    dataset_captions = pad_sequences(dataset_captions, maxlen=PARAMS[\"max_caption_length\"], padding='post')\n",
    "    \n",
    "    return dataset_img_paths, dataset_captions, dataset_target, dataset_target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 20453.77it/s]\n"
     ]
    }
   ],
   "source": [
    "parallel_img_paths, parallel_captions, parallel_target, parallel_target_position = preprocess_parallel_dataset(train_img_paths, caption_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = parallel_captions.shape[0]\n",
    "MAX_CAPTION_LENGTH = parallel_captions.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_target_position = tf.one_hot(parallel_target_position, depth=MAX_CAPTION_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "\n",
    "def load_dataset(img_name, caption, target, target_pos):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, caption, target, target_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((parallel_img_paths, parallel_captions, parallel_target, parallel_target_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "\n",
    "dataset = dataset.map(lambda item1, item2, item3, item4: tf.numpy_function(\n",
    "          load_dataset, [item1, item2, item3, item4], [tf.float32, tf.int32, tf.int32, tf.float32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(DATA_SIZE / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 16, image_feature = 100, 2048)\n",
    "# caption => (batch_size = 16, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1395 batches, (total : 44640)\n",
      "eval : 298 batches, (total : 9536)\n",
      "test : 300 batches, (total : 9600 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captioning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=PARAMS[\"learning_rate\"])\n",
    "# loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    real  => (batch_size,)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate loss\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # create mask to filter out padding token \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    # Ignore loss_ if real token is padding\n",
    "    loss_ *= mask\n",
    "    \n",
    "    # Get mean of curren batch's loss (somewhat batch norm)\n",
    "    result_loss = tf.reduce_mean(loss_)\n",
    "    \n",
    "    return result_loss\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    loss_  => (batch_size, 1)\n",
    "    mask   => (batch_size, 1)  : indicate is padding or not\n",
    "\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim)\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # x => (batch_size, 100, 2048)\n",
    "        x = tf.reshape(x, (-1, x.shape[1] * x.shape[2]))\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, image_feature_size, image_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "#         self.W1 = Dense(units)\n",
    "#         self.W2 = Dense(units)\n",
    "#         self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, img_feature_size, image_context_size)\n",
    "        hidden                        => (batch_size, embedding_size)\n",
    "        \n",
    "        note : \n",
    "        img_feature_size ==  64 for Inception V3,\n",
    "        img_feature_size == 100 for Xception,\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        _w1 = self.W1(features)\n",
    "        _w2 = self.W2(hidden_with_time_axis)\n",
    "        score = tf.nn.tanh(_w1 + _w2)\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, 0\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_with_time_axis      => (batch_size, 1, embedding_size)\n",
    "        score                      => (batch_size, img_feature_size, units)\n",
    "        attention_weights          => (batch_size, img_feature_size, 1)\n",
    "        context_vector (after sum) => (batch_size, img_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=256, \n",
    "                 embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 combine_strategy=\"merge\", combine_layer=\"concat\",\n",
    "                 vocab_size=3000, batch_size=32):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # when to use context_vector [\"inject_init\", \"inject_pre\", \"inject_par\", \"merge\"]\n",
    "        self.combine_strategy = combine_strategy\n",
    "        \n",
    "        # how to use context_vector [\"add\", \"concat\"]\n",
    "        self.combine_layer = combine_layer\n",
    "        \n",
    "        # =====================================================\n",
    "        \n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "        \n",
    "#         self.mask = Masking(mask_value=0, dtype=\"int32\")\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "        self.positional_embedding = Embedding(\n",
    "            input_dim=MAX_CAPTION_LENGTH,\n",
    "            output_dim=16,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "        \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\") # same size as vocab\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "           \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\") # same size as vocab\n",
    "        \n",
    "        \n",
    "    def _init_embedding(self):\n",
    "        \n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.bert_embedding = TFBertModel.from_pretrained('bert-base-uncased', proxies=PROXIES)\n",
    "            self.bert_embedding.trainable = False\n",
    "            self.embedding_dim = self.bert_embedding.config.hidden_size\n",
    "            \n",
    "        else:\n",
    "            self.default_embedding = Embedding(\n",
    "                input_dim=self.vocab_size, \n",
    "                output_dim=self.embedding_dim, \n",
    "                mask_zero=True,\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def _init_rnn(self):\n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector'\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            self.bilstm = Bidirectional(LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        \n",
    "    def embedding(self, tokens, as_sentence=False):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embedding_type == \"BERT\": \n",
    "            embedding, sentence_embedding = self._bert_embedding(tokens)\n",
    "        \n",
    "        else:\n",
    "            embedding = self._default_embedding(tokens)\n",
    "            sentence_embedding = tf.reduce_mean(embedding, 1)\n",
    "            \n",
    "\n",
    "        if as_sentence:\n",
    "            # embedding => (batch_size, embedding_dim)\n",
    "            return sentence_embedding\n",
    "        \n",
    "        else:\n",
    "            # embedding => (batch_size, tokens_length, embedding_dim)\n",
    "            return embedding\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        embedding (tokens)   => (batch_size, tokens_length, embedding_dim)\n",
    "        embedding (sentence) => (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def _bert_embedding(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        # mask out attention from padding\n",
    "        # mask => (batch_size, sequence_length)\n",
    "        attention_mask = x == 0\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, attention_mask=attention_mask)\n",
    "        \"\"\"\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        token_type_ids = tf.cast((x == 0), tf.int32)\n",
    "        \n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, token_type_ids=token_type_ids)\n",
    "        return hidden_states, sentence_embedding\n",
    "    \n",
    "        \"\"\"\n",
    "        hidden states contains hidden state for each word\n",
    "        sentence_embedding is general embedding for whole sentences\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def _default_embedding(self, x):\n",
    "        \n",
    "        # embedding => (batch_size, sequence_length, embedding_size)\n",
    "        embedding = self.default_embedding(x)\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "    def apply_strategy(self, x, context_vector, curr_iter=-1):\n",
    "        \"\"\"\n",
    "        context_vector : image's vector\n",
    "        x              : rnn input (word embedding)\n",
    "        strategy       : \n",
    "        curr_iter      : current iteration number\n",
    "        \n",
    "        No longer support inject_init & inject_pre, since training become fully parallel\n",
    "        \"\"\"\n",
    "        \n",
    "#         if self.combine_strategy == \"inject_init\":\n",
    "#             initial_state = tf.squeeze(context_vector) if curr_iter == 1 else None\n",
    "#             output, state = self.rnn_model(x, initial_state=initial_state)  \n",
    "            \n",
    "#         elif self.combine_strategy == \"inject_pre\":\n",
    "#             x = context_vector if curr_iter == 1 else x\n",
    "#             output, state = self.rnn_model(x)  \n",
    "            \n",
    "        if self.combine_strategy == \"inject_par\":\n",
    "            \n",
    "            context_vector = tf.expand_dims(context_vector, 1)\n",
    "            context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "                        \n",
    "            x = self.custom_combine_layer(context_vector, x)\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output, state = self.rnn_model(x)              \n",
    "            \n",
    "        else: # merge (as default)\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output, state = self.rnn_model(x) \n",
    "            \n",
    "            context_vector = tf.expand_dims(context_vector, 1)\n",
    "            context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output = self.custom_combine_layer(context_vector, output)\n",
    "            \n",
    "        return output, state\n",
    "    \n",
    "    \n",
    "    def rnn_model(self, x, initial_state=None):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            \n",
    "            # adjust initial state, LSTM has 2 hidden states (h and c)\n",
    "            if initial_state is not None:\n",
    "                init_h = initial_state\n",
    "                init_c = tf.zeros(initial_state.shape)\n",
    "                initial_state = [init_h, init_c]\n",
    "            \n",
    "            output, h_state, c_state = self.lstm(x, initial_state=initial_state)\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            output, _, h_state, _, _ = self.bilstm(x)\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, h_state = self.gru(x, initial_state=initial_state)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, h_state\n",
    "    \n",
    "        \"\"\"\n",
    "        output => (batch_size, rnn_size)\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def custom_combine_layer(self, x, y):\n",
    "        if self.combine_layer == \"add\":\n",
    "            return self._add_layer(x, y)\n",
    "        else:\n",
    "            return self._concat_layer(x, y)\n",
    "\n",
    "        \n",
    "    def _add_layer(self, x, y):\n",
    "        \n",
    "        if x.shape[1] != y.shape[1] :\n",
    "            exception = \"Cannot combine using 'add' strategy, both tensor has different shape {} & {}\"\n",
    "            raise Exception(exception.format(x.shape, y.shape))\n",
    "            \n",
    "        return tf.keras.layers.add([x, y])\n",
    "            \n",
    "        \n",
    "    def _concat_layer(self, x, y):\n",
    "        return tf.concat([x, y], axis=-1)\n",
    "        \n",
    "    \n",
    "    def call(self, decoder_input, context_vector, position):\n",
    "        \"\"\" \n",
    "        decoder_input  : last predicted word => (batch_size, sequence len)\n",
    "        context_vector : image's vector      => (batch_size, img_context_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Mask out padding (0)\n",
    "#         decoder_input = self.mask(decoder_input)\n",
    "        \n",
    "        # x1 => (batch_size, input_sentence_len, embedding_dim + positional_dim)\n",
    "        x1 = self.embedding(decoder_input)\n",
    "#         x1_1 = self.embedding(decoder_input)\n",
    "#         x1_2 = self.positional_embedding(position)\n",
    "        \n",
    "#         x1_2 = tf.reduce_mean(x1_2, axis=1)\n",
    "#         x1_2 = tf.expand_dims(x1_2, axis=1)\n",
    "#         x1_2 = tf.tile(x1_2, multiples=[1, x1_1.shape[1], 1])\n",
    "        \n",
    "#         x1 = tf.concat([x1_1, x1_2], axis=-1)\n",
    "        \n",
    "        # x2 (concat) => (batch_size, input_sentence_len, embedding_dim + image_context_size)\n",
    "        # x2 (add) => (batch_size, embedding_dim)\n",
    "        x2, rnn_state = self.apply_strategy(x1, context_vector)\n",
    "        \n",
    "        ## ============================================\n",
    "        ## TODO: add another attention layer ? \n",
    "        ## ============================================\n",
    "        \n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        # x3 => (batch_size, sequence_len, rnn_units)\n",
    "        x3 = self.fc1(x2)   # how important is every sequence \n",
    "        x3 = self.batchnorm(x3)\n",
    "        x3 = self.leakyrelu(x3)\n",
    "        \n",
    "        # x4 => (batch_size, sequence_len * rnn_units) || was (batch_size * sequence_len=1, rnn_units)\n",
    "        x4 = tf.reshape(x3, (x3.shape[0], -1))\n",
    "\n",
    "        x4 = self.dropout(x4)\n",
    "        \n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x4)\n",
    "        \n",
    "        return word_predictions, rnn_state\n",
    "    \n",
    "\n",
    "    def reset_state(self, batch_size=None):\n",
    "        \n",
    "        if batch_size is not None:\n",
    "            return tf.zeros((batch_size, self.rnn_units))\n",
    "        \n",
    "        return tf.zeros((self.batch_size, self.rnn_units))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rnn_units': 256,\n",
       " 'rnn_type': 'BiLSTM',\n",
       " 'tokenizer': 'BERT',\n",
       " 'word_embedding': 'BERT',\n",
       " 'vocab_size': 3000,\n",
       " 'combine_strategy': 'merge',\n",
       " 'combine_layer': 'concat',\n",
       " 'image_context_size': 256,\n",
       " 'word_embedding_dim': 256,\n",
       " 'batch_size': 32,\n",
       " 'data_size': 5000,\n",
       " 'use_mapping': True,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_caption_length': 25,\n",
       " 'image_feature_extractor': 'xception',\n",
       " 'use_sequence': True,\n",
       " 'epoch': 20,\n",
       " 'use_mask': True,\n",
       " 'beam_search_k': 5,\n",
       " 'version': 'v5 : positional embedding'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0401 12:56:58.743880 140465195878208 configuration_utils.py:256] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/m13516112/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0401 12:56:58.745487 140465195878208 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0401 12:56:59.887520 140465195878208 modeling_tf_utils.py:333] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5 from cache at /home/m13516112/.cache/torch/transformers/d667df51ec24c20190f01fb4c20a21debc4c4fc12f7e2f5441ac0a99690e3ee9.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5\n",
      "I0401 12:57:03.947730 140465195878208 modeling_tf_utils.py:375] Layers from pretrained model not used in TFBertModel: ['mlm___cls', 'nsp___cls']\n"
     ]
    }
   ],
   "source": [
    "encoder = CNN_Encoder(\n",
    "    output_dim=PARAMS[\"image_context_size\"]\n",
    ")\n",
    "\n",
    "# attention = BahdanauAttention(\n",
    "#     units=PARAMS[\"rnn_units\"]\n",
    "# )\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    rnn_type=PARAMS[\"rnn_type\"], \n",
    "    rnn_units=PARAMS[\"rnn_units\"],\n",
    "    embedding_type=PARAMS[\"word_embedding\"], \n",
    "    embedding_dim=PARAMS[\"word_embedding_dim\"],  \n",
    "    combine_strategy=PARAMS[\"combine_strategy\"], \n",
    "    combine_layer=PARAMS[\"combine_layer\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    "    batch_size=PARAMS[\"batch_size\"]\n",
    ")\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Requirements\n",
    "\n",
    "# combine_strategy = \"inject_init\" : IMAGE_CONTEXT_SIZE == UNITS\n",
    "# combine_strategy = \"inject_pre\"  : IMAGE_CONTEXT_SIZE == WORD_EMBEDDING_DIM\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default feed forward function\n",
    "\n",
    "@tf.function\n",
    "def feed_forward(img_tensor, caption, position):\n",
    "    \"\"\"\n",
    "    img_tensor => (batch_size, image_feature_size, 2048)\n",
    "    caption => (batch_size, max_caption_length)\n",
    "    \"\"\"\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    # note : used to be decoder hidden state, \n",
    "    # but changed into current word embedding for paralel training\n",
    "    \n",
    "    # hidden => (batch_size, embedding_size)\n",
    "    hidden = decoder.embedding(caption, as_sentence=True)\n",
    "\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = encoder(img_tensor)\n",
    "\n",
    "    # context_vector => (batch_size, image_context_size)\n",
    "#     context_vector, _ = attention(features, hidden)\n",
    "    context_vector = features\n",
    "\n",
    "\n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions, _ = decoder(caption, context_vector, position)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, caption, target, position):\n",
    "    \"\"\"\n",
    "    target => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training model\n",
    "    with tf.GradientTape() as gradient_tape:\n",
    "        \n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions = feed_forward(img_tensor, caption, position)\n",
    "        \n",
    "        # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "        loss = loss_function(target, predictions)\n",
    "\n",
    "        \n",
    "    # Apply gradient\n",
    "    trainable_variables = encoder.trainable_variables + \\\n",
    "                          decoder.trainable_variables\n",
    "#                           attention.trainable_variables\n",
    "    \n",
    "    gradients = gradient_tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def eval_step(img_tensor, caption, target):\n",
    "    \n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions = feed_forward(img_tensor, caption)\n",
    "    \n",
    "    # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "    loss = loss_function(target, predictions)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def choose_predicted_id(predictions, strategy=\"max\", sampling_k=10):\n",
    "    \"\"\"\n",
    "    predictions : encoder word prediction => (batch_size, vocab_size)\n",
    "    strategy    : how to choose word [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sampling method (categorical dist)\n",
    "    if strategy == \"sample\":\n",
    "        \n",
    "        # sampled_proba & sampled_ids => (batch_size, sampling_k)\n",
    "        sampled_proba, sampled_ids = tf.math.top_k(predictions, sampling_k)\n",
    "        \n",
    "        # chosen_sampled_col => (batch_size, )\n",
    "        chosen_sampled_col = tf.squeeze(tf.random.categorical(sampled_proba, 1))\n",
    "        \n",
    "        # create row idx to zip with chosen_sampled_col\n",
    "        row_idx = tf.range(predictions.shape[0], dtype=chosen_sampled_col.dtype)\n",
    "        row_col_idx = tf.stack([row_idx, chosen_sampled_col], axis=1)\n",
    "        \n",
    "        # predicted_ids => (batch_size, )\n",
    "        predicted_ids = tf.gather_nd(sampled_ids, row_col_idx)\n",
    "\n",
    "    # Max index method\n",
    "    else:\n",
    "        predicted_ids = tf.argmax(predictions, 1)\n",
    "    \n",
    "    # predicted_ids => (batch_size, )\n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(images_paths):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [load_image(x)[0] for x in images_paths]\n",
    "    \n",
    "    # x => (batch_size, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, 10, 10, 2048)\n",
    "    x = extractor(x)\n",
    "    \n",
    "    # x  => (batch_size, img_feature_size, 2048)\n",
    "    x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = encoder(x)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def get_supporting_features(images_paths, strategy=\"mean\"):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, img_count, 1)\n",
    "    strategy : strategy to aggregate multiple supporting image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [[load_image(x)[0] for x in images_set] for images_set in images_paths]\n",
    "    \n",
    "    # x => (batch_size, img_count, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, img_count, 10, 10, 2048)\n",
    "    x = [extractor(image_set) for image_set in x]\n",
    "    \n",
    "    # features => (batch_size, img_count, img_feature_size, image_context_size)\n",
    "    features = encoder(x)\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    if strategy == \"logsumexp\":\n",
    "        features = tf.reduce_logsumexp(features, 1)\n",
    "    elif strategy == \"max\":\n",
    "        features = tf.reduce_max(features, 1)\n",
    "    elif strategy == \"min\":\n",
    "        features = tf.reduce_min(features, 1)\n",
    "    else:\n",
    "        features = tf.reduce_mean(features, 1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_indices(support_text):\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(0, len(support_text)):\n",
    "        context_token = target_tokenizer.tokenize(support_text[i])\n",
    "        context_token_id = target_tokenizer.convert_tokens_to_ids(context_token)\n",
    "        context_token_id = set(context_token_id)\n",
    "        context_token_id.discard(0)\n",
    "        for x in sorted(context_token_id):\n",
    "            indices.append([i, x])\n",
    "\n",
    "    # return => (word_count, 2)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_supporting_text_vector(support_text, vocab_size):\n",
    "    \"\"\"\n",
    "    support_text : list of text describing main image context => (batch_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(support_text)\n",
    "    \n",
    "    # indices => ( sum(batch_size * ?word_count), 2)\n",
    "    indices = get_one_hot_indices(support_text)\n",
    "    values = tf.ones(len(indices))\n",
    "    sparse_one_hot = tf.sparse.SparseTensor(indices, values, dense_shape=[batch_size, vocab_size])\n",
    "    \n",
    "    # sparse_one_hot => (batch_size, vocab_size)\n",
    "    return sparse_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluate(images_paths,\n",
    "                    support_text=None,\n",
    "                    support_imgs=None, \n",
    "                    support_aggregate_strategy=\"mean\",\n",
    "                    pplm_iteration=3,\n",
    "                    pplm_weight=0.03,\n",
    "                    pplm_gm_weight=0.8,\n",
    "                    choose_word_strategy=\"sample\",\n",
    "                    choose_word_sample_k=5,\n",
    "                   ):\n",
    "    \n",
    "    \"\"\"\n",
    "    images_paths : list of image_path                           => (batch_size, 1)\n",
    "    support_text : list of text describing main image context   => (batch_size)\n",
    "    support_imgs : list of list of image_path                   => (batch_size, image_count, 1)\n",
    "    support_aggregate_strategy : how to aggregate support image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    pplm_iteration : number of pplm step done for every decoding step\n",
    "    pplm_weight    : weight of pplm loss\n",
    "    pplm_gm_weight : geometric mean fusion weight (0 means use only original prediction, 1 means use only pplm prediction)\n",
    "    choose_word_strategy : how to choose word from prediction distribution [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(images_paths)\n",
    "    \n",
    "    # initialize captions placeholder\n",
    "    start_token = caption_tokenizer.convert_tokens_to_ids(['[CLS]']) # use bert id's not custom id\n",
    "    result_captions = tf.tile(tf.expand_dims(start_token, 1), [batch_size, 1])\n",
    "    attention_plot = tf.reshape([], shape=(batch_size, 0, IMAGE_FEATURE_SHAPE[0]))\n",
    "    \n",
    "    # Extract features from main images\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = get_image_features(images_paths)\n",
    "    \n",
    "    \n",
    "    if support_text is not None:\n",
    "        # support_text_vector => (batch_size, vocab_size)\n",
    "        support_text_vector = get_supporting_text_vector(support_text, decoder.vocab_size) \n",
    "    else:\n",
    "        # set all pplm related variable to 0\n",
    "        pplm_iteration = 0\n",
    "        pplm_weight = 0\n",
    "        pplm_gm_weight = 0\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(MAX_CAPTION_LENGTH)):\n",
    "        \n",
    "        # decoder_input => (batch_size, ~MAX_CAPTION_LENGTH)\n",
    "        caption = pad_sequences(result_captions, maxlen=MAX_CAPTION_LENGTH, padding=\"post\")\n",
    "        \n",
    "        # hidden => (batch_size, embedding_size)\n",
    "        hidden = decoder.embedding(caption, as_sentence=True)  ## BOTTLE\n",
    "\n",
    "        # context_vector => (batch_size, image_context_size)\n",
    "        # attention_weights => (batch_size, img_feature_size, 1)\n",
    "#         context_vector, attention_weights = attention(features, hidden)\n",
    "        context_vector = features\n",
    "\n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions, _ = decoder(caption, context_vector, position=None)  \n",
    "        \n",
    "        # ======================== PPLM section ========================\n",
    "        ori_prediction = predictions\n",
    "        \n",
    "        curr_pertubation = tf.Variable(tf.zeros((batch_size, decoder.embedding_dim)), name=\"curr_pertubation\", trainable=True)\n",
    "        \n",
    "        \n",
    "        for j in range(pplm_iteration):\n",
    "            \n",
    "            with tf.GradientTape() as pplm_tape: \n",
    "                \n",
    "                hidden += curr_pertubation\n",
    "                \n",
    "                context_vector, attention_weights = attention(features, hidden)\n",
    "                predictions, _ = decoder(caption, context_vector)\n",
    "                pplm_loss = pplm_loss_function(support_text_vector, predictions, pplm_weight=pplm_weight)\n",
    "                \n",
    "            \"\"\"\n",
    "            most impactfull layer to train = last dense layer\n",
    "            \"\"\"\n",
    "            \n",
    "            trainable_variables = [curr_pertubation]\n",
    "            gradients = pplm_tape.gradient(pplm_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "            \n",
    "            predictions, _ = decoder(caption, context_vector)\n",
    "        \n",
    "        # fuse final pplm_prediction and original prediction\n",
    "        fused_predictions = (predictions * pplm_gm_weight) + (ori_prediction * (1 - pplm_gm_weight)) \n",
    "        \n",
    "        # predicted_ids => (batch_size,)\n",
    "        predicted_ids = choose_predicted_id(fused_predictions, strategy=choose_word_strategy, sampling_k=choose_word_sample_k)\n",
    "        \n",
    "        # convert custom id mapping to bert's id\n",
    "        # predicted_bert_ids => (batch_size, 1)\n",
    "        predicted_bert_ids = target_tokenizer._convert_custom_id_to_bert_id(predicted_ids.numpy())\n",
    "        predicted_bert_ids = tf.expand_dims(predicted_bert_ids, 1)\n",
    "        \n",
    "        # store result\n",
    "        result_captions = tf.concat([result_captions, predicted_bert_ids], axis=1)\n",
    "        \n",
    "        # attention_weights => (batch_size, 1, img_feature_size)\n",
    "#         attention_weights = tf.reshape(attention_weights, shape=(batch_size, 1, -1))\n",
    "        \n",
    "        # assign attention weights to respective generated word\n",
    "        # attention_plot => (batch_size, ~max_caption_len, feature_size)\n",
    "#         attention_plot = tf.concat([attention_plot, attention_weights], axis=1)\n",
    "        \n",
    "    # remove start token & revert to tokens\n",
    "    result_captions = [caption_tokenizer.convert_ids_to_tokens(x[1:]) for x in result_captions]\n",
    "    \n",
    "    return result_captions, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy, MeanSquaredError\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def pplm_loss_function(real, pred, pplm_weight=0.03):\n",
    "    \"\"\"\n",
    "    real  => (batch_size, vocab_size)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "\n",
    "#     mm = tf.sparse.sparse_dense_matmul(real, tf.transpose(pred))\n",
    "#     mm = tf.reduce_sum(tf.abs(mm), 1)\n",
    "#     loss = tf.reduce_sum(mm, 0)\n",
    "    \n",
    "    real = tf.sparse.to_dense(real, default_value=0)\n",
    "    pplm_loss = CategoricalCrossentropy(from_logits=True)\n",
    "    loss = pplm_loss(real, pred, pplm_weight)\n",
    "    \n",
    "#     print(loss)\n",
    "    return loss\n",
    "\n",
    "    \"\"\"\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1395it [03:59,  5.82it/s]\n",
      "1395it [03:44,  6.22it/s]\n",
      "1395it [03:42,  6.27it/s]\n",
      "1395it [03:42,  6.28it/s]\n",
      "1395it [03:43,  6.24it/s]\n",
      "1395it [03:41,  6.30it/s]\n",
      "1395it [03:29,  6.67it/s]\n",
      "1395it [02:53,  8.06it/s]\n",
      "1395it [02:52,  8.11it/s]\n",
      "1395it [02:52,  8.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbd1d5495f8>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    for (img_tensor, captions, target, target_position) in tqdm(train_dataset):\n",
    "        batch_loss = train_step(img_tensor, captions, target, target_position)\n",
    "        loss.append(batch_loss.numpy())\n",
    "        \n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbd1d4d57b8>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.012968702316284"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-500:]) / len(loss[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.009140644454956"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-5000:]) / len(loss[-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:00<00:00, 536303.29it/s]\n"
     ]
    }
   ],
   "source": [
    "true_captions = {}\n",
    "for annot in tqdm(annotations[\"annotations\"]):\n",
    "    image_id = annot[\"image_id\"]\n",
    "\n",
    "    if image_id not in true_captions:\n",
    "        true_captions[image_id] = []\n",
    "    \n",
    "    true_captions[image_id].append(annot['caption'])\n",
    "    \n",
    "    \n",
    "def get_true_captions(image_paths):\n",
    "    return [true_captions[int(x[-10:-4])] for x in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:15<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "a a a a a a [PAD]\n",
      "\n",
      "true caps : \n",
      "A white toilet hanging off the side of a wall.\n",
      "a fancy bathroom with a lot of cabinet space around the toilet \n",
      "A small, clean, unoccupied bathroom on public transportation.\n",
      "The interior of a bathroom on an airplane\n",
      "A compact style bathroom with the toilet and sink in the wall along with cabinets on the wall.\n",
      "=====================================\n",
      "predicted : \n",
      "- [PAD] [CLS] a a\n",
      "\n",
      "true caps : \n",
      "A picture of a stoplight in the nighttime.\n",
      "Light traffic on a city street at dusk\n",
      "A red streetlight at a rainy city intersection\n",
      "Cars stopped at a stop light during the evening.\n",
      "a city street with cars and street lights at dusk\n",
      "=====================================\n",
      "predicted : \n",
      "a [PAD] a [PAD] , , ! ' , [PAD] a\n",
      "\n",
      "true caps : \n",
      "A white truck is passing by a large crowd.\n",
      "The truck is riding slowing down the street in the parade. \n",
      "A Parks and Recreation truck driving along the road as onlookers watch\n",
      "A white utility truck driving down a street.\n",
      "A white utility truck passing by building with people out front.\n",
      "=====================================\n",
      "predicted : \n",
      ", [CLS]\n",
      "\n",
      "true caps : \n",
      "A refrigerator cluttered with magnets,papers, and clippings. \n",
      "A white refrigerator freezer sitting on top of a kitchen floor.\n",
      "The refrigerator has a lot of things on it.\n",
      "White refrigerator with papers stuck to the side of it. \n",
      "A refrigerator has magnets and papers and food on top of it in the corner of a yellow tiled kitchen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[155:159]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_sample_k=10)\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = all_img_paths[155:159]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_strategy=\"max\")\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = all_img_paths[155:159]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_sample_k=5)\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train/{}\".format(str(datetime.now())[:-10])\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           attention=attention,\n",
    "                           optimizer=optimizer\n",
    "                          )\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------\")\n",
    "print(checkpoint_path)\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(checkpoint_path)\n",
    "with open(checkpoint_path + \"/config.txt\", \"w\") as f:\n",
    "    f.write(str(PARAMS))\n",
    "    \n",
    "log_file = open(checkpoint_path + \"/log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, PARAMS[\"epoch\"]):\n",
    "    \n",
    "    start = time.time()\n",
    "    loss = 0\n",
    "    batch = 1\n",
    "\n",
    "    for img_tensor, captions, target in tqdm(train_dataset):\n",
    "        \n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "\n",
    "            with open(checkpoint_path + \"/log.txt\", \"a\") as f:\n",
    "                log_message = ' {} Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                    checkpoint_path, epoch + 1, batch, batch_loss.numpy())\n",
    "                f.write(str(log_message + \"\\n\"))\n",
    "                print(log_message)   \n",
    "\n",
    "        batch += 1\n",
    "        \n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(batch_loss.numpy())\n",
    "\n",
    "#     ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = []\n",
    "\n",
    "# for (img_tensor, captions, target) in tqdm(train_dataset):\n",
    "#     batch_loss = train_step(img_tensor, captions, target)\n",
    "#     loss.append(batch_loss.numpy())\n",
    "    \n",
    "# plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PPLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "# text = [\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "# ]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images, support_text=text, pplm_iteration=5, pplm_weight=1)\n",
    "# result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
