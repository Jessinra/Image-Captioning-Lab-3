{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git add \"Keras.ipynb\"\n",
    "!git commit -m \"test pred using pplm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"rnn_units\": 256,\n",
    "    \"rnn_type\": \"BiLSTM\",\n",
    "    \"tokenizer\": \"BERT\",\n",
    "    \"word_embedding\": \"BERT\",\n",
    "    \"vocab_size\": 7000,\n",
    "    \"combine_strategy\": \"merge\",\n",
    "    \"combine_layer\": \"concat\",\n",
    "    \"image_context_size\": 256,\n",
    "    \"word_embedding_dim\": 256,\n",
    "    \"batch_size\": 32,\n",
    "    \"data_size\": 15000,\n",
    "    \"use_mapping\": True,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_caption_length\": 25, # use <int> or None\n",
    "    \"image_feature_extractor\": \"xception\",\n",
    "    \"use_sequence\": True,\n",
    "    \"epoch\": 20,\n",
    "    \"use_mask\": True,\n",
    "    \"version\": \"v5 : positional embedding\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "\n",
    "# for TFBertModel\n",
    "PROXIES = {\n",
    "  \"http\": \"http://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "  \"https\": \"https://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"[CLS] \" + annot['caption'] + \" [SEP]\"\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "\n",
    "stopper = -1 if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]\n",
    "train_captions = all_captions[:stopper]\n",
    "train_img_paths = all_img_paths[:stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 15000\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = len(train_captions) if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature_extractor(model_type=\"xception\"):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.compat.v1.keras.applications.xception\n",
    "        cnn_model = tf.compat.v1.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"CNN encoder model not supported yet\")\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "def get_image_feature_shape(model_type):\n",
    "    \n",
    "    if model_type == \"xception\":\n",
    "        return (100, 2048)\n",
    "    elif model_type == \"inception_v3\":\n",
    "        return (64, 2048)\n",
    "    else:\n",
    "        raise Exception (\"model unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = extractor_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FEATURE_SHAPE = get_image_feature_shape(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extractor, extractor_preprocessor = get_image_feature_extractor(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image feature\n",
    "\n",
    "this step mostly skipped since the results are cached already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 469.75\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = PARAMS[\"batch_size\"]\n",
    "\n",
    "estimated_batch_count = DATA_SIZE / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique images\n",
    "# unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# # Prepare dataset\n",
    "# image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "# image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "# image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessed image (batch)\n",
    "\n",
    "# for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "#     # get context vector of batch images\n",
    "#     batch_features = extractor(batch_imgs)\n",
    "    \n",
    "#     # flatten 2D cnn result into 1D for RNN decoder input\n",
    "#     # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "#     # image_feature = 100 (Xception)\n",
    "#     # image_feature = 64 (Inception V3)\n",
    "#     batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "#     # Cache preprocessed image\n",
    "#     for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "#         image_path = image_path.numpy().decode(\"utf-8\")\n",
    "#         np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Caption dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 16:16:29.170826 140130036610880 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0330 16:16:29.171897 140130036610880 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = PARAMS[\"tokenizer\"]\n",
    "VOCAB_SIZE = PARAMS[\"vocab_size\"]  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "    \n",
    "    def use_custom_mapping(self, use_mapping=True, vocab_size=3000):\n",
    "        \n",
    "        self.use_mapping = use_mapping\n",
    "        self.cust_vocab_size = vocab_size\n",
    "        self.mapping_initialized = False\n",
    "\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into custom indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        bert_ids = [self.convert_tokens_to_ids(self.tokenize(x)) for x in tqdm(texts)]\n",
    "        \n",
    "        if not self.use_mapping:\n",
    "            return bert_ids\n",
    "        \n",
    "        if not self.mapping_initialized:\n",
    "            self._initialize_custom_mapping(bert_ids)\n",
    "            return [self._convert_bert_id_to_custom_id(x) for x in tqdm(bert_ids)]\n",
    "        \n",
    "        return bert_ids\n",
    "    \n",
    "        \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \n",
    "        bert_ids = super().convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            return self._convert_bert_id_to_custom_id(bert_ids)\n",
    "        else:\n",
    "            return bert_ids\n",
    "        \n",
    "        \n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            bert_ids = self._convert_custom_id_to_bert_id(token_ids)\n",
    "        else:\n",
    "            bert_ids = token_ids\n",
    "            \n",
    "        bert_tokens = super().convert_ids_to_tokens(bert_ids)\n",
    "        return bert_tokens\n",
    "    \n",
    "    \n",
    "    def _initialize_custom_mapping(self, corpus_bert_ids):\n",
    "        \n",
    "        print(\"    > constructing custom mapping < \\n\")\n",
    "        self._build_occurence_table(corpus_bert_ids)\n",
    "        self._build_custom_mapping_table()\n",
    "        self.mapping_initialized = True\n",
    "        \n",
    "        \n",
    "    def _build_occurence_table(self, tokenized_captions):\n",
    "        \"\"\"\n",
    "        build dict of token frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        self.occurence_table = {}\n",
    "        for caption in tqdm(tokenized_captions):\n",
    "            for token in caption:\n",
    "                if token not in self.occurence_table:\n",
    "                    self.occurence_table[token] = 0\n",
    "                self.occurence_table[token] += 1\n",
    "                \n",
    "    \n",
    "    def _build_custom_mapping_table(self):\n",
    "        \n",
    "        _special_token = ['[UNK]', '[PAD]']\n",
    "        _actual_vocab_size = self.cust_vocab_size - len(_special_token)\n",
    "        \n",
    "        sorted_occurence = {k: v for k, v in sorted(\n",
    "            self.occurence_table.items(), reverse=True, key=lambda item: item[1]\n",
    "        )}\n",
    "        \n",
    "        used_tokens = sorted(list(sorted_occurence)[:_actual_vocab_size])\n",
    "        mapping_size = min(len(used_tokens), _actual_vocab_size)\n",
    "        \n",
    "        _bert_pad = 0\n",
    "        _bert_oov = 100\n",
    "        self._custom_pad = 0\n",
    "        self._custom_oov = mapping_size + 1\n",
    "        \n",
    "        self.bert_id_to_custom_id = {\n",
    "            _bert_pad: self._custom_pad, \n",
    "            _bert_oov: self._custom_oov\n",
    "        }\n",
    "        self.custom_id_to_bert_id = {\n",
    "            self._custom_pad: _bert_pad, \n",
    "            self._custom_oov: _bert_oov\n",
    "        }\n",
    "        \n",
    "        for i in range(0, mapping_size):\n",
    "            bert_token = used_tokens[i]\n",
    "            self.bert_id_to_custom_id[bert_token] = i + 1    \n",
    "            self.custom_id_to_bert_id[i + 1] = bert_token\n",
    "            \n",
    "        print(\"Vocab contains {0} / {1} unique tokens ({2:.2f} %)\".format(\n",
    "            len(used_tokens) + 2,\\\n",
    "            len(sorted_occurence),\\\n",
    "            (len(used_tokens) / len(sorted_occurence) * 100)\n",
    "        ))\n",
    "        \n",
    "        sorted_occurence_count = list(sorted_occurence.values())\n",
    "        used_tokens_count = sum(sorted_occurence_count[:_actual_vocab_size])\n",
    "        total_tokens_count = sum(sorted_occurence_count)\n",
    "        \n",
    "        print(\"Using {0} / {1} tokens available ({2:.2f} %)\".format(\n",
    "            used_tokens_count,\\\n",
    "            total_tokens_count,\\\n",
    "            (used_tokens_count / total_tokens_count * 100)\n",
    "        ))        \n",
    "        \n",
    "    def _convert_bert_id_to_custom_id(self, token_ids):\n",
    "        return [self.bert_id_to_custom_id[x] if x in self.bert_id_to_custom_id else self._custom_oov for x in token_ids]\n",
    "    \n",
    "    def _convert_custom_id_to_bert_id(self, token_ids):\n",
    "        return [self.custom_id_to_bert_id[x] for x in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_type, use_mapping, vocab_size):\n",
    "    \n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    if tokenizer_type == \"BERT\" :\n",
    "        tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "        tokenizer.use_custom_mapping(use_mapping, vocab_size)\n",
    "\n",
    "    # use default keras tokenizer\n",
    "    else : \n",
    "        tokenizer = TokenizerWrapper(num_words=vocab_size, oov_token=\"[UNK]\")\n",
    "        tokenizer.fit_on_texts(train_captions)    \n",
    "        tokenizer.word_index['[PAD]'] = 0\n",
    "        tokenizer.index_word[0] = '[PAD]'\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 16:16:30.967753 140130036610880 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0330 16:16:32.230451 140130036610880 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "caption_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=False,\n",
    "    vocab_size=0\n",
    ")\n",
    "\n",
    "target_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=PARAMS[\"use_mapping\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:03<00:00, 4178.78it/s]\n",
      "100%|██████████| 15000/15000 [00:03<00:00, 4185.59it/s]\n",
      "100%|██████████| 15000/15000 [00:00<00:00, 397679.96it/s]\n",
      "100%|██████████| 15000/15000 [00:00<00:00, 379208.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > constructing custom mapping < \n",
      "\n",
      "Vocab contains 5723 / 5721 unique tokens (100.00 %)\n",
      "Using 206900 / 206900 tokens available (100.00 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "caption_tokens = caption_tokenizer.texts_to_sequences(train_captions)\n",
    "target_tokens = target_tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parallel dataset\n",
    "\n",
    "prepare captions and target to support parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parallel_dataset(img_paths, caption_tokens, target_tokens):\n",
    "    \n",
    "    dataset_img_paths = []\n",
    "    dataset_captions = []\n",
    "    dataset_target = []\n",
    "    dataset_target_position = []\n",
    "\n",
    "    for i in tqdm(range(0, len(train_img_paths))):\n",
    "        img = img_paths[i]\n",
    "        cap = caption_tokens[i]\n",
    "        tar = target_tokens[i]\n",
    "\n",
    "        for j in range(1, len(cap)):\n",
    "            dataset_img_paths.append(img)\n",
    "            dataset_captions.append(cap[:j])\n",
    "            dataset_target.append(tar[j])\n",
    "            dataset_target_position.append(j)\n",
    "\n",
    "    dataset_captions = pad_sequences(dataset_captions, maxlen=PARAMS[\"max_caption_length\"], padding='post')\n",
    "    \n",
    "    return dataset_img_paths, dataset_captions, dataset_target, dataset_target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:00<00:00, 34890.95it/s]\n"
     ]
    }
   ],
   "source": [
    "parallel_img_paths, parallel_captions, parallel_target, parallel_target_position = preprocess_parallel_dataset(train_img_paths, caption_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = parallel_captions.shape[0]\n",
    "MAX_CAPTION_LENGTH = parallel_captions.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_target_position = tf.one_hot(parallel_target_position, depth=MAX_CAPTION_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "\n",
    "def load_dataset(img_name, caption, target, target_pos):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, caption, target, target_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((parallel_img_paths, parallel_captions, parallel_target, parallel_target_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "\n",
    "dataset = dataset.map(lambda item1, item2, item3, item4: tf.numpy_function(\n",
    "          load_dataset, [item1, item2, item3, item4], [tf.float32, tf.int32, tf.int32, tf.float32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(DATA_SIZE / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 16, image_feature = 100, 2048)\n",
    "# caption => (batch_size = 16, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4197 batches, (total : 134304)\n",
      "eval : 899 batches, (total : 28768)\n",
      "test : 901 batches, (total : 28832 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captioning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=PARAMS[\"learning_rate\"])\n",
    "# loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    real  => (batch_size,)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate loss\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # create mask to filter out padding token \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    # Ignore loss_ if real token is padding\n",
    "    loss_ *= mask\n",
    "    \n",
    "    # Get mean of curren batch's loss (somewhat batch norm)\n",
    "    result_loss = tf.reduce_mean(loss_)\n",
    "    \n",
    "    return result_loss\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    loss_  => (batch_size, 1)\n",
    "    mask   => (batch_size, 1)  : indicate is padding or not\n",
    "\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim)\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # x => (batch_size, 100, 2048)\n",
    "        x = self.fc(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, image_feature_size, image_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, img_feature_size, image_context_size)\n",
    "        hidden                        => (batch_size, embedding_size)\n",
    "        \n",
    "        note : \n",
    "        img_feature_size ==  64 for Inception V3,\n",
    "        img_feature_size == 100 for Xception,\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        _w1 = self.W1(features)\n",
    "        _w2 = self.W2(hidden_with_time_axis)\n",
    "        score = tf.nn.tanh(_w1 + _w2)\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_with_time_axis      => (batch_size, 1, embedding_size)\n",
    "        score                      => (batch_size, img_feature_size, units)\n",
    "        attention_weights          => (batch_size, img_feature_size, 1)\n",
    "        context_vector (after sum) => (batch_size, img_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=256, \n",
    "                 embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 combine_strategy=\"merge\", combine_layer=\"concat\",\n",
    "                 vocab_size=3000, batch_size=32):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # when to use context_vector [\"inject_init\", \"inject_pre\", \"inject_par\", \"merge\"]\n",
    "        self.combine_strategy = combine_strategy\n",
    "        \n",
    "        # how to use context_vector [\"add\", \"concat\"]\n",
    "        self.combine_layer = combine_layer\n",
    "        \n",
    "        # =====================================================\n",
    "        \n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "        \n",
    "#         self.mask = Masking(mask_value=0, dtype=\"int32\")\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "        self.positional_embedding = Embedding(\n",
    "            input_dim=MAX_CAPTION_LENGTH,\n",
    "            output_dim=16,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "        \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\") # same size as vocab\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "           \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\") # same size as vocab\n",
    "        \n",
    "        \n",
    "    def _init_embedding(self):\n",
    "        \n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.bert_embedding = TFBertModel.from_pretrained('bert-base-uncased', proxies=PROXIES)\n",
    "            self.bert_embedding.trainable = False\n",
    "            self.embedding_dim = self.bert_embedding.config.hidden_size\n",
    "            \n",
    "        else:\n",
    "            self.default_embedding = Embedding(\n",
    "                input_dim=self.vocab_size, \n",
    "                output_dim=self.embedding_dim, \n",
    "                mask_zero=True,\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def _init_rnn(self):\n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector'\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            self.bilstm = Bidirectional(LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        \n",
    "    def embedding(self, tokens, as_sentence=False):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embedding_type == \"BERT\": \n",
    "            embedding, sentence_embedding = self._bert_embedding(tokens)\n",
    "        \n",
    "        else:\n",
    "            embedding = self._default_embedding(tokens)\n",
    "            sentence_embedding = tf.reduce_mean(embedding, 1)\n",
    "            \n",
    "\n",
    "        if as_sentence:\n",
    "            # embedding => (batch_size, embedding_dim)\n",
    "            return sentence_embedding\n",
    "        \n",
    "        else:\n",
    "            # embedding => (batch_size, tokens_length, embedding_dim)\n",
    "            return embedding\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        embedding (tokens)   => (batch_size, tokens_length, embedding_dim)\n",
    "        embedding (sentence) => (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def _bert_embedding(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        # mask out attention from padding\n",
    "        # mask => (batch_size, sequence_length)\n",
    "        attention_mask = x == 0\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, attention_mask=attention_mask)\n",
    "        \"\"\"\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        token_type_ids = tf.cast((x == 0), tf.int32)\n",
    "        \n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, token_type_ids=token_type_ids)\n",
    "        return hidden_states, sentence_embedding\n",
    "    \n",
    "        \"\"\"\n",
    "        hidden states contains hidden state for each word\n",
    "        sentence_embedding is general embedding for whole sentences\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def _default_embedding(self, x):\n",
    "        \n",
    "        # embedding => (batch_size, sequence_length, embedding_size)\n",
    "        embedding = self.default_embedding(x)\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "    def apply_strategy(self, x, context_vector, curr_iter=-1):\n",
    "        \"\"\"\n",
    "        context_vector : image's vector\n",
    "        x              : rnn input (word embedding)\n",
    "        strategy       : \n",
    "        curr_iter      : current iteration number\n",
    "        \n",
    "        No longer support inject_init & inject_pre, since training become fully parallel\n",
    "        \"\"\"\n",
    "        \n",
    "#         if self.combine_strategy == \"inject_init\":\n",
    "#             initial_state = tf.squeeze(context_vector) if curr_iter == 1 else None\n",
    "#             output, state = self.rnn_model(x, initial_state=initial_state)  \n",
    "            \n",
    "#         elif self.combine_strategy == \"inject_pre\":\n",
    "#             x = context_vector if curr_iter == 1 else x\n",
    "#             output, state = self.rnn_model(x)  \n",
    "            \n",
    "        if self.combine_strategy == \"inject_par\":\n",
    "            \n",
    "            context_vector = tf.expand_dims(context_vector, 1)\n",
    "            context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "                        \n",
    "            x = self.custom_combine_layer(context_vector, x)\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output, state = self.rnn_model(x)              \n",
    "            \n",
    "        else: # merge (as default)\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output, state = self.rnn_model(x) \n",
    "            \n",
    "            context_vector = tf.expand_dims(context_vector, 1)\n",
    "            context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output = self.custom_combine_layer(context_vector, output)\n",
    "            \n",
    "        return output, state\n",
    "    \n",
    "    \n",
    "    def rnn_model(self, x, initial_state=None):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            \n",
    "            # adjust initial state, LSTM has 2 hidden states (h and c)\n",
    "            if initial_state is not None:\n",
    "                init_h = initial_state\n",
    "                init_c = tf.zeros(initial_state.shape)\n",
    "                initial_state = [init_h, init_c]\n",
    "            \n",
    "            output, h_state, c_state = self.lstm(x, initial_state=initial_state)\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            output, _, h_state, _, _ = self.bilstm(x)\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, h_state = self.gru(x, initial_state=initial_state)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, h_state\n",
    "    \n",
    "        \"\"\"\n",
    "        output => (batch_size, rnn_size)\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def custom_combine_layer(self, x, y):\n",
    "        if self.combine_layer == \"add\":\n",
    "            return self._add_layer(x, y)\n",
    "        else:\n",
    "            return self._concat_layer(x, y)\n",
    "\n",
    "        \n",
    "    def _add_layer(self, x, y):\n",
    "        \n",
    "        if x.shape[1] != y.shape[1] :\n",
    "            exception = \"Cannot combine using 'add' strategy, both tensor has different shape {} & {}\"\n",
    "            raise Exception(exception.format(x.shape, y.shape))\n",
    "            \n",
    "        return tf.keras.layers.add([x, y])\n",
    "            \n",
    "        \n",
    "    def _concat_layer(self, x, y):\n",
    "        return tf.concat([x, y], axis=-1)\n",
    "        \n",
    "    \n",
    "    def call(self, decoder_input, context_vector, position):\n",
    "        \"\"\" \n",
    "        decoder_input  : last predicted word => (batch_size, sequence len)\n",
    "        context_vector : image's vector      => (batch_size, img_context_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Mask out padding (0)\n",
    "#         decoder_input = self.mask(decoder_input)\n",
    "        \n",
    "        # x1 => (batch_size, input_sentence_len, embedding_dim + positional_dim)\n",
    "        x1 = self.embedding(decoder_input)\n",
    "#         x1_1 = self.embedding(decoder_input)\n",
    "#         x1_2 = self.positional_embedding(position)\n",
    "        \n",
    "#         x1_2 = tf.reduce_mean(x1_2, axis=1)\n",
    "#         x1_2 = tf.expand_dims(x1_2, axis=1)\n",
    "#         x1_2 = tf.tile(x1_2, multiples=[1, x1_1.shape[1], 1])\n",
    "        \n",
    "#         x1 = tf.concat([x1_1, x1_2], axis=-1)\n",
    "        \n",
    "        # x2 (concat) => (batch_size, input_sentence_len, embedding_dim + image_context_size)\n",
    "        # x2 (add) => (batch_size, embedding_dim)\n",
    "        x2, rnn_state = self.apply_strategy(x1, context_vector)\n",
    "        \n",
    "        ## ============================================\n",
    "        ## TODO: add another attention layer ? \n",
    "        ## ============================================\n",
    "        \n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        # x3 => (batch_size, sequence_len, rnn_units)\n",
    "        x3 = self.fc1(x2)   # how important is every sequence \n",
    "        x3 = self.batchnorm(x3)\n",
    "        x3 = self.leakyrelu(x3)\n",
    "        \n",
    "        # x4 => (batch_size, sequence_len * rnn_units) || was (batch_size * sequence_len=1, rnn_units)\n",
    "        x4 = tf.reshape(x3, (x3.shape[0], -1))\n",
    "\n",
    "        x4 = self.dropout(x4)\n",
    "        \n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x4)\n",
    "        \n",
    "        return word_predictions, rnn_state\n",
    "    \n",
    "\n",
    "    def reset_state(self, batch_size=None):\n",
    "        \n",
    "        if batch_size is not None:\n",
    "            return tf.zeros((batch_size, self.rnn_units))\n",
    "        \n",
    "        return tf.zeros((self.batch_size, self.rnn_units))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rnn_units': 256,\n",
       " 'rnn_type': 'BiLSTM',\n",
       " 'tokenizer': 'BERT',\n",
       " 'word_embedding': 'BERT',\n",
       " 'vocab_size': 7000,\n",
       " 'combine_strategy': 'merge',\n",
       " 'combine_layer': 'concat',\n",
       " 'image_context_size': 256,\n",
       " 'word_embedding_dim': 256,\n",
       " 'batch_size': 32,\n",
       " 'data_size': 15000,\n",
       " 'use_mapping': True,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_caption_length': 25,\n",
       " 'image_feature_extractor': 'xception',\n",
       " 'use_sequence': True,\n",
       " 'epoch': 20,\n",
       " 'use_mask': True,\n",
       " 'version': 'v5 : positional embedding'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 16:16:44.545381 140130036610880 configuration_utils.py:256] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/m13516112/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0330 16:16:44.547157 140130036610880 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0330 16:16:45.729253 140130036610880 modeling_tf_utils.py:333] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5 from cache at /home/m13516112/.cache/torch/transformers/d667df51ec24c20190f01fb4c20a21debc4c4fc12f7e2f5441ac0a99690e3ee9.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5\n",
      "I0330 16:16:46.963063 140130036610880 modeling_tf_utils.py:375] Layers from pretrained model not used in TFBertModel: ['mlm___cls', 'nsp___cls']\n"
     ]
    }
   ],
   "source": [
    "encoder = CNN_Encoder(\n",
    "    output_dim=PARAMS[\"image_context_size\"]\n",
    ")\n",
    "\n",
    "attention = BahdanauAttention(\n",
    "    units=PARAMS[\"rnn_units\"]\n",
    ")\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    rnn_type=PARAMS[\"rnn_type\"], \n",
    "    rnn_units=PARAMS[\"rnn_units\"],\n",
    "    embedding_type=PARAMS[\"word_embedding\"], \n",
    "    embedding_dim=PARAMS[\"word_embedding_dim\"],  \n",
    "    combine_strategy=PARAMS[\"combine_strategy\"], \n",
    "    combine_layer=PARAMS[\"combine_layer\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    "    batch_size=PARAMS[\"batch_size\"]\n",
    ")\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Requirements\n",
    "\n",
    "# combine_strategy = \"inject_init\" : IMAGE_CONTEXT_SIZE == UNITS\n",
    "# combine_strategy = \"inject_pre\"  : IMAGE_CONTEXT_SIZE == WORD_EMBEDDING_DIM\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default feed forward function\n",
    "\n",
    "@tf.function\n",
    "def feed_forward(img_tensor, caption, position):\n",
    "    \"\"\"\n",
    "    img_tensor => (batch_size, image_feature_size, 2048)\n",
    "    caption => (batch_size, max_caption_length)\n",
    "    \"\"\"\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    # note : used to be decoder hidden state, \n",
    "    # but changed into current word embedding for paralel training\n",
    "    \n",
    "    # hidden => (batch_size, embedding_size)\n",
    "    hidden = decoder.embedding(caption, as_sentence=True)\n",
    "\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = encoder(img_tensor)\n",
    "\n",
    "    # context_vector => (batch_size, image_context_size)\n",
    "    context_vector, _ = attention(features, hidden)\n",
    "\n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions, _ = decoder(caption, context_vector, position)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, caption, target, position):\n",
    "    \"\"\"\n",
    "    target => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training model\n",
    "    with tf.GradientTape() as gradient_tape:\n",
    "        \n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions = feed_forward(img_tensor, caption, position)\n",
    "        \n",
    "        # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "        loss = loss_function(target, predictions)\n",
    "\n",
    "        \n",
    "    # Apply gradient\n",
    "    trainable_variables = encoder.trainable_variables + \\\n",
    "                          decoder.trainable_variables + \\\n",
    "                          attention.trainable_variables\n",
    "    \n",
    "    gradients = gradient_tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def eval_step(img_tensor, caption, target):\n",
    "    \n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions = feed_forward(img_tensor, caption)\n",
    "    \n",
    "    # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "    loss = loss_function(target, predictions)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def choose_predicted_id(predictions, strategy=\"max\", sampling_k=10):\n",
    "    \"\"\"\n",
    "    predictions : encoder word prediction => (batch_size, vocab_size)\n",
    "    strategy    : how to choose word [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sampling method (categorical dist)\n",
    "    if strategy == \"sample\":\n",
    "        \n",
    "        # sampled_proba & sampled_ids => (batch_size, sampling_k)\n",
    "        sampled_proba, sampled_ids = tf.math.top_k(predictions, sampling_k)\n",
    "        \n",
    "        # chosen_sampled_col => (batch_size, )\n",
    "        chosen_sampled_col = tf.squeeze(tf.random.categorical(sampled_proba, 1))\n",
    "        \n",
    "        # create row idx to zip with chosen_sampled_col\n",
    "        row_idx = tf.range(predictions.shape[0], dtype=chosen_sampled_col.dtype)\n",
    "        row_col_idx = tf.stack([row_idx, chosen_sampled_col], axis=1)\n",
    "        \n",
    "        # predicted_ids => (batch_size, )\n",
    "        predicted_ids = tf.gather_nd(sampled_ids, row_col_idx)\n",
    "\n",
    "    # Max index method\n",
    "    else:\n",
    "        predicted_ids = tf.argmax(predictions, 1)\n",
    "    \n",
    "    # predicted_ids => (batch_size, )\n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(images_paths):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [load_image(x)[0] for x in images_paths]\n",
    "    \n",
    "    # x => (batch_size, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, 10, 10, 2048)\n",
    "    x = extractor(x)\n",
    "    \n",
    "    # x  => (batch_size, img_feature_size, 2048)\n",
    "    x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = encoder(x)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def get_supporting_features(images_paths, strategy=\"mean\"):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, img_count, 1)\n",
    "    strategy : strategy to aggregate multiple supporting image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [[load_image(x)[0] for x in images_set] for images_set in images_paths]\n",
    "    \n",
    "    # x => (batch_size, img_count, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, img_count, 10, 10, 2048)\n",
    "    x = [extractor(image_set) for image_set in x]\n",
    "    \n",
    "    # features => (batch_size, img_count, img_feature_size, image_context_size)\n",
    "    features = encoder(x)\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    if strategy == \"logsumexp\":\n",
    "        features = tf.reduce_logsumexp(features, 1)\n",
    "    elif strategy == \"max\":\n",
    "        features = tf.reduce_max(features, 1)\n",
    "    elif strategy == \"min\":\n",
    "        features = tf.reduce_min(features, 1)\n",
    "    else:\n",
    "        features = tf.reduce_mean(features, 1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_indices(support_text):\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(0, len(support_text)):\n",
    "        context_token = target_tokenizer.tokenize(support_text[i])\n",
    "        context_token_id = target_tokenizer.convert_tokens_to_ids(context_token)\n",
    "        context_token_id = set(context_token_id)\n",
    "        context_token_id.discard(0)\n",
    "        for x in sorted(context_token_id):\n",
    "            indices.append([i, x])\n",
    "\n",
    "    # return => (word_count, 2)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_supporting_text_vector(support_text, vocab_size):\n",
    "    \"\"\"\n",
    "    support_text : list of text describing main image context => (batch_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(support_text)\n",
    "    \n",
    "    # indices => ( sum(batch_size * ?word_count), 2)\n",
    "    indices = get_one_hot_indices(support_text)\n",
    "    values = tf.ones(len(indices))\n",
    "    sparse_one_hot = tf.sparse.SparseTensor(indices, values, dense_shape=[batch_size, vocab_size])\n",
    "    \n",
    "    # sparse_one_hot => (batch_size, vocab_size)\n",
    "    return sparse_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluate(images_paths,\n",
    "                    support_text=None,\n",
    "                    support_imgs=None, \n",
    "                    support_aggregate_strategy=\"mean\",\n",
    "                    pplm_iteration=3,\n",
    "                    pplm_weight=0.03,\n",
    "                    pplm_gm_weight=0.8,\n",
    "                    choose_word_strategy=\"sample\",\n",
    "                   ):\n",
    "    \n",
    "    \"\"\"\n",
    "    images_paths : list of image_path                           => (batch_size, 1)\n",
    "    support_text : list of text describing main image context   => (batch_size)\n",
    "    support_imgs : list of list of image_path                   => (batch_size, image_count, 1)\n",
    "    support_aggregate_strategy : how to aggregate support image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    pplm_iteration : number of pplm step done for every decoding step\n",
    "    pplm_weight    : weight of pplm loss\n",
    "    pplm_gm_weight : geometric mean fusion weight (0 means use only original prediction, 1 means use only pplm prediction)\n",
    "    choose_word_strategy : how to choose word from prediction distribution [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(images_paths)\n",
    "    \n",
    "    # initialize captions placeholder\n",
    "    start_token = caption_tokenizer.convert_tokens_to_ids(['[CLS]']) # use bert id's not custom id\n",
    "    result_captions = tf.tile(tf.expand_dims(start_token, 1), [batch_size, 1])\n",
    "    attention_plot = tf.reshape([], shape=(batch_size, 0, IMAGE_FEATURE_SHAPE[0]))\n",
    "    \n",
    "    # Extract features from main images\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = get_image_features(images_paths)\n",
    "    \n",
    "    \n",
    "    if support_text is not None:\n",
    "        # support_text_vector => (batch_size, vocab_size)\n",
    "        support_text_vector = get_supporting_text_vector(support_text, decoder.vocab_size) \n",
    "    else:\n",
    "        # set all pplm related variable to 0\n",
    "        pplm_iteration = 0\n",
    "        pplm_weight = 0\n",
    "        pplm_gm_weight = 0\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(MAX_CAPTION_LENGTH)):\n",
    "        \n",
    "        # decoder_input => (batch_size, ~MAX_CAPTION_LENGTH)\n",
    "        caption = pad_sequences(result_captions, maxlen=MAX_CAPTION_LENGTH, padding=\"post\")\n",
    "        \n",
    "        # hidden => (batch_size, embedding_size)\n",
    "        hidden = decoder.embedding(caption, as_sentence=True)  ## BOTTLE\n",
    "\n",
    "        # context_vector => (batch_size, image_context_size)\n",
    "        # attention_weights => (batch_size, img_feature_size, 1)\n",
    "        context_vector, attention_weights = attention(features, hidden)\n",
    "\n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions, _ = decoder(caption, context_vector, position=None)  \n",
    "        \n",
    "        # ======================== PPLM section ========================\n",
    "        ori_prediction = predictions\n",
    "        \n",
    "        curr_pertubation = tf.Variable(tf.zeros((batch_size, decoder.embedding_dim)), name=\"curr_pertubation\", trainable=True)\n",
    "        \n",
    "        \n",
    "        for j in range(pplm_iteration):\n",
    "            \n",
    "            with tf.GradientTape() as pplm_tape: \n",
    "                \n",
    "                hidden += curr_pertubation\n",
    "                \n",
    "                context_vector, attention_weights = attention(features, hidden)\n",
    "                predictions, _ = decoder(caption, context_vector)\n",
    "                pplm_loss = pplm_loss_function(support_text_vector, predictions, pplm_weight=pplm_weight)\n",
    "                \n",
    "            \"\"\"\n",
    "            most impactfull layer to train = last dense layer\n",
    "            \"\"\"\n",
    "            \n",
    "            trainable_variables = [curr_pertubation]\n",
    "            gradients = pplm_tape.gradient(pplm_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "            \n",
    "            predictions, _ = decoder(caption, context_vector)\n",
    "        \n",
    "        # fuse final pplm_prediction and original prediction\n",
    "        fused_predictions = (predictions * pplm_gm_weight) + (ori_prediction * (1 - pplm_gm_weight)) \n",
    "        \n",
    "        # predicted_ids => (batch_size,)\n",
    "        predicted_ids = choose_predicted_id(fused_predictions, strategy=choose_word_strategy)\n",
    "        \n",
    "        # convert custom id mapping to bert's id\n",
    "        # predicted_bert_ids => (batch_size, 1)\n",
    "        predicted_bert_ids = target_tokenizer._convert_custom_id_to_bert_id(predicted_ids.numpy())\n",
    "        predicted_bert_ids = tf.expand_dims(predicted_bert_ids, 1)\n",
    "        \n",
    "        # store result\n",
    "        result_captions = tf.concat([result_captions, predicted_bert_ids], axis=1)\n",
    "        \n",
    "        # attention_weights => (batch_size, 1, img_feature_size)\n",
    "        attention_weights = tf.reshape(attention_weights, shape=(batch_size, 1, -1))\n",
    "        \n",
    "        # assign attention weights to respective generated word\n",
    "        # attention_plot => (batch_size, ~max_caption_len, feature_size)\n",
    "        attention_plot = tf.concat([attention_plot, attention_weights], axis=1)\n",
    "        \n",
    "    # remove start token & revert to tokens\n",
    "    result_captions = [caption_tokenizer.convert_ids_to_tokens(x[1:]) for x in result_captions]\n",
    "    \n",
    "    return result_captions, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy, MeanSquaredError\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def pplm_loss_function(real, pred, pplm_weight=0.03):\n",
    "    \"\"\"\n",
    "    real  => (batch_size, vocab_size)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "\n",
    "#     mm = tf.sparse.sparse_dense_matmul(real, tf.transpose(pred))\n",
    "#     mm = tf.reduce_sum(tf.abs(mm), 1)\n",
    "#     loss = tf.reduce_sum(mm, 0)\n",
    "    \n",
    "    real = tf.sparse.to_dense(real, default_value=0)\n",
    "    pplm_loss = CategoricalCrossentropy(from_logits=True)\n",
    "    loss = pplm_loss(real, pred, pplm_weight)\n",
    "    \n",
    "#     print(loss)\n",
    "    return loss\n",
    "\n",
    "    \"\"\"\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4197it [07:32,  9.28it/s]\n",
      "4197it [10:03,  6.96it/s]\n",
      "4197it [10:28,  6.68it/s]\n",
      "4197it [10:31,  6.65it/s]\n",
      "4197it [09:44,  7.18it/s]\n",
      "4197it [06:01, 11.62it/s]\n",
      "4197it [06:01, 11.60it/s]\n",
      "4197it [06:01, 11.62it/s]\n",
      "4197it [06:02, 11.59it/s]\n",
      "4197it [06:05, 11.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6ed1f174a8>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    for (img_tensor, captions, target, target_position) in tqdm(train_dataset):\n",
    "        batch_loss = train_step(img_tensor, captions, target, target_position)\n",
    "        loss.append(batch_loss.numpy())\n",
    "        \n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6ed0688b00>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.706183842420578"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-500:]) / len(loss[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7213474143385887"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-5000:]) / len(loss[-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:00<00:00, 1003690.67it/s]\n"
     ]
    }
   ],
   "source": [
    "true_captions = {}\n",
    "for annot in tqdm(annotations[\"annotations\"]):\n",
    "    image_id = annot[\"image_id\"]\n",
    "\n",
    "    if image_id not in true_captions:\n",
    "        true_captions[image_id] = []\n",
    "    \n",
    "    true_captions[image_id].append(annot['caption'])\n",
    "    \n",
    "    \n",
    "def get_true_captions(image_paths):\n",
    "    return [true_captions[int(x[-10:-4])] for x in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:06<00:00,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "some man playing the back piano of two people . a woman holding on back a fr\n",
      "\n",
      "true caps : \n",
      "some people are standing near some public toilets\n",
      "A group of people are standing out in front of portable toilets.\n",
      "people are standing around some porta potties and mess\n",
      "a couple of people stand in front of some toilets \n",
      "groups of people standing around the toilet area\n",
      "=====================================\n",
      "predicted : \n",
      "a person sw holding computer phone to a bottle on his face . next\n",
      "\n",
      "true caps : \n",
      "A person is holding onto a cellphone somewhere.\n",
      "there is someone holding a small phone with a small screen\n",
      "A picture of a cell phone showing the time as 8:56 PM.\n",
      "a hand is holding a black and silver cellphone\n",
      "This is a verizon LG Envy 2 cell phone.\n",
      "=====================================\n",
      "predicted : \n",
      "two working smiling player chairs at his table . in and . for and the\n",
      "\n",
      "true caps : \n",
      "People working on laptop computers in a student dining room\n",
      "People sitting in a room at tables with laptops \n",
      "A group of people that are sitting in front of laptops.\n",
      "Many people have signed up for the computer seminar.\n",
      "A group of people at tables with laptops having conversation.\n",
      "=====================================\n",
      "predicted : \n",
      "a brown dog is eating something the rocks of in its habitat and a\n",
      "\n",
      "true caps : \n",
      "two birds near one another in a field \n",
      "Two black colored birds sitting in a field together. \n",
      "Two birds that are sitting on the ground.\n",
      "2 birds standing on the ground in a field \n",
      "Two birds that are standing next to each other on the grass. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[5:9]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images)\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:06<00:00,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "a man runs to catch the flying frisbee .\n",
      "\n",
      "true caps : \n",
      "some people are standing near some public toilets\n",
      "A group of people are standing out in front of portable toilets.\n",
      "people are standing around some porta potties and mess\n",
      "a couple of people stand in front of some toilets \n",
      "groups of people standing around the toilet area\n",
      "=====================================\n",
      "predicted : \n",
      "a person holding a cell phone while holding a cellphone .\n",
      "\n",
      "true caps : \n",
      "A person is holding onto a cellphone somewhere.\n",
      "there is someone holding a small phone with a small screen\n",
      "A picture of a cell phone showing the time as 8:56 PM.\n",
      "a hand is holding a black and silver cellphone\n",
      "This is a verizon LG Envy 2 cell phone.\n",
      "=====================================\n",
      "predicted : \n",
      "a man with a bunch of laptop computers to get his photo\n",
      "\n",
      "true caps : \n",
      "People working on laptop computers in a student dining room\n",
      "People sitting in a room at tables with laptops \n",
      "A group of people that are sitting in front of laptops.\n",
      "Many people have signed up for the computer seminar.\n",
      "A group of people at tables with laptops having conversation.\n",
      "=====================================\n",
      "predicted : \n",
      "a dog in a green field playing a frisbee .\n",
      "\n",
      "true caps : \n",
      "two birds near one another in a field \n",
      "Two black colored birds sitting in a field together. \n",
      "Two birds that are sitting on the ground.\n",
      "2 birds standing on the ground in a field \n",
      "Two birds that are standing next to each other on the grass. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[5:9]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_strategy=\"max\")\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "stop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-5981adc29e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: stop"
     ]
    }
   ],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train/{}\".format(str(datetime.now())[:-10])\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           attention=attention,\n",
    "                           optimizer=optimizer\n",
    "                          )\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------\")\n",
    "print(checkpoint_path)\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(checkpoint_path)\n",
    "with open(checkpoint_path + \"/config.txt\", \"w\") as f:\n",
    "    f.write(str(PARAMS))\n",
    "    \n",
    "log_file = open(checkpoint_path + \"/log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, PARAMS[\"epoch\"]):\n",
    "    \n",
    "    start = time.time()\n",
    "    loss = 0\n",
    "    batch = 1\n",
    "\n",
    "    for img_tensor, captions, target in tqdm(train_dataset):\n",
    "        \n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "\n",
    "            with open(checkpoint_path + \"/log.txt\", \"a\") as f:\n",
    "                log_message = ' {} Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                    checkpoint_path, epoch + 1, batch, batch_loss.numpy())\n",
    "                f.write(str(log_message + \"\\n\"))\n",
    "                print(log_message)   \n",
    "\n",
    "        batch += 1\n",
    "        \n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(batch_loss.numpy())\n",
    "\n",
    "#     ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = []\n",
    "\n",
    "# for (img_tensor, captions, target) in tqdm(train_dataset):\n",
    "#     batch_loss = train_step(img_tensor, captions, target)\n",
    "#     loss.append(batch_loss.numpy())\n",
    "    \n",
    "# plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PPLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "# text = [\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "# ]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images, support_text=text, pplm_iteration=5, pplm_weight=1)\n",
    "# result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
