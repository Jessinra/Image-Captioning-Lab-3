{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git add \"Keras.ipynb\"\n",
    "!git commit -m \"test pred using pplm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"rnn_units\": 256,\n",
    "    \"rnn_type\": \"BiLSTM\",\n",
    "    \"tokenizer\": \"BERT\",\n",
    "    \"word_embedding\": \"BERT\",\n",
    "    \"vocab_size\": 3000,\n",
    "    \"combine_strategy\": \"merge\",\n",
    "    \"combine_layer\": \"concat\",\n",
    "    \"image_context_size\": 256,\n",
    "    \"word_embedding_dim\": 256,\n",
    "    \"batch_size\": 32,\n",
    "    \"data_size\": 5000,\n",
    "    \"use_mapping\": True,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_caption_length\": 25, # use <int> or None\n",
    "    \"image_feature_extractor\": \"xception\",\n",
    "    \"use_sequence\": True,\n",
    "    \"epoch\": 20,\n",
    "    \"use_mask\": True,\n",
    "    \"version\": \"v5 : positional embedding\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "\n",
    "# for TFBertModel\n",
    "PROXIES = {\n",
    "  \"http\": \"http://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "  \"https\": \"https://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"[CLS] \" + annot['caption'] + \" [SEP]\"\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "\n",
    "stopper = -1 if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]\n",
    "train_captions = all_captions[:stopper]\n",
    "train_img_paths = all_img_paths[:stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 5000\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = len(train_captions) if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature_extractor(model_type=\"xception\"):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.compat.v1.keras.applications.xception\n",
    "        cnn_model = tf.compat.v1.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"CNN encoder model not supported yet\")\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "def get_image_feature_shape(model_type):\n",
    "    \n",
    "    if model_type == \"xception\":\n",
    "        return (100, 2048)\n",
    "    elif model_type == \"inception_v3\":\n",
    "        return (64, 2048)\n",
    "    else:\n",
    "        raise Exception (\"model unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = extractor_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FEATURE_SHAPE = get_image_feature_shape(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extractor, extractor_preprocessor = get_image_feature_extractor(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image feature\n",
    "\n",
    "this step mostly skipped since the results are cached already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 157.25\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = PARAMS[\"batch_size\"]\n",
    "\n",
    "estimated_batch_count = DATA_SIZE / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique images\n",
    "# unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# # Prepare dataset\n",
    "# image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "# image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "# image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessed image (batch)\n",
    "\n",
    "# for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "#     # get context vector of batch images\n",
    "#     batch_features = extractor(batch_imgs)\n",
    "    \n",
    "#     # flatten 2D cnn result into 1D for RNN decoder input\n",
    "#     # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "#     # image_feature = 100 (Xception)\n",
    "#     # image_feature = 64 (Inception V3)\n",
    "#     batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "#     # Cache preprocessed image\n",
    "#     for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "#         image_path = image_path.numpy().decode(\"utf-8\")\n",
    "#         np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Caption dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 12:09:14.147871 140443886188352 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0330 12:09:14.148968 140443886188352 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = PARAMS[\"tokenizer\"]\n",
    "VOCAB_SIZE = PARAMS[\"vocab_size\"]  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "    \n",
    "    def use_custom_mapping(self, use_mapping=True, vocab_size=3000):\n",
    "        \n",
    "        self.use_mapping = use_mapping\n",
    "        self.cust_vocab_size = vocab_size\n",
    "        self.mapping_initialized = False\n",
    "\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into custom indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        bert_ids = [self.convert_tokens_to_ids(self.tokenize(x)) for x in tqdm(texts)]\n",
    "        \n",
    "        if not self.use_mapping:\n",
    "            return bert_ids\n",
    "        \n",
    "        if not self.mapping_initialized:\n",
    "            self._initialize_custom_mapping(bert_ids)\n",
    "            return [self._convert_bert_id_to_custom_id(x) for x in tqdm(bert_ids)]\n",
    "        \n",
    "        return bert_ids\n",
    "    \n",
    "        \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \n",
    "        bert_ids = super().convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            return self._convert_bert_id_to_custom_id(bert_ids)\n",
    "        else:\n",
    "            return bert_ids\n",
    "        \n",
    "        \n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            bert_ids = self._convert_custom_id_to_bert_id(token_ids)\n",
    "        else:\n",
    "            bert_ids = token_ids\n",
    "            \n",
    "        bert_tokens = super().convert_ids_to_tokens(bert_ids)\n",
    "        return bert_tokens\n",
    "    \n",
    "    \n",
    "    def _initialize_custom_mapping(self, corpus_bert_ids):\n",
    "        \n",
    "        print(\"    > constructing custom mapping < \\n\")\n",
    "        self._build_occurence_table(corpus_bert_ids)\n",
    "        self._build_custom_mapping_table()\n",
    "        self.mapping_initialized = True\n",
    "        \n",
    "        \n",
    "    def _build_occurence_table(self, tokenized_captions):\n",
    "        \"\"\"\n",
    "        build dict of token frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        self.occurence_table = {}\n",
    "        for caption in tqdm(tokenized_captions):\n",
    "            for token in caption:\n",
    "                if token not in self.occurence_table:\n",
    "                    self.occurence_table[token] = 0\n",
    "                self.occurence_table[token] += 1\n",
    "                \n",
    "    \n",
    "    def _build_custom_mapping_table(self):\n",
    "        \n",
    "        _special_token = ['[UNK]', '[PAD]']\n",
    "        _actual_vocab_size = self.cust_vocab_size - len(_special_token)\n",
    "        \n",
    "        sorted_occurence = {k: v for k, v in sorted(\n",
    "            self.occurence_table.items(), reverse=True, key=lambda item: item[1]\n",
    "        )}\n",
    "        \n",
    "        used_tokens = sorted(list(sorted_occurence)[:_actual_vocab_size])\n",
    "        mapping_size = min(len(used_tokens), _actual_vocab_size)\n",
    "        \n",
    "        _bert_pad = 0\n",
    "        _bert_oov = 100\n",
    "        self._custom_pad = 0\n",
    "        self._custom_oov = mapping_size + 1\n",
    "        \n",
    "        self.bert_id_to_custom_id = {\n",
    "            _bert_pad: self._custom_pad, \n",
    "            _bert_oov: self._custom_oov\n",
    "        }\n",
    "        self.custom_id_to_bert_id = {\n",
    "            self._custom_pad: _bert_pad, \n",
    "            self._custom_oov: _bert_oov\n",
    "        }\n",
    "        \n",
    "        for i in range(0, mapping_size):\n",
    "            bert_token = used_tokens[i]\n",
    "            self.bert_id_to_custom_id[bert_token] = i + 1    \n",
    "            self.custom_id_to_bert_id[i + 1] = bert_token\n",
    "            \n",
    "        print(\"Vocab contains {0} / {1} unique tokens ({2:.2f} %)\".format(\n",
    "            len(used_tokens) + 2,\\\n",
    "            len(sorted_occurence),\\\n",
    "            (len(used_tokens) / len(sorted_occurence) * 100)\n",
    "        ))\n",
    "        \n",
    "        sorted_occurence_count = list(sorted_occurence.values())\n",
    "        used_tokens_count = sum(sorted_occurence_count[:_actual_vocab_size])\n",
    "        total_tokens_count = sum(sorted_occurence_count)\n",
    "        \n",
    "        print(\"Using {0} / {1} tokens available ({2:.2f} %)\".format(\n",
    "            used_tokens_count,\\\n",
    "            total_tokens_count,\\\n",
    "            (used_tokens_count / total_tokens_count * 100)\n",
    "        ))        \n",
    "        \n",
    "    def _convert_bert_id_to_custom_id(self, token_ids):\n",
    "        return [self.bert_id_to_custom_id[x] if x in self.bert_id_to_custom_id else self._custom_oov for x in token_ids]\n",
    "    \n",
    "    def _convert_custom_id_to_bert_id(self, token_ids):\n",
    "        return [self.custom_id_to_bert_id[x] for x in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_type, use_mapping, vocab_size):\n",
    "    \n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    if tokenizer_type == \"BERT\" :\n",
    "        tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "        tokenizer.use_custom_mapping(use_mapping, vocab_size)\n",
    "\n",
    "    # use default keras tokenizer\n",
    "    else : \n",
    "        tokenizer = TokenizerWrapper(num_words=vocab_size, oov_token=\"[UNK]\")\n",
    "        tokenizer.fit_on_texts(train_captions)    \n",
    "        tokenizer.word_index['[PAD]'] = 0\n",
    "        tokenizer.index_word[0] = '[PAD]'\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 12:09:15.983914 140443886188352 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0330 12:09:17.239660 140443886188352 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "caption_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=False,\n",
    "    vocab_size=0\n",
    ")\n",
    "\n",
    "target_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=PARAMS[\"use_mapping\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 3429.74it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 3372.86it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 267391.56it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 332385.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > constructing custom mapping < \n",
      "\n",
      "Vocab contains 3000 / 3584 unique tokens (83.65 %)\n",
      "Using 68179 / 68765 tokens available (99.15 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "caption_tokens = caption_tokenizer.texts_to_sequences(train_captions)\n",
    "target_tokens = target_tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parallel dataset\n",
    "\n",
    "prepare captions and target to support parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parallel_dataset(img_paths, caption_tokens, target_tokens):\n",
    "    \n",
    "    dataset_img_paths = []\n",
    "    dataset_captions = []\n",
    "    dataset_target = []\n",
    "    dataset_target_position = []\n",
    "\n",
    "    for i in tqdm(range(0, len(train_img_paths))):\n",
    "        img = img_paths[i]\n",
    "        cap = caption_tokens[i]\n",
    "        tar = target_tokens[i]\n",
    "\n",
    "        for j in range(1, len(cap)):\n",
    "            dataset_img_paths.append(img)\n",
    "            dataset_captions.append(cap[:j])\n",
    "            dataset_target.append(tar[j])\n",
    "            dataset_target_position.append(j)\n",
    "\n",
    "    dataset_captions = pad_sequences(dataset_captions, maxlen=PARAMS[\"max_caption_length\"], padding='post')\n",
    "    \n",
    "    return dataset_img_paths, dataset_captions, dataset_target, dataset_target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 22325.38it/s]\n"
     ]
    }
   ],
   "source": [
    "parallel_img_paths, parallel_captions, parallel_target, parallel_target_position = preprocess_parallel_dataset(train_img_paths, caption_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = parallel_captions.shape[0]\n",
    "MAX_CAPTION_LENGTH = parallel_captions.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_target_position = tf.one_hot(parallel_target_position, depth=MAX_CAPTION_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "\n",
    "def load_dataset(img_name, caption, target, target_pos):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, caption, target, target_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((parallel_img_paths, parallel_captions, parallel_target, parallel_target_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "\n",
    "dataset = dataset.map(lambda item1, item2, item3, item4: tf.numpy_function(\n",
    "          load_dataset, [item1, item2, item3, item4], [tf.float32, tf.int32, tf.int32, tf.float32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(DATA_SIZE / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 16, image_feature = 100, 2048)\n",
    "# caption => (batch_size = 16, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1395 batches, (total : 44640)\n",
      "eval : 298 batches, (total : 9536)\n",
      "test : 300 batches, (total : 9600 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captioning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=PARAMS[\"learning_rate\"])\n",
    "# loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    real  => (batch_size,)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate loss\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # create mask to filter out padding token \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    # Ignore loss_ if real token is padding\n",
    "    loss_ *= mask\n",
    "    \n",
    "    # Get mean of curren batch's loss (somewhat batch norm)\n",
    "    result_loss = tf.reduce_mean(loss_)\n",
    "    \n",
    "    return result_loss\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    loss_  => (batch_size, 1)\n",
    "    mask   => (batch_size, 1)  : indicate is padding or not\n",
    "\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim)\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # x => (batch_size, 100, 2048)\n",
    "        x = self.fc(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, image_feature_size, image_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, img_feature_size, image_context_size)\n",
    "        hidden                        => (batch_size, embedding_size)\n",
    "        \n",
    "        note : \n",
    "        img_feature_size ==  64 for Inception V3,\n",
    "        img_feature_size == 100 for Xception,\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        _w1 = self.W1(features)\n",
    "        _w2 = self.W2(hidden_with_time_axis)\n",
    "        score = tf.nn.tanh(_w1 + _w2)\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_with_time_axis      => (batch_size, 1, embedding_size)\n",
    "        score                      => (batch_size, img_feature_size, units)\n",
    "        attention_weights          => (batch_size, img_feature_size, 1)\n",
    "        context_vector (after sum) => (batch_size, img_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rnn Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class RNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=256, \n",
    "                 embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 vocab_size=3000, batch_size=32):\n",
    "        \n",
    "        super(RNN_Encoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # =====================================================\n",
    "        \n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "        \n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "\n",
    "    def _init_embedding(self):\n",
    "        \n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.bert_embedding = TFBertModel.from_pretrained('bert-base-uncased', proxies=PROXIES)\n",
    "            self.bert_embedding.trainable = False\n",
    "            self.embedding_dim = self.bert_embedding.config.hidden_size\n",
    "            \n",
    "        else:\n",
    "            self.default_embedding = Embedding(\n",
    "                input_dim=self.vocab_size, \n",
    "                output_dim=self.embedding_dim, \n",
    "                mask_zero=True,\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def _init_rnn(self):\n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector'\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            self.bilstm = Bidirectional(LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        \n",
    "    def embedding(self, tokens, as_sentence=False):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embedding_type == \"BERT\": \n",
    "            embedding, sentence_embedding = self._bert_embedding(tokens)\n",
    "        \n",
    "        else:\n",
    "            embedding = self._default_embedding(tokens)\n",
    "            sentence_embedding = tf.reduce_mean(embedding, 1)\n",
    "            \n",
    "\n",
    "        if as_sentence:\n",
    "            # embedding => (batch_size, embedding_dim)\n",
    "            return sentence_embedding\n",
    "        \n",
    "        else:\n",
    "            # embedding => (batch_size, tokens_length, embedding_dim)\n",
    "            return embedding\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        embedding (tokens)   => (batch_size, tokens_length, embedding_dim)\n",
    "        embedding (sentence) => (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def _bert_embedding(self, x):\n",
    "        \n",
    "        token_type_ids = tf.cast((x == 0), tf.int32)\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, token_type_ids=token_type_ids)\n",
    "\n",
    "        return hidden_states, sentence_embedding\n",
    "    \n",
    "        \"\"\"\n",
    "        hidden states contains hidden state for each word\n",
    "        sentence_embedding is general embedding for whole sentences\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def _default_embedding(self, x):\n",
    "        \n",
    "        # embedding => (batch_size, sequence_length, embedding_size)\n",
    "        embedding = self.default_embedding(x)\n",
    "        return embedding\n",
    "\n",
    "    \n",
    "    def rnn_model(self, x):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            output, h_state, _ = self.lstm(x)\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            output, _, h_state, _, _ = self.bilstm(x)\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, h_state = self.gru(x)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, h_state\n",
    "    \n",
    "        \"\"\"\n",
    "        output => (batch_size, rnn_size)\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "    def call(self, decoder_input):\n",
    "        \"\"\" \n",
    "        decoder_input  : last predicted word => (batch_size, sequence len)\n",
    "        \"\"\"\n",
    "        \n",
    "        # x1 => (batch_size, input_sentence_len, embedding_dim + positional_dim)\n",
    "        x1 = self.embedding(decoder_input)\n",
    "        \n",
    "        # output => (batch_size, sequence_len, rnn_unit)\n",
    "        x2, rnn_state = self.rnn_model(x1) \n",
    "        output = self.dropout(x2)\n",
    "\n",
    "        return output, rnn_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, units=256, combine_layer=\"concat\", vocab_size=3000, batch_size=32):\n",
    "\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.combine_layer = combine_layer  # how to use context_vector [\"add\", \"concat\"]\n",
    "\n",
    "        # =====================================================\n",
    "\n",
    "        self._init_combine_layer()\n",
    "        \n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.dropout = Dropout(0.2)\n",
    "\n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\")  # same size as vocab\n",
    "\n",
    "    def _init_combine_layer(self):\n",
    "\n",
    "        if self.combine_layer == \"add\":\n",
    "            self.combine = tf.keras.layers.Add()\n",
    "\n",
    "        else:\n",
    "            self.combine = tf.keras.layers.Concatenate(axis=-1)\n",
    "\n",
    "    def _format_context_vector(self, context_vector):\n",
    "\n",
    "        context_vector = tf.expand_dims(context_vector, 1)\n",
    "        context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "    def call(self, decoder_input, context_vector):\n",
    "        \"\"\" \n",
    "        decoder_input  : \n",
    "        context_vector : image's vector      => (batch_size, img_context_size)\n",
    "        \"\"\"\n",
    "\n",
    "        context_vector = self._format_context_vector(context_vector)\n",
    "\n",
    "        # x1 (concat) => (batch_size, input_sentence_len, embedding_dim + image_context_size)\n",
    "        # x1 (add) => (batch_size, embedding_dim)\n",
    "        x1 = self.combine([context_vector, decoder_input])\n",
    "\n",
    "        # ============================================\n",
    "        # TODO: add another attention layer ?\n",
    "        # ============================================\n",
    "\n",
    "        # x2 => (batch_size, sequence_len, rnn_units)\n",
    "        x2 = self.fc1(x1)   # how important is every sequence\n",
    "        x2 = self.batchnorm(x2)\n",
    "        x2 = self.leakyrelu(x2)\n",
    "        x2 = self.dropout(x2)\n",
    "\n",
    "        # x3 => (batch_size, sequence_len * rnn_units) || was (batch_size * sequence_len=1, rnn_units)\n",
    "        x3 = tf.reshape(x2, (x2.shape[0], -1))\n",
    "\n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x3)\n",
    "\n",
    "        return word_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rnn_units': 256,\n",
       " 'rnn_type': 'BiLSTM',\n",
       " 'tokenizer': 'BERT',\n",
       " 'word_embedding': 'BERT',\n",
       " 'vocab_size': 3000,\n",
       " 'combine_strategy': 'merge',\n",
       " 'combine_layer': 'concat',\n",
       " 'image_context_size': 256,\n",
       " 'word_embedding_dim': 256,\n",
       " 'batch_size': 32,\n",
       " 'data_size': 5000,\n",
       " 'use_mapping': True,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_caption_length': 25,\n",
       " 'image_feature_extractor': 'xception',\n",
       " 'use_sequence': True,\n",
       " 'epoch': 20,\n",
       " 'use_mask': True,\n",
       " 'version': 'v5 : positional embedding'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 12:09:24.030811 140443886188352 configuration_utils.py:256] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/m13516112/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0330 12:09:24.032533 140443886188352 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0330 12:09:25.194196 140443886188352 modeling_tf_utils.py:333] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5 from cache at /home/m13516112/.cache/torch/transformers/d667df51ec24c20190f01fb4c20a21debc4c4fc12f7e2f5441ac0a99690e3ee9.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5\n",
      "I0330 12:09:28.075641 140443886188352 modeling_tf_utils.py:375] Layers from pretrained model not used in TFBertModel: ['mlm___cls', 'nsp___cls']\n"
     ]
    }
   ],
   "source": [
    "cnn_encoder = CNN_Encoder(\n",
    "    output_dim=PARAMS[\"image_context_size\"]\n",
    ")\n",
    "\n",
    "attention = BahdanauAttention(\n",
    "    units=PARAMS[\"rnn_units\"]\n",
    ")\n",
    "\n",
    "rnn_encoder = RNN_Encoder(\n",
    "    rnn_type=PARAMS[\"rnn_type\"], \n",
    "    rnn_units=PARAMS[\"rnn_units\"],\n",
    "    embedding_type=PARAMS[\"word_embedding\"], \n",
    "    embedding_dim=PARAMS[\"word_embedding_dim\"],  \n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    "    batch_size=PARAMS[\"batch_size\"]\n",
    ")\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    units=PARAMS[\"rnn_units\"],\n",
    "    combine_layer=PARAMS[\"combine_layer\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    "    batch_size=PARAMS[\"batch_size\"]\n",
    ")\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Requirements\n",
    "\n",
    "# combine_strategy = \"inject_init\" : IMAGE_CONTEXT_SIZE == UNITS\n",
    "# combine_strategy = \"inject_pre\"  : IMAGE_CONTEXT_SIZE == WORD_EMBEDDING_DIM\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default feed forward function\n",
    "\n",
    "@tf.function\n",
    "def feed_forward(img_tensor, caption):\n",
    "    \"\"\"\n",
    "    img_tensor => (batch_size, image_feature_size, 2048)\n",
    "    caption => (batch_size, max_caption_length)\n",
    "    \"\"\"\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    # note : used to be decoder hidden state, \n",
    "    # but changed into current word embedding for paralel training\n",
    "    \n",
    "    # hidden => (batch_size, embedding_size)\n",
    "    text_features, hidden = rnn_encoder(caption)\n",
    "\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    img_features = cnn_encoder(img_tensor)\n",
    "\n",
    "    # context_vector => (batch_size, image_context_size)\n",
    "    context_vector, _ = attention(img_features, hidden)\n",
    "\n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions = decoder(text_features, context_vector)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, caption, target):\n",
    "    \"\"\"\n",
    "    target => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training model\n",
    "    with tf.GradientTape() as gradient_tape:\n",
    "        \n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions = feed_forward(img_tensor, caption)\n",
    "        \n",
    "        # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "        loss = loss_function(target, predictions)\n",
    "\n",
    "        \n",
    "    # Apply gradient\n",
    "    trainable_variables = cnn_encoder.trainable_variables + \\\n",
    "                          rnn_encoder.trainable_variables + \\\n",
    "                          decoder.trainable_variables + \\\n",
    "                          attention.trainable_variables\n",
    "    \n",
    "    gradients = gradient_tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def eval_step(img_tensor, caption, target):\n",
    "    \n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions = feed_forward(img_tensor, caption)\n",
    "    \n",
    "    # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "    loss = loss_function(target, predictions)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def choose_predicted_id(predictions, strategy=\"max\", sampling_k=10):\n",
    "    \"\"\"\n",
    "    predictions : encoder word prediction => (batch_size, vocab_size)\n",
    "    strategy    : how to choose word [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sampling method (categorical dist)\n",
    "    if strategy == \"sample\":\n",
    "        \n",
    "        # sampled_proba & sampled_ids => (batch_size, sampling_k)\n",
    "        sampled_proba, sampled_ids = tf.math.top_k(predictions, sampling_k)\n",
    "        \n",
    "        # chosen_sampled_col => (batch_size, )\n",
    "        chosen_sampled_col = tf.squeeze(tf.random.categorical(sampled_proba, 1))\n",
    "        \n",
    "        # create row idx to zip with chosen_sampled_col\n",
    "        row_idx = tf.range(predictions.shape[0], dtype=chosen_sampled_col.dtype)\n",
    "        row_col_idx = tf.stack([row_idx, chosen_sampled_col], axis=1)\n",
    "        \n",
    "        # predicted_ids => (batch_size, )\n",
    "        predicted_ids = tf.gather_nd(sampled_ids, row_col_idx)\n",
    "\n",
    "    # Max index method\n",
    "    else:\n",
    "        predicted_ids = tf.argmax(predictions, 1)\n",
    "    \n",
    "    # predicted_ids => (batch_size, )\n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(images_paths):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [load_image(x)[0] for x in images_paths]\n",
    "    \n",
    "    # x => (batch_size, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, 10, 10, 2048)\n",
    "    x = extractor(x)\n",
    "    \n",
    "    # x  => (batch_size, img_feature_size, 2048)\n",
    "    x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = cnn_encoder(x)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def get_supporting_features(images_paths, strategy=\"mean\"):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, img_count, 1)\n",
    "    strategy : strategy to aggregate multiple supporting image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [[load_image(x)[0] for x in images_set] for images_set in images_paths]\n",
    "    \n",
    "    # x => (batch_size, img_count, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, img_count, 10, 10, 2048)\n",
    "    x = [extractor(image_set) for image_set in x]\n",
    "    \n",
    "    # features => (batch_size, img_count, img_feature_size, image_context_size)\n",
    "    features = cnn_encoder(x)\n",
    "    \n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    if strategy == \"logsumexp\":\n",
    "        features = tf.reduce_logsumexp(features, 1)\n",
    "    elif strategy == \"max\":\n",
    "        features = tf.reduce_max(features, 1)\n",
    "    elif strategy == \"min\":\n",
    "        features = tf.reduce_min(features, 1)\n",
    "    else:\n",
    "        features = tf.reduce_mean(features, 1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_indices(support_text):\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(0, len(support_text)):\n",
    "        context_token = target_tokenizer.tokenize(support_text[i])\n",
    "        context_token_id = target_tokenizer.convert_tokens_to_ids(context_token)\n",
    "        context_token_id = set(context_token_id)\n",
    "        context_token_id.discard(0)\n",
    "        for x in sorted(context_token_id):\n",
    "            indices.append([i, x])\n",
    "\n",
    "    # return => (word_count, 2)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_supporting_text_vector(support_text, vocab_size):\n",
    "    \"\"\"\n",
    "    support_text : list of text describing main image context => (batch_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(support_text)\n",
    "    \n",
    "    # indices => ( sum(batch_size * ?word_count), 2)\n",
    "    indices = get_one_hot_indices(support_text)\n",
    "    values = tf.ones(len(indices))\n",
    "    sparse_one_hot = tf.sparse.SparseTensor(indices, values, dense_shape=[batch_size, vocab_size])\n",
    "    \n",
    "    # sparse_one_hot => (batch_size, vocab_size)\n",
    "    return sparse_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluate(images_paths,\n",
    "                    support_text=None,\n",
    "                    support_imgs=None, \n",
    "                    support_aggregate_strategy=\"mean\",\n",
    "                    pplm_iteration=3,\n",
    "                    pplm_weight=0.03,\n",
    "                    pplm_gm_weight=0.8,\n",
    "                    choose_word_strategy=\"sample\",\n",
    "                   ):\n",
    "    \n",
    "    \"\"\"\n",
    "    images_paths : list of image_path                           => (batch_size, 1)\n",
    "    support_text : list of text describing main image context   => (batch_size)\n",
    "    support_imgs : list of list of image_path                   => (batch_size, image_count, 1)\n",
    "    support_aggregate_strategy : how to aggregate support image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    pplm_iteration : number of pplm step done for every decoding step\n",
    "    pplm_weight    : weight of pplm loss\n",
    "    pplm_gm_weight : geometric mean fusion weight (0 means use only original prediction, 1 means use only pplm prediction)\n",
    "    choose_word_strategy : how to choose word from prediction distribution [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(images_paths)\n",
    "    \n",
    "    # initialize captions placeholder\n",
    "    start_token = caption_tokenizer.convert_tokens_to_ids(['[CLS]']) # use bert id's not custom id\n",
    "    result_captions = tf.tile(tf.expand_dims(start_token, 1), [batch_size, 1])\n",
    "    attention_plot = tf.reshape([], shape=(batch_size, 0, IMAGE_FEATURE_SHAPE[0]))\n",
    "    \n",
    "    # Extract features from main images\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = get_image_features(images_paths)\n",
    "    \n",
    "    \n",
    "    if support_text is not None:\n",
    "        # support_text_vector => (batch_size, vocab_size)\n",
    "        support_text_vector = get_supporting_text_vector(support_text, decoder.vocab_size) \n",
    "    else:\n",
    "        # set all pplm related variable to 0\n",
    "        pplm_iteration = 0\n",
    "        pplm_weight = 0\n",
    "        pplm_gm_weight = 0\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(MAX_CAPTION_LENGTH)):\n",
    "        \n",
    "        # decoder_input => (batch_size, ~MAX_CAPTION_LENGTH)\n",
    "        caption = pad_sequences(result_captions, maxlen=MAX_CAPTION_LENGTH, padding=\"post\")\n",
    "        \n",
    "        # hidden => (batch_size, embedding_size)\n",
    "        text_features, hidden = rnn_encoder(caption)\n",
    "\n",
    "        # context_vector => (batch_size, image_context_size)\n",
    "        # attention_weights => (batch_size, img_feature_size, 1)\n",
    "        context_vector, attention_weights = attention(features, hidden)\n",
    "\n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions = decoder(text_features, context_vector)  \n",
    "        \n",
    "        \n",
    "        # ======================== PPLM section ========================\n",
    "        ori_prediction = predictions\n",
    "        \n",
    "        curr_pertubation = tf.Variable(tf.zeros((batch_size, rnn_encoder.embedding_dim)), name=\"curr_pertubation\", trainable=True)\n",
    "        \n",
    "        \n",
    "        for j in range(pplm_iteration):\n",
    "            \n",
    "            with tf.GradientTape() as pplm_tape: \n",
    "                \n",
    "                hidden += curr_pertubation\n",
    "                \n",
    "                context_vector, attention_weights = attention(features, hidden)\n",
    "                predictions = decoder(text_features, context_vector)  \n",
    "                pplm_loss = pplm_loss_function(support_text_vector, predictions, pplm_weight=pplm_weight)\n",
    "                \n",
    "            \"\"\"\n",
    "            most impactfull layer to train = last dense layer\n",
    "            \"\"\"\n",
    "            \n",
    "            trainable_variables = [curr_pertubation]\n",
    "            gradients = pplm_tape.gradient(pplm_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "            \n",
    "            predictions = decoder(text_features, context_vector)  \n",
    "        \n",
    "        # fuse final pplm_prediction and original prediction\n",
    "        fused_predictions = (predictions * pplm_gm_weight) + (ori_prediction * (1 - pplm_gm_weight)) \n",
    "        \n",
    "        # predicted_ids => (batch_size,)\n",
    "        predicted_ids = choose_predicted_id(fused_predictions, strategy=choose_word_strategy)\n",
    "        \n",
    "        # convert custom id mapping to bert's id\n",
    "        # predicted_bert_ids => (batch_size, 1)\n",
    "        predicted_bert_ids = target_tokenizer._convert_custom_id_to_bert_id(predicted_ids.numpy())\n",
    "        predicted_bert_ids = tf.expand_dims(predicted_bert_ids, 1)\n",
    "        \n",
    "        # store result\n",
    "        result_captions = tf.concat([result_captions, predicted_bert_ids], axis=1)\n",
    "        \n",
    "        # attention_weights => (batch_size, 1, img_feature_size)\n",
    "        attention_weights = tf.reshape(attention_weights, shape=(batch_size, 1, -1))\n",
    "        \n",
    "        # assign attention weights to respective generated word\n",
    "        # attention_plot => (batch_size, ~max_caption_len, feature_size)\n",
    "        attention_plot = tf.concat([attention_plot, attention_weights], axis=1)\n",
    "        \n",
    "    # remove start token & revert to tokens\n",
    "    result_captions = [caption_tokenizer.convert_ids_to_tokens(x[1:]) for x in result_captions]\n",
    "    \n",
    "    return result_captions, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy, MeanSquaredError\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def pplm_loss_function(real, pred, pplm_weight=0.03):\n",
    "    \"\"\"\n",
    "    real  => (batch_size, vocab_size)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "\n",
    "#     mm = tf.sparse.sparse_dense_matmul(real, tf.transpose(pred))\n",
    "#     mm = tf.reduce_sum(tf.abs(mm), 1)\n",
    "#     loss = tf.reduce_sum(mm, 0)\n",
    "    \n",
    "    real = tf.sparse.to_dense(real, default_value=0)\n",
    "    pplm_loss = CategoricalCrossentropy(from_logits=True)\n",
    "    loss = pplm_loss(real, pred, pplm_weight)\n",
    "    \n",
    "#     print(loss)\n",
    "    return loss\n",
    "\n",
    "    \"\"\"\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1395it [02:03, 11.27it/s]\n",
      "1395it [02:08, 10.88it/s]\n",
      "1395it [01:45, 13.26it/s]\n",
      "1395it [02:06, 10.99it/s]\n",
      "1395it [01:47, 13.00it/s]\n",
      "1395it [02:06, 10.99it/s]\n",
      "1395it [01:37, 14.37it/s]\n",
      "1395it [01:14, 18.80it/s]\n",
      "1395it [01:13, 18.90it/s]\n",
      "1395it [01:14, 18.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb85887c240>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    for (img_tensor, captions, target, _) in tqdm(train_dataset):\n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss.append(batch_loss.numpy())\n",
    "        \n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbb627a8ac8>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5437058862447739"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-500:]) / len(loss[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7114340108007192"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-5000:]) / len(loss[-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:00<00:00, 686610.76it/s]\n"
     ]
    }
   ],
   "source": [
    "true_captions = {}\n",
    "for annot in tqdm(annotations[\"annotations\"]):\n",
    "    image_id = annot[\"image_id\"]\n",
    "\n",
    "    if image_id not in true_captions:\n",
    "        true_captions[image_id] = []\n",
    "    \n",
    "    true_captions[image_id].append(annot['caption'])\n",
    "    \n",
    "    \n",
    "def get_true_captions(image_paths):\n",
    "    return [true_captions[int(x[-10:-4])] for x in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:03<00:00,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "small in blue hydra bottle on top with in in\n",
      "\n",
      "true caps : \n",
      "A bathroom with a toilet and sink, \n",
      "View of a white toilet next to a vanity with a red sink.\n",
      "A white remote and sink in a room.\n",
      "A white toilet sitting next to a bathroom sink.\n",
      "A bathroom with a toilet and a red sink.\n",
      "=====================================\n",
      "predicted : \n",
      "a large sink next next mirror to with people .\n",
      "\n",
      "true caps : \n",
      "A small bathroom with sink, mirror, toilet and tub\n",
      "A white sink sitting under a bathroom mirror.\n",
      "A bathroom mirror and sink in a bathroom \n",
      "this bathroom is all beige and has a white sink\n",
      "A mirror reflection above the sink shows the toilet and bathtub.\n",
      "=====================================\n",
      "predicted : \n",
      "a cluster of giraffe their in trains with it . are to an side them that to near out to to in out\n",
      "\n",
      "true caps : \n",
      "a yellow and grey passenger train parked next to a platform\n",
      "A train traveling down a set of tracks near a train station.\n",
      "Train arriving or departing at an empty station.\n",
      "A train is coming down the tracks near a platform.\n",
      "A subway train driving through a subway station. \n",
      "=====================================\n",
      "predicted : \n",
      "a couple people sitting together and their their sitting playing at their them it it\n",
      "\n",
      "true caps : \n",
      "A group of people sitting at the table smiling. \n",
      "A group of elderly people having dinner at a restaurant. \n",
      "a number of older people seated around a table\n",
      "A group of people are seated at a restaurant dining table.\n",
      "A group of people sitting around a restaurant table.\n",
      "=====================================\n",
      "predicted : \n",
      "the guy hold some a parked of something with a in his a it . at in them sink the sink . has the couple\n",
      "\n",
      "true caps : \n",
      "Two men standing close together examining some red wine.\n",
      "One man holding a wine glass with some wine while his friend looks on\n",
      "Two men looking up while toasting with wine glasses. \n",
      "A couple of men standing next to each other holding wine glasses.\n",
      "Two people are posing for the camera with their win glasses. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[50:55]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images)\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:03<00:00,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "groups standing in front of a fence with people in the background .\n",
      "\n",
      "true caps : \n",
      "some people are standing near some public toilets\n",
      "A group of people are standing out in front of portable toilets.\n",
      "people are standing around some porta potties and mess\n",
      "a couple of people stand in front of some toilets \n",
      "groups of people standing around the toilet area\n",
      "=====================================\n",
      "predicted : \n",
      "a person holding a white and jacket is on a white plate .\n",
      "\n",
      "true caps : \n",
      "A person is holding onto a cellphone somewhere.\n",
      "there is someone holding a small phone with a small screen\n",
      "A picture of a cell phone showing the time as 8:56 PM.\n",
      "a hand is holding a black and silver cellphone\n",
      "This is a verizon LG Envy 2 cell phone.\n",
      "=====================================\n",
      "predicted : \n",
      "people working a laptop in a room of people\n",
      "\n",
      "true caps : \n",
      "People working on laptop computers in a student dining room\n",
      "People sitting in a room at tables with laptops \n",
      "A group of people that are sitting in front of laptops.\n",
      "Many people have signed up for the computer seminar.\n",
      "A group of people at tables with laptops having conversation.\n",
      "=====================================\n",
      "predicted : \n",
      "a large white and white zebra in a field .\n",
      "\n",
      "true caps : \n",
      "two birds near one another in a field \n",
      "Two black colored birds sitting in a field together. \n",
      "Two birds that are sitting on the ground.\n",
      "2 birds standing on the ground in a field \n",
      "Two birds that are standing next to each other on the grass. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[5:9]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_strategy=\"max\")\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train/{}\".format(str(datetime.now())[:-10])\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           attention=attention,\n",
    "                           optimizer=optimizer\n",
    "                          )\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------\")\n",
    "print(checkpoint_path)\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(checkpoint_path)\n",
    "with open(checkpoint_path + \"/config.txt\", \"w\") as f:\n",
    "    f.write(str(PARAMS))\n",
    "    \n",
    "log_file = open(checkpoint_path + \"/log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, PARAMS[\"epoch\"]):\n",
    "    \n",
    "    start = time.time()\n",
    "    loss = 0\n",
    "    batch = 1\n",
    "\n",
    "    for img_tensor, captions, target in tqdm(train_dataset):\n",
    "        \n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "\n",
    "            with open(checkpoint_path + \"/log.txt\", \"a\") as f:\n",
    "                log_message = ' {} Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                    checkpoint_path, epoch + 1, batch, batch_loss.numpy())\n",
    "                f.write(str(log_message + \"\\n\"))\n",
    "                print(log_message)   \n",
    "\n",
    "        batch += 1\n",
    "        \n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(batch_loss.numpy())\n",
    "\n",
    "#     ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = []\n",
    "\n",
    "# for (img_tensor, captions, target) in tqdm(train_dataset):\n",
    "#     batch_loss = train_step(img_tensor, captions, target)\n",
    "#     loss.append(batch_loss.numpy())\n",
    "    \n",
    "# plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PPLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "# text = [\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "# ]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images, support_text=text, pplm_iteration=5, pplm_weight=1)\n",
    "# result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
