{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git add \"Keras.ipynb\"\n",
    "!git commit -m \"test pred using pplm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"rnn_units\": 256,\n",
    "    \"rnn_type\": \"BiLSTM\",\n",
    "    \"tokenizer\": \"BERT\",\n",
    "    \"word_embedding\": \"BERT\",\n",
    "    \"vocab_size\": 3000,\n",
    "    \"combine_strategy\": \"merge\",\n",
    "    \"combine_layer\": \"concat\",\n",
    "    \"image_context_size\": 256,\n",
    "    \"word_embedding_dim\": 256,\n",
    "    \"batch_size\": 32,\n",
    "    \"data_size\": 25000,\n",
    "    \"use_mapping\": True,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_caption_length\": 25, # use <int> or None\n",
    "    \"image_feature_extractor\": \"xception\",\n",
    "    \"use_sequence\": True,\n",
    "    \"epoch\": 20,\n",
    "    \"use_mask\": True,\n",
    "    \"version\": \"v5 : positional embedding\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "\n",
    "# for TFBertModel\n",
    "PROXIES = {\n",
    "  \"http\": \"http://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "  \"https\": \"https://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"[CLS] \" + annot['caption'] + \" [SEP]\"\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "\n",
    "stopper = -1 if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]\n",
    "train_captions = all_captions[:stopper]\n",
    "train_img_paths = all_img_paths[:stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 25000\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = len(train_captions) if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature_extractor(model_type=\"xception\"):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.compat.v1.keras.applications.xception\n",
    "        cnn_model = tf.compat.v1.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"CNN encoder model not supported yet\")\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "def get_image_feature_shape(model_type):\n",
    "    \n",
    "    if model_type == \"xception\":\n",
    "        return (100, 2048)\n",
    "    elif model_type == \"inception_v3\":\n",
    "        return (64, 2048)\n",
    "    else:\n",
    "        raise Exception (\"model unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = extractor_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FEATURE_SHAPE = get_image_feature_shape(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extractor, extractor_preprocessor = get_image_feature_extractor(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image feature\n",
    "\n",
    "this step mostly skipped since the results are cached already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 782.25\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = PARAMS[\"batch_size\"]\n",
    "\n",
    "estimated_batch_count = DATA_SIZE / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique images\n",
    "# unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# # Prepare dataset\n",
    "# image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "# image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "# image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessed image (batch)\n",
    "\n",
    "# for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "#     # get context vector of batch images\n",
    "#     batch_features = extractor(batch_imgs)\n",
    "    \n",
    "#     # flatten 2D cnn result into 1D for RNN decoder input\n",
    "#     # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "#     # image_feature = 100 (Xception)\n",
    "#     # image_feature = 64 (Inception V3)\n",
    "#     batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "#     # Cache preprocessed image\n",
    "#     for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "#         image_path = image_path.numpy().decode(\"utf-8\")\n",
    "#         np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Caption dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0415 01:46:49.667076 140569263748928 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0415 01:46:49.668245 140569263748928 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = PARAMS[\"tokenizer\"]\n",
    "VOCAB_SIZE = PARAMS[\"vocab_size\"]  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "    \n",
    "    def use_custom_mapping(self, use_mapping=True, vocab_size=3000):\n",
    "        \n",
    "        self.use_mapping = use_mapping\n",
    "        self.cust_vocab_size = vocab_size\n",
    "        self.mapping_initialized = False\n",
    "\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into custom indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        bert_ids = [self.convert_tokens_to_ids(self.tokenize(x)) for x in tqdm(texts)]\n",
    "        \n",
    "        if not self.use_mapping:\n",
    "            return bert_ids\n",
    "        \n",
    "        if not self.mapping_initialized:\n",
    "            self._initialize_custom_mapping(bert_ids)\n",
    "            return [self._convert_bert_id_to_custom_id(x) for x in tqdm(bert_ids)]\n",
    "        \n",
    "        return bert_ids\n",
    "    \n",
    "        \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \n",
    "        bert_ids = super().convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            return self._convert_bert_id_to_custom_id(bert_ids)\n",
    "        else:\n",
    "            return bert_ids\n",
    "        \n",
    "        \n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            bert_ids = self._convert_custom_id_to_bert_id(token_ids)\n",
    "        else:\n",
    "            bert_ids = token_ids\n",
    "            \n",
    "        bert_tokens = super().convert_ids_to_tokens(bert_ids)\n",
    "        return bert_tokens\n",
    "    \n",
    "    \n",
    "    def _initialize_custom_mapping(self, corpus_bert_ids):\n",
    "        \n",
    "        print(\"    > constructing custom mapping < \\n\")\n",
    "        self._build_occurence_table(corpus_bert_ids)\n",
    "        self._build_custom_mapping_table()\n",
    "        self.mapping_initialized = True\n",
    "        \n",
    "        \n",
    "    def _build_occurence_table(self, tokenized_captions):\n",
    "        \"\"\"\n",
    "        build dict of token frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        self.occurence_table = {}\n",
    "        for caption in tqdm(tokenized_captions):\n",
    "            for token in caption:\n",
    "                if token not in self.occurence_table:\n",
    "                    self.occurence_table[token] = 0\n",
    "                self.occurence_table[token] += 1\n",
    "                \n",
    "    \n",
    "    def _build_custom_mapping_table(self):\n",
    "        \n",
    "        _special_token = ['[UNK]', '[PAD]']\n",
    "        _actual_vocab_size = self.cust_vocab_size - len(_special_token)\n",
    "        \n",
    "        sorted_occurence = {k: v for k, v in sorted(\n",
    "            self.occurence_table.items(), reverse=True, key=lambda item: item[1]\n",
    "        )}\n",
    "        \n",
    "        used_tokens = sorted(list(sorted_occurence)[:_actual_vocab_size])\n",
    "        mapping_size = min(len(used_tokens), _actual_vocab_size)\n",
    "        \n",
    "        _bert_pad = 0\n",
    "        _bert_oov = 100\n",
    "        self._custom_pad = 0\n",
    "        self._custom_oov = mapping_size + 1\n",
    "        \n",
    "        self.bert_id_to_custom_id = {\n",
    "            _bert_pad: self._custom_pad, \n",
    "            _bert_oov: self._custom_oov\n",
    "        }\n",
    "        self.custom_id_to_bert_id = {\n",
    "            self._custom_pad: _bert_pad, \n",
    "            self._custom_oov: _bert_oov\n",
    "        }\n",
    "        \n",
    "        for i in range(0, mapping_size):\n",
    "            bert_token = used_tokens[i]\n",
    "            self.bert_id_to_custom_id[bert_token] = i + 1    \n",
    "            self.custom_id_to_bert_id[i + 1] = bert_token\n",
    "            \n",
    "        print(\"Vocab contains {0} / {1} unique tokens ({2:.2f} %)\".format(\n",
    "            len(used_tokens) + 2,\\\n",
    "            len(sorted_occurence),\\\n",
    "            (len(used_tokens) / len(sorted_occurence) * 100)\n",
    "        ))\n",
    "        \n",
    "        sorted_occurence_count = list(sorted_occurence.values())\n",
    "        used_tokens_count = sum(sorted_occurence_count[:_actual_vocab_size])\n",
    "        total_tokens_count = sum(sorted_occurence_count)\n",
    "        \n",
    "        print(\"Using {0} / {1} tokens available ({2:.2f} %)\".format(\n",
    "            used_tokens_count,\\\n",
    "            total_tokens_count,\\\n",
    "            (used_tokens_count / total_tokens_count * 100)\n",
    "        ))        \n",
    "        \n",
    "    def _convert_bert_id_to_custom_id(self, token_ids):\n",
    "        return [self.bert_id_to_custom_id[x] if x in self.bert_id_to_custom_id else self._custom_oov for x in token_ids]\n",
    "    \n",
    "    def _convert_custom_id_to_bert_id(self, token_ids):\n",
    "        return [self.custom_id_to_bert_id[x] for x in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_type, use_mapping, vocab_size):\n",
    "    \n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    if tokenizer_type == \"BERT\" :\n",
    "        tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "        tokenizer.use_custom_mapping(use_mapping, vocab_size)\n",
    "\n",
    "    # use default keras tokenizer\n",
    "    else : \n",
    "        tokenizer = TokenizerWrapper(num_words=vocab_size, oov_token=\"[UNK]\")\n",
    "        tokenizer.fit_on_texts(train_captions)    \n",
    "        tokenizer.word_index['[PAD]'] = 0\n",
    "        tokenizer.index_word[0] = '[PAD]'\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0415 01:46:51.515678 140569263748928 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0415 01:46:52.734189 140569263748928 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "caption_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=False,\n",
    "    vocab_size=0\n",
    ")\n",
    "\n",
    "target_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=PARAMS[\"use_mapping\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:08<00:00, 2794.41it/s]\n",
      "100%|██████████| 25000/25000 [00:12<00:00, 1992.73it/s]\n",
      "100%|██████████| 25000/25000 [00:00<00:00, 289826.81it/s]\n",
      "100%|██████████| 25000/25000 [00:00<00:00, 286101.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > constructing custom mapping < \n",
      "\n",
      "Vocab contains 3000 / 6906 unique tokens (43.41 %)\n",
      "Using 339312 / 345367 tokens available (98.25 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "caption_tokens = caption_tokenizer.texts_to_sequences(train_captions)\n",
    "target_tokens = target_tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parallel dataset\n",
    "\n",
    "prepare captions and target to support parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_parallel_dataset(img_paths, caption_tokens, target_tokens):\n",
    "    \n",
    "#     dataset_img_paths = []\n",
    "#     dataset_captions = []\n",
    "#     dataset_target = []\n",
    "\n",
    "#     for i in tqdm(range(0, len(train_img_paths))):\n",
    "#         img = img_paths[i]\n",
    "#         cap = caption_tokens[i]\n",
    "#         tar = target_tokens[i]\n",
    "\n",
    "#         for j in range(1, len(cap)):\n",
    "#             dataset_img_paths.append(img)\n",
    "#             dataset_captions.append(cap[:j])\n",
    "#             dataset_target.append(tar[j])\n",
    "\n",
    "#     dataset_captions = pad_sequences(dataset_captions, maxlen=PARAMS[\"max_caption_length\"], padding='post')\n",
    "    \n",
    "#     return dataset_img_paths, dataset_captions, dataset_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parallel_dataset(img_paths, caption_tokens, target_tokens):\n",
    "    \n",
    "    dataset_img = []\n",
    "    dataset_captions = []\n",
    "    dataset_target = []\n",
    "\n",
    "    for i in tqdm(range(0, len(img_paths))):\n",
    "#         img_tensor = np.load(img_paths[i] + '.npy')\n",
    "        img_tensor = img_paths[i]\n",
    "        cap = caption_tokens[i]\n",
    "        tar = target_tokens[i]\n",
    "\n",
    "        for j in range(1, len(cap)):\n",
    "            dataset_img.append(img_tensor)\n",
    "            dataset_captions.append(cap[:j])\n",
    "            dataset_target.append(tar[j])\n",
    "\n",
    "    dataset_captions = pad_sequences(dataset_captions, maxlen=PARAMS[\"max_caption_length\"], padding='post')\n",
    "    \n",
    "    return dataset_img, dataset_captions, dataset_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:00<00:00, 30101.80it/s]\n"
     ]
    }
   ],
   "source": [
    "parallel_img, parallel_captions, parallel_target = preprocess_parallel_dataset(train_img_paths, caption_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = parallel_captions.shape[0]\n",
    "MAX_CAPTION_LENGTH = parallel_captions.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 256293)\n",
      "eval  : 64074)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_image_train, X_image_eval, X_caption_train, X_caption_eval, y_train, y_eval = train_test_split(parallel_img, parallel_captions, parallel_target, test_size=0.2, random_state=42)\n",
    "# X_image_train, X_image_test, X_caption_train, X_caption_test, y_train, y_test = train_test_split(X_image_train, X_caption_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"train : {})\".format(len(X_image_train)))\n",
    "print(\"eval  : {})\".format(len(X_image_eval)))\n",
    "# print(\"test  : {})\".format(len(X_image_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset generator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, image_ids, captions, targets, batch_size=32, dim=(100, 2048), seq_len=25, shuffle=True):\n",
    "\n",
    "        self.image_ids = image_ids\n",
    "        self.captions = captions\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.seq_len = seq_len\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.image_ids) / self.batch_size))\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.image_ids))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            \n",
    "    def __data_generation(self, indexes):\n",
    "        \n",
    "        X_captions = np.array([self.captions[k] for k in indexes])\n",
    "        y = np.array([self.targets[k] for k in indexes])\n",
    "        \n",
    "        # Initialization\n",
    "        X_images = np.empty((self.batch_size, *self.dim))\n",
    "\n",
    "        # Generate data\n",
    "        image_ids_temp = [self.image_ids[k] for k in indexes]\n",
    "        for i in range(0, len(image_ids_temp)):\n",
    "            X_images[i,] = np.load(image_ids_temp[i] + '.npy')\n",
    "            \n",
    "        return {\n",
    "            \"input_image\" : X_images, \n",
    "            \"input_caption\" : X_captions \n",
    "        }, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'dim': (100, 2048),\n",
    "    'seq_len': 25, \n",
    "    'shuffle': True,\n",
    "}\n",
    "\n",
    "training_generator = DataGenerator(X_image_train, X_caption_train, y_train, **params)\n",
    "validation_generator = DataGenerator(X_image_eval, X_caption_eval, y_eval, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Dense, LeakyReLU, BatchNormalization, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(Layer):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim)\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # x => (batch_size, 100, 2048)\n",
    "        x = self.fc(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, image_feature_size, image_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(Layer):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, img_feature_size, image_context_size)\n",
    "        hidden                        => (batch_size, embedding_size)\n",
    "        \n",
    "        note : \n",
    "        img_feature_size ==  64 for Inception V3,\n",
    "        img_feature_size == 100 for Xception,\n",
    "        \"\"\"\n",
    "        \n",
    "        features, hidden = inputs\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        _w1 = self.W1(features)\n",
    "        _w2 = self.W2(hidden_with_time_axis)\n",
    "        score = tf.nn.tanh(_w1 + _w2)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        context_vector = tf.expand_dims(context_vector, 1)\n",
    "        context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "        \n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_with_time_axis      => (batch_size, 1, embedding_size)\n",
    "        score                      => (batch_size, img_feature_size, units)\n",
    "        attention_weights          => (batch_size, img_feature_size, 1)\n",
    "        context_vector (after sum) => (batch_size, img_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class BertEmbedding(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BertEmbedding, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding = TFBertModel.from_pretrained('bert-base-uncased', proxies=PROXIES)\n",
    "        self.embedding.trainable = False\n",
    "        self.embedding_dim = self.embedding.config.hidden_size\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super(BertEmbedding, self).get_config()\n",
    "        config.update({'embedding_dim': self.embedding_dim})\n",
    "        return config  \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        token_type_ids = tf.cast((inputs == 0), tf.int32)\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        hidden_states, sentence_embedding = self.embedding(inputs=inputs, token_type_ids=token_type_ids)\n",
    "        \n",
    "        return hidden_states, sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbedding(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size=5000, \n",
    "                 embedding_type=\"default\", \n",
    "                 embedding_dim=256, \n",
    "                 mask_zero=True,\n",
    "                 return_sentence_embedding=False, \n",
    "                 **kwargs):\n",
    "        \n",
    "        super(CustomEmbedding, self).__init__(**kwargs)\n",
    "        self.type = embedding_type\n",
    "        self.return_sentence_embedding = return_sentence_embedding\n",
    "        \n",
    "        if self.type == \"BERT\":\n",
    "            self._bert_emb = BertEmbedding()\n",
    "        else:\n",
    "            self._default_emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=mask_zero)\n",
    " \n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        if self.type == \"BERT\": \n",
    "            output, sentence_embedding = self._bert_emb(x)\n",
    "        else:\n",
    "            output = self._default_emb(x)\n",
    "            sentence_embedding = tf.reduce_mean(output, 1)\n",
    "\n",
    "        if self.return_sentence_embedding:\n",
    "            output = sentence_embedding\n",
    "        \n",
    "        \"\"\"\n",
    "        embedding (tokens)   => (batch_size, tokens_length, embedding_dim)\n",
    "        embedding (sentence) => (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout\n",
    "\n",
    "\n",
    "class RNN_Encoder(Layer):\n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=256):\n",
    "        \n",
    "        super(RNN_Encoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            self.bilstm = Bidirectional(LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "    \n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            output, h_state, c_state = self.lstm(x)\n",
    "            hidden = [h_state, c_state]\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            output, fwd_h, fwd_c, bkwd_h, bkwd_c = self.bilstm(x)\n",
    "            hidden = [fwd_h, fwd_c, bkwd_h, bkwd_c]\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, hidden = self.gru(x)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "        \"\"\"\n",
    "        output => (batch_size, rnn_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class Position_Embedding(Layer):\n",
    "\n",
    "    def __init__(self, size=None, mode='sum', **kwargs):\n",
    "        self.size = size  # 必须为偶数\n",
    "        self.mode = mode\n",
    "        super(Position_Embedding, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        if (self.size == None) or (self.mode == 'sum'):\n",
    "            self.size = int(x.shape[-1])\n",
    "            \n",
    "        batch_size, seq_len = K.shape(x)[0], K.shape(x)[1]\n",
    "        \n",
    "        position_j = 1. / K.pow(10000., 2 * K.arange(self.size / 2, dtype='float32') / self.size)\n",
    "        position_j = K.expand_dims(position_j, 0)\n",
    "        position_i = K.cumsum(K.ones_like(x[:, :, 0]), 1) - 1  # K.arange不支持变长，只好用这种方法生成\n",
    "        position_i = K.expand_dims(position_i, 2)\n",
    "        position_ij = K.dot(position_i, position_j)\n",
    "        position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2)\n",
    "        \n",
    "        if self.mode == 'sum':\n",
    "            return position_ij + x\n",
    "        \n",
    "        elif self.mode == 'concat':\n",
    "            return K.concatenate([position_ij, x], 2)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \n",
    "        if self.mode == 'sum':\n",
    "            return input_shape\n",
    "        \n",
    "        elif self.mode == 'concat':\n",
    "            return (input_shape[0], input_shape[1], input_shape[2] + self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCombineLayer(Layer):\n",
    "    \n",
    "    def __init__(self, strategy='concat', **kwargs):\n",
    "        super(CustomCombineLayer, self).__init__()\n",
    "        \n",
    "        if strategy == \"add\":\n",
    "            self.combine = tf.keras.layers.Add()\n",
    "        else:\n",
    "            self.combine = tf.keras.layers.Concatenate(axis=-1)\n",
    "            \n",
    "    def call(self, x):\n",
    "        return self.combine(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class RNN_Decoder(Layer):\n",
    "\n",
    "    def __init__(self, units=256, vocab_size=3000):\n",
    "\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.dropout = Dropout(0.2)\n",
    "\n",
    "        self.fc1 = Dense(units)\n",
    "        self.fc2 = Dense(vocab_size, activation=\"softmax\")\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.leakyrelu(x)\n",
    "\n",
    "        x = tf.reshape(x, (-1, x.shape[1] * x.shape[2]))\n",
    "        x = self.dropout(x)\n",
    "        word_predictions = self.fc2(x)\n",
    "\n",
    "        return word_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0415 01:47:20.051019 140569263748928 configuration_utils.py:256] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/m13516112/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0415 01:47:20.052591 140569263748928 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0415 01:47:21.228335 140569263748928 modeling_tf_utils.py:333] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5 from cache at /home/m13516112/.cache/torch/transformers/d667df51ec24c20190f01fb4c20a21debc4c4fc12f7e2f5441ac0a99690e3ee9.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5\n",
      "I0415 01:47:22.879848 140569263748928 modeling_tf_utils.py:375] Layers from pretrained model not used in TFBertModel: ['nsp___cls', 'mlm___cls']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, 100, 2048)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cnn__encoder (CNN_Encoder)      (None, 100, 256)     525568      input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_caption (InputLayer)      [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "rnn__decoder (RNN_Decoder)      (None, 3000)         76869816    cnn__encoder[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 77,395,384\n",
      "Trainable params: 77,394,360\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, LeakyReLU, \\\n",
    "                                    Embedding, Bidirectional, LSTM, Concatenate, GlobalAveragePooling1D, Dropout\n",
    "\n",
    "\n",
    "input_image_shape = tuple(list(IMAGE_FEATURE_SHAPE) + [])\n",
    "input_image = Input(shape=input_image_shape, name=\"input_image\")\n",
    "input_caption = Input(shape=(PARAMS[\"max_caption_length\"],), name=\"input_caption\", dtype=\"int32\")\n",
    "\n",
    "embedding = CustomEmbedding(vocab_size=PARAMS[\"vocab_size\"], embedding_dim=256, embedding_type=\"BERT\", mask_zero=True, return_sentence_embedding=False)(input_caption)\n",
    "embedding = Position_Embedding()(embedding)\n",
    "\n",
    "# encoder\n",
    "enc_caption, rnn_hidden = RNN_Encoder(rnn_type=PARAMS[\"rnn_type\"], rnn_units=PARAMS[\"rnn_units\"])(embedding)\n",
    "rnn_hidden = Concatenate(axis=-1)(rnn_hidden) \n",
    "\n",
    "enc_image = CNN_Encoder(output_dim=PARAMS[\"image_context_size\"])(input_image)\n",
    "context_vector, _ = BahdanauAttention(units=256)([enc_image, rnn_hidden])\n",
    "enc_output = CustomCombineLayer(strategy='concat')([context_vector, enc_caption])\n",
    "\n",
    "output = RNN_Decoder(units=256, vocab_size=PARAMS[\"vocab_size\"])(enc_image)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=[input_image, input_caption], outputs=output)\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "adam = tf.keras.optimizers.Adam()\n",
    "model.compile(loss=loss, optimizer=adam)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0415 01:47:29.990382 140569263748928 data_adapter.py:1091] sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "W0415 01:47:31.064516 140569263748928 data_adapter.py:1091] sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 8009 steps, validate for 2002 steps\n",
      "Epoch 1/5\n",
      "  74/8009 [..............................] - ETA: 1:59:04 - loss: 9.1374"
     ]
    }
   ],
   "source": [
    "hist = model.fit(training_generator,\n",
    "                validation_data=validation_generator,\n",
    "                use_multiprocessing=False,\n",
    "                epochs=5,\n",
    "                workers=6, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for label in [\"loss\", \"val_loss\"]:\n",
    "    plt.plot(hist.history[label], label=label)\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def choose_predicted_id(predictions, strategy=\"max\", sampling_k=10):\n",
    "    \"\"\"\n",
    "    predictions : encoder word prediction => (batch_size, vocab_size)\n",
    "    strategy    : how to choose word [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sampling method (categorical dist)\n",
    "    if strategy == \"sample\":\n",
    "        \n",
    "        # sampled_proba & sampled_ids => (batch_size, sampling_k)\n",
    "        sampled_proba, sampled_ids = tf.math.top_k(predictions, sampling_k)\n",
    "        \n",
    "        # chosen_sampled_col => (batch_size, )\n",
    "        chosen_sampled_col = tf.squeeze(tf.random.categorical(sampled_proba, 1))\n",
    "        \n",
    "        # create row idx to zip with chosen_sampled_col\n",
    "        row_idx = tf.range(predictions.shape[0], dtype=chosen_sampled_col.dtype)\n",
    "        row_col_idx = tf.stack([row_idx, chosen_sampled_col], axis=1)\n",
    "        \n",
    "        # predicted_ids => (batch_size, )\n",
    "        predicted_ids = tf.gather_nd(sampled_ids, row_col_idx)\n",
    "\n",
    "    # Max index method\n",
    "    else:\n",
    "        predicted_ids = tf.argmax(predictions, 1)\n",
    "    \n",
    "    # predicted_ids => (batch_size, )\n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(images_paths):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [load_image(x)[0] for x in images_paths]\n",
    "    \n",
    "    # x => (batch_size, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, 10, 10, 2048)\n",
    "    x = extractor(x)\n",
    "    \n",
    "    # x  => (batch_size, img_feature_size, 2048)\n",
    "    x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = cnn_encoder(x)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def get_supporting_features(images_paths, strategy=\"mean\"):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, img_count, 1)\n",
    "    strategy : strategy to aggregate multiple supporting image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [[load_image(x)[0] for x in images_set] for images_set in images_paths]\n",
    "    \n",
    "    # x => (batch_size, img_count, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, img_count, 10, 10, 2048)\n",
    "    x = [extractor(image_set) for image_set in x]\n",
    "    \n",
    "    # features => (batch_size, img_count, img_feature_size, image_context_size)\n",
    "    features = cnn_encoder(x)\n",
    "    \n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    if strategy == \"logsumexp\":\n",
    "        features = tf.reduce_logsumexp(features, 1)\n",
    "    elif strategy == \"max\":\n",
    "        features = tf.reduce_max(features, 1)\n",
    "    elif strategy == \"min\":\n",
    "        features = tf.reduce_min(features, 1)\n",
    "    else:\n",
    "        features = tf.reduce_mean(features, 1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_indices(support_text):\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(0, len(support_text)):\n",
    "        context_token = target_tokenizer.tokenize(support_text[i])\n",
    "        context_token_id = target_tokenizer.convert_tokens_to_ids(context_token)\n",
    "        context_token_id = set(context_token_id)\n",
    "        context_token_id.discard(0)\n",
    "        for x in sorted(context_token_id):\n",
    "            indices.append([i, x])\n",
    "\n",
    "    # return => (word_count, 2)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_supporting_text_vector(support_text, vocab_size):\n",
    "    \"\"\"\n",
    "    support_text : list of text describing main image context => (batch_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(support_text)\n",
    "    \n",
    "    # indices => ( sum(batch_size * ?word_count), 2)\n",
    "    indices = get_one_hot_indices(support_text)\n",
    "    values = tf.ones(len(indices))\n",
    "    sparse_one_hot = tf.sparse.SparseTensor(indices, values, dense_shape=[batch_size, vocab_size])\n",
    "    \n",
    "    # sparse_one_hot => (batch_size, vocab_size)\n",
    "    return sparse_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluate(images_paths,\n",
    "                    support_text=None,\n",
    "                    support_imgs=None, \n",
    "                    support_aggregate_strategy=\"mean\",\n",
    "                    pplm_iteration=3,\n",
    "                    pplm_weight=0.03,\n",
    "                    pplm_gm_weight=0.8,\n",
    "                    choose_word_strategy=\"sample\",\n",
    "                   ):\n",
    "    \n",
    "    \"\"\"\n",
    "    images_paths : list of image_path                           => (batch_size, 1)\n",
    "    support_text : list of text describing main image context   => (batch_size)\n",
    "    support_imgs : list of list of image_path                   => (batch_size, image_count, 1)\n",
    "    support_aggregate_strategy : how to aggregate support image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    pplm_iteration : number of pplm step done for every decoding step\n",
    "    pplm_weight    : weight of pplm loss\n",
    "    pplm_gm_weight : geometric mean fusion weight (0 means use only original prediction, 1 means use only pplm prediction)\n",
    "    choose_word_strategy : how to choose word from prediction distribution [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(images_paths)\n",
    "    \n",
    "    # initialize captions placeholder\n",
    "    start_token = caption_tokenizer.convert_tokens_to_ids(['[CLS]']) # use bert id's not custom id\n",
    "    result_captions = tf.tile(tf.expand_dims(start_token, 1), [batch_size, 1])\n",
    "    attention_plot = tf.reshape([], shape=(batch_size, 0, IMAGE_FEATURE_SHAPE[0]))\n",
    "    \n",
    "    # Extract features from main images\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = get_image_features(images_paths)\n",
    "    \n",
    "    \n",
    "    if support_text is not None:\n",
    "        # support_text_vector => (batch_size, vocab_size)\n",
    "        support_text_vector = get_supporting_text_vector(support_text, decoder.vocab_size) \n",
    "    else:\n",
    "        # set all pplm related variable to 0\n",
    "        pplm_iteration = 0\n",
    "        pplm_weight = 0\n",
    "        pplm_gm_weight = 0\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(MAX_CAPTION_LENGTH)):\n",
    "        \n",
    "        # decoder_input => (batch_size, ~MAX_CAPTION_LENGTH)\n",
    "        caption = pad_sequences(result_captions, maxlen=MAX_CAPTION_LENGTH, padding=\"post\")\n",
    "        \n",
    "        # hidden => (batch_size, embedding_size)\n",
    "        text_features, hidden = rnn_encoder(caption)\n",
    "\n",
    "        # context_vector => (batch_size, image_context_size)\n",
    "        # attention_weights => (batch_size, img_feature_size, 1)\n",
    "        context_vector, attention_weights = attention(features, hidden)\n",
    "\n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions = decoder(text_features, context_vector)  \n",
    "        \n",
    "        \n",
    "        # ======================== PPLM section ========================\n",
    "        ori_prediction = predictions\n",
    "        \n",
    "        curr_pertubation = tf.Variable(tf.zeros((batch_size, rnn_encoder.embedding_dim)), name=\"curr_pertubation\", trainable=True)\n",
    "        \n",
    "        \n",
    "        for j in range(pplm_iteration):\n",
    "            \n",
    "            with tf.GradientTape() as pplm_tape: \n",
    "                \n",
    "                hidden += curr_pertubation\n",
    "                \n",
    "                context_vector, attention_weights = attention(features, hidden)\n",
    "                predictions = decoder(text_features, context_vector)  \n",
    "                pplm_loss = pplm_loss_function(support_text_vector, predictions, pplm_weight=pplm_weight)\n",
    "                \n",
    "            \"\"\"\n",
    "            most impactfull layer to train = last dense layer\n",
    "            \"\"\"\n",
    "            \n",
    "            trainable_variables = [curr_pertubation]\n",
    "            gradients = pplm_tape.gradient(pplm_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "            \n",
    "            predictions = decoder(text_features, context_vector)  \n",
    "        \n",
    "        # fuse final pplm_prediction and original prediction\n",
    "        fused_predictions = (predictions * pplm_gm_weight) + (ori_prediction * (1 - pplm_gm_weight)) \n",
    "        \n",
    "        # predicted_ids => (batch_size,)\n",
    "        predicted_ids = choose_predicted_id(fused_predictions, strategy=choose_word_strategy)\n",
    "        \n",
    "        # convert custom id mapping to bert's id\n",
    "        # predicted_bert_ids => (batch_size, 1)\n",
    "        predicted_bert_ids = target_tokenizer._convert_custom_id_to_bert_id(predicted_ids.numpy())\n",
    "        predicted_bert_ids = tf.expand_dims(predicted_bert_ids, 1)\n",
    "        \n",
    "        # store result\n",
    "        result_captions = tf.concat([result_captions, predicted_bert_ids], axis=1)\n",
    "        \n",
    "        # attention_weights => (batch_size, 1, img_feature_size)\n",
    "        attention_weights = tf.reshape(attention_weights, shape=(batch_size, 1, -1))\n",
    "        \n",
    "        # assign attention weights to respective generated word\n",
    "        # attention_plot => (batch_size, ~max_caption_len, feature_size)\n",
    "        attention_plot = tf.concat([attention_plot, attention_weights], axis=1)\n",
    "        \n",
    "    # remove start token & revert to tokens\n",
    "    result_captions = [caption_tokenizer.convert_ids_to_tokens(x[1:]) for x in result_captions]\n",
    "    \n",
    "    return result_captions, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy, MeanSquaredError\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def pplm_loss_function(real, pred, pplm_weight=0.03):\n",
    "    \"\"\"\n",
    "    real  => (batch_size, vocab_size)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "\n",
    "#     mm = tf.sparse.sparse_dense_matmul(real, tf.transpose(pred))\n",
    "#     mm = tf.reduce_sum(tf.abs(mm), 1)\n",
    "#     loss = tf.reduce_sum(mm, 0)\n",
    "    \n",
    "    real = tf.sparse.to_dense(real, default_value=0)\n",
    "    pplm_loss = CategoricalCrossentropy(from_logits=True)\n",
    "    loss = pplm_loss(real, pred, pplm_weight)\n",
    "    \n",
    "#     print(loss)\n",
    "    return loss\n",
    "\n",
    "    \"\"\"\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    for (img_tensor, captions, target, _) in tqdm(train_dataset):\n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss.append(batch_loss.numpy())\n",
    "        \n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(loss[-500:]) / len(loss[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(loss[-5000:]) / len(loss[-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:00<00:00, 706203.14it/s]\n"
     ]
    }
   ],
   "source": [
    "true_captions = {}\n",
    "for annot in tqdm(annotations[\"annotations\"]):\n",
    "    image_id = annot[\"image_id\"]\n",
    "\n",
    "    if image_id not in true_captions:\n",
    "        true_captions[image_id] = []\n",
    "    \n",
    "    true_captions[image_id].append(annot['caption'])\n",
    "    \n",
    "    \n",
    "def get_true_captions(image_paths):\n",
    "    return [true_captions[int(x[-10:-4])] for x in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-20b2e40e2e7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_img_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m55\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtruth_caption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_true_captions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-809cdcd3c648>\u001b[0m in \u001b[0;36mcustom_evaluate\u001b[0;34m(images_paths, support_text, support_imgs, support_aggregate_strategy, pplm_iteration, pplm_weight, pplm_gm_weight, choose_word_strategy)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Extract features from main images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# features => (batch_size, img_feature_size, image_context_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-98106ff51d66>\u001b[0m in \u001b[0;36mget_image_features\u001b[0;34m(images_paths)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# features => (batch_size, img_feature_size, image_context_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[50:55]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images)\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = all_img_paths[5:9]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_strategy=\"max\")\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train/{}\".format(str(datetime.now())[:-10])\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           attention=attention,\n",
    "                           optimizer=optimizer\n",
    "                          )\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------\")\n",
    "print(checkpoint_path)\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(checkpoint_path)\n",
    "with open(checkpoint_path + \"/config.txt\", \"w\") as f:\n",
    "    f.write(str(PARAMS))\n",
    "    \n",
    "log_file = open(checkpoint_path + \"/log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, PARAMS[\"epoch\"]):\n",
    "    \n",
    "    start = time.time()\n",
    "    loss = 0\n",
    "    batch = 1\n",
    "\n",
    "    for img_tensor, captions, target in tqdm(train_dataset):\n",
    "        \n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "\n",
    "            with open(checkpoint_path + \"/log.txt\", \"a\") as f:\n",
    "                log_message = ' {} Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                    checkpoint_path, epoch + 1, batch, batch_loss.numpy())\n",
    "                f.write(str(log_message + \"\\n\"))\n",
    "                print(log_message)   \n",
    "\n",
    "        batch += 1\n",
    "        \n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(batch_loss.numpy())\n",
    "\n",
    "#     ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = []\n",
    "\n",
    "# for (img_tensor, captions, target) in tqdm(train_dataset):\n",
    "#     batch_loss = train_step(img_tensor, captions, target)\n",
    "#     loss.append(batch_loss.numpy())\n",
    "    \n",
    "# plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PPLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "# text = [\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "# ]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images, support_text=text, pplm_iteration=5, pplm_weight=1)\n",
    "# result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
