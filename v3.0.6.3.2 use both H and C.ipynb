{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git add \"Keras.ipynb\"\n",
    "!git commit -m \"test pred using pplm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"rnn_units\": 256,\n",
    "    \"rnn_type\": \"BiLSTM\",\n",
    "    \"tokenizer\": \"BERT\",\n",
    "    \"word_embedding\": \"BERT\",\n",
    "    \"vocab_size\": 3000,\n",
    "    \"combine_strategy\": \"merge\",\n",
    "    \"combine_layer\": \"concat\",\n",
    "    \"image_context_size\": 256,\n",
    "    \"word_embedding_dim\": 256,\n",
    "    \"batch_size\": 32,\n",
    "    \"data_size\": 5000,\n",
    "    \"use_mapping\": True,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_caption_length\": 25, # use <int> or None\n",
    "    \"image_feature_extractor\": \"xception\",\n",
    "    \"use_sequence\": True,\n",
    "    \"epoch\": 20,\n",
    "    \"use_mask\": True,\n",
    "    \"version\": \"v5 : positional embedding\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "\n",
    "# for TFBertModel\n",
    "PROXIES = {\n",
    "  \"http\": \"http://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "  \"https\": \"https://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"[CLS] \" + annot['caption'] + \" [SEP]\"\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "\n",
    "stopper = -1 if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]\n",
    "train_captions = all_captions[:stopper]\n",
    "train_img_paths = all_img_paths[:stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 5000\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = len(train_captions) if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature_extractor(model_type=\"xception\"):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.compat.v1.keras.applications.xception\n",
    "        cnn_model = tf.compat.v1.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"CNN encoder model not supported yet\")\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "def get_image_feature_shape(model_type):\n",
    "    \n",
    "    if model_type == \"xception\":\n",
    "        return (100, 2048)\n",
    "    elif model_type == \"inception_v3\":\n",
    "        return (64, 2048)\n",
    "    else:\n",
    "        raise Exception (\"model unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = extractor_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FEATURE_SHAPE = get_image_feature_shape(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extractor, extractor_preprocessor = get_image_feature_extractor(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image feature\n",
    "\n",
    "this step mostly skipped since the results are cached already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 157.25\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = PARAMS[\"batch_size\"]\n",
    "\n",
    "estimated_batch_count = DATA_SIZE / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique images\n",
    "# unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# # Prepare dataset\n",
    "# image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "# image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "# image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessed image (batch)\n",
    "\n",
    "# for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "#     # get context vector of batch images\n",
    "#     batch_features = extractor(batch_imgs)\n",
    "    \n",
    "#     # flatten 2D cnn result into 1D for RNN decoder input\n",
    "#     # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "#     # image_feature = 100 (Xception)\n",
    "#     # image_feature = 64 (Inception V3)\n",
    "#     batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "#     # Cache preprocessed image\n",
    "#     for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "#         image_path = image_path.numpy().decode(\"utf-8\")\n",
    "#         np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Caption dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 14:45:45.925558 139858796947264 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0402 14:45:45.926692 139858796947264 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = PARAMS[\"tokenizer\"]\n",
    "VOCAB_SIZE = PARAMS[\"vocab_size\"]  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "    \n",
    "    def use_custom_mapping(self, use_mapping=True, vocab_size=3000):\n",
    "        \n",
    "        self.use_mapping = use_mapping\n",
    "        self.cust_vocab_size = vocab_size\n",
    "        self.mapping_initialized = False\n",
    "\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into custom indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        bert_ids = [self.convert_tokens_to_ids(self.tokenize(x)) for x in tqdm(texts)]\n",
    "        \n",
    "        if not self.use_mapping:\n",
    "            return bert_ids\n",
    "        \n",
    "        if not self.mapping_initialized:\n",
    "            self._initialize_custom_mapping(bert_ids)\n",
    "            return [self._convert_bert_id_to_custom_id(x) for x in tqdm(bert_ids)]\n",
    "        \n",
    "        return bert_ids\n",
    "    \n",
    "        \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \n",
    "        bert_ids = super().convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            return self._convert_bert_id_to_custom_id(bert_ids)\n",
    "        else:\n",
    "            return bert_ids\n",
    "        \n",
    "        \n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            bert_ids = self._convert_custom_id_to_bert_id(token_ids)\n",
    "        else:\n",
    "            bert_ids = token_ids\n",
    "            \n",
    "        bert_tokens = super().convert_ids_to_tokens(bert_ids)\n",
    "        return bert_tokens\n",
    "    \n",
    "    \n",
    "    def _initialize_custom_mapping(self, corpus_bert_ids):\n",
    "        \n",
    "        print(\"    > constructing custom mapping < \\n\")\n",
    "        self._build_occurence_table(corpus_bert_ids)\n",
    "        self._build_custom_mapping_table()\n",
    "        self.mapping_initialized = True\n",
    "        \n",
    "        \n",
    "    def _build_occurence_table(self, tokenized_captions):\n",
    "        \"\"\"\n",
    "        build dict of token frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        self.occurence_table = {}\n",
    "        for caption in tqdm(tokenized_captions):\n",
    "            for token in caption:\n",
    "                if token not in self.occurence_table:\n",
    "                    self.occurence_table[token] = 0\n",
    "                self.occurence_table[token] += 1\n",
    "                \n",
    "    \n",
    "    def _build_custom_mapping_table(self):\n",
    "        \n",
    "        _special_token = ['[UNK]', '[PAD]']\n",
    "        _actual_vocab_size = self.cust_vocab_size - len(_special_token)\n",
    "        \n",
    "        sorted_occurence = {k: v for k, v in sorted(\n",
    "            self.occurence_table.items(), reverse=True, key=lambda item: item[1]\n",
    "        )}\n",
    "        \n",
    "        used_tokens = sorted(list(sorted_occurence)[:_actual_vocab_size])\n",
    "        mapping_size = min(len(used_tokens), _actual_vocab_size)\n",
    "        \n",
    "        _bert_pad = 0\n",
    "        _bert_oov = 100\n",
    "        self._custom_pad = 0\n",
    "        self._custom_oov = mapping_size + 1\n",
    "        \n",
    "        self.bert_id_to_custom_id = {\n",
    "            _bert_pad: self._custom_pad, \n",
    "            _bert_oov: self._custom_oov\n",
    "        }\n",
    "        self.custom_id_to_bert_id = {\n",
    "            self._custom_pad: _bert_pad, \n",
    "            self._custom_oov: _bert_oov\n",
    "        }\n",
    "        \n",
    "        for i in range(0, mapping_size):\n",
    "            bert_token = used_tokens[i]\n",
    "            self.bert_id_to_custom_id[bert_token] = i + 1    \n",
    "            self.custom_id_to_bert_id[i + 1] = bert_token\n",
    "            \n",
    "        print(\"Vocab contains {0} / {1} unique tokens ({2:.2f} %)\".format(\n",
    "            len(used_tokens) + 2,\\\n",
    "            len(sorted_occurence),\\\n",
    "            (len(used_tokens) / len(sorted_occurence) * 100)\n",
    "        ))\n",
    "        \n",
    "        sorted_occurence_count = list(sorted_occurence.values())\n",
    "        used_tokens_count = sum(sorted_occurence_count[:_actual_vocab_size])\n",
    "        total_tokens_count = sum(sorted_occurence_count)\n",
    "        \n",
    "        print(\"Using {0} / {1} tokens available ({2:.2f} %)\".format(\n",
    "            used_tokens_count,\\\n",
    "            total_tokens_count,\\\n",
    "            (used_tokens_count / total_tokens_count * 100)\n",
    "        ))        \n",
    "        \n",
    "    def _convert_bert_id_to_custom_id(self, token_ids):\n",
    "        return [self.bert_id_to_custom_id[x] if x in self.bert_id_to_custom_id else self._custom_oov for x in token_ids]\n",
    "    \n",
    "    def _convert_custom_id_to_bert_id(self, token_ids):\n",
    "        return [self.custom_id_to_bert_id[x] for x in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_type, use_mapping, vocab_size):\n",
    "    \n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    if tokenizer_type == \"BERT\" :\n",
    "        tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "        tokenizer.use_custom_mapping(use_mapping, vocab_size)\n",
    "\n",
    "    # use default keras tokenizer\n",
    "    else : \n",
    "        tokenizer = TokenizerWrapper(num_words=vocab_size, oov_token=\"[UNK]\")\n",
    "        tokenizer.fit_on_texts(train_captions)    \n",
    "        tokenizer.word_index['[PAD]'] = 0\n",
    "        tokenizer.index_word[0] = '[PAD]'\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 14:45:47.706774 139858796947264 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0402 14:45:49.092490 139858796947264 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "caption_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=False,\n",
    "    vocab_size=0\n",
    ")\n",
    "\n",
    "target_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=PARAMS[\"use_mapping\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 3671.35it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 3784.00it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 345933.39it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 353359.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > constructing custom mapping < \n",
      "\n",
      "Vocab contains 3000 / 3584 unique tokens (83.65 %)\n",
      "Using 68179 / 68765 tokens available (99.15 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "caption_tokens = caption_tokenizer.texts_to_sequences(train_captions)\n",
    "target_tokens = target_tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parallel dataset\n",
    "\n",
    "prepare captions and target to support parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parallel_dataset(img_paths, caption_tokens, target_tokens):\n",
    "    \n",
    "    dataset_img_paths = []\n",
    "    dataset_captions = []\n",
    "    dataset_target = []\n",
    "    dataset_target_position = []\n",
    "\n",
    "    for i in tqdm(range(0, len(train_img_paths))):\n",
    "        img = img_paths[i]\n",
    "        cap = caption_tokens[i]\n",
    "        tar = target_tokens[i]\n",
    "\n",
    "        for j in range(1, len(cap)):\n",
    "            dataset_img_paths.append(img)\n",
    "            dataset_captions.append(cap[:j])\n",
    "            dataset_target.append(tar[j])\n",
    "            dataset_target_position.append(j)\n",
    "\n",
    "    dataset_captions = pad_sequences(dataset_captions, maxlen=PARAMS[\"max_caption_length\"], padding='post')\n",
    "    \n",
    "    return dataset_img_paths, dataset_captions, dataset_target, dataset_target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 24913.60it/s]\n"
     ]
    }
   ],
   "source": [
    "parallel_img_paths, parallel_captions, parallel_target, parallel_target_position = preprocess_parallel_dataset(train_img_paths, caption_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = parallel_captions.shape[0]\n",
    "MAX_CAPTION_LENGTH = parallel_captions.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_target_position = tf.one_hot(parallel_target_position, depth=MAX_CAPTION_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "\n",
    "def load_dataset(img_name, caption, target, target_pos):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, caption, target, target_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((parallel_img_paths, parallel_captions, parallel_target, parallel_target_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "\n",
    "dataset = dataset.map(lambda item1, item2, item3, item4: tf.numpy_function(\n",
    "          load_dataset, [item1, item2, item3, item4], [tf.float32, tf.int32, tf.int32, tf.float32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(DATA_SIZE / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 16, image_feature = 100, 2048)\n",
    "# caption => (batch_size = 16, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1395 batches, (total : 44640)\n",
      "eval : 298 batches, (total : 9536)\n",
      "test : 300 batches, (total : 9600 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captioning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=PARAMS[\"learning_rate\"])\n",
    "# loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    real  => (batch_size,)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate loss\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # create mask to filter out padding token \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    # Ignore loss_ if real token is padding\n",
    "    loss_ *= mask\n",
    "    \n",
    "    # Get mean of curren batch's loss (somewhat batch norm)\n",
    "    result_loss = tf.reduce_mean(loss_)\n",
    "    \n",
    "    return result_loss\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    loss_  => (batch_size, 1)\n",
    "    mask   => (batch_size, 1)  : indicate is padding or not\n",
    "\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim)\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # x => (batch_size, 100, 2048)\n",
    "        x = self.fc(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, image_feature_size, image_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, img_feature_size, image_context_size)\n",
    "        hidden                        => (batch_size, embedding_size)\n",
    "        \n",
    "        note : \n",
    "        img_feature_size ==  64 for Inception V3,\n",
    "        img_feature_size == 100 for Xception,\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        _w1 = self.W1(features)\n",
    "        _w2 = self.W2(hidden_with_time_axis)\n",
    "        score = tf.nn.tanh(_w1 + _w2)\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_with_time_axis      => (batch_size, 1, embedding_size)\n",
    "        score                      => (batch_size, img_feature_size, units)\n",
    "        attention_weights          => (batch_size, img_feature_size, 1)\n",
    "        context_vector (after sum) => (batch_size, img_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rnn Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class RNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=256, \n",
    "                 embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 vocab_size=3000, batch_size=32):\n",
    "        \n",
    "        super(RNN_Encoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # =====================================================\n",
    "        \n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "        \n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "\n",
    "    def _init_embedding(self):\n",
    "        \n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.bert_embedding = TFBertModel.from_pretrained('bert-base-uncased', proxies=PROXIES)\n",
    "            self.bert_embedding.trainable = False\n",
    "            self.embedding_dim = self.bert_embedding.config.hidden_size\n",
    "            \n",
    "        else:\n",
    "            self.default_embedding = Embedding(\n",
    "                input_dim=self.vocab_size, \n",
    "                output_dim=self.embedding_dim, \n",
    "                mask_zero=True,\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def _init_rnn(self):\n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector'\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            self.bilstm = Bidirectional(LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        \n",
    "    def embedding(self, tokens, as_sentence=False):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embedding_type == \"BERT\": \n",
    "            embedding, sentence_embedding = self._bert_embedding(tokens)\n",
    "        \n",
    "        else:\n",
    "            embedding = self._default_embedding(tokens)\n",
    "            sentence_embedding = tf.reduce_mean(embedding, 1)\n",
    "            \n",
    "\n",
    "        if as_sentence:\n",
    "            # embedding => (batch_size, embedding_dim)\n",
    "            return sentence_embedding\n",
    "        \n",
    "        else:\n",
    "            # embedding => (batch_size, tokens_length, embedding_dim)\n",
    "            return embedding\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        embedding (tokens)   => (batch_size, tokens_length, embedding_dim)\n",
    "        embedding (sentence) => (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def _bert_embedding(self, x):\n",
    "        \n",
    "        token_type_ids = tf.cast((x == 0), tf.int32)\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, token_type_ids=token_type_ids)\n",
    "\n",
    "        return hidden_states, sentence_embedding\n",
    "    \n",
    "        \"\"\"\n",
    "        hidden states contains hidden state for each word\n",
    "        sentence_embedding is general embedding for whole sentences\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def _default_embedding(self, x):\n",
    "        \n",
    "        # embedding => (batch_size, sequence_length, embedding_size)\n",
    "        embedding = self.default_embedding(x)\n",
    "        return embedding\n",
    "\n",
    "    \n",
    "    def rnn_model(self, x):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            output, h_state, c_state = self.lstm(x)\n",
    "            hidden = tf.concat([h_state, c_state], axis=-1)\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            output, forw_h, forw_c, back_h, back_c = self.bilstm(x)\n",
    "            hidden = tf.concat([forw_h, forw_c, back_h, back_c], axis=-1)\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, hidden = self.gru(x)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "        \"\"\"\n",
    "        output => (batch_size, rnn_size)\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "    def call(self, decoder_input):\n",
    "        \"\"\" \n",
    "        decoder_input  : last predicted word => (batch_size, sequence len)\n",
    "        \"\"\"\n",
    "        \n",
    "        # x1 => (batch_size, input_sentence_len, embedding_dim + positional_dim)\n",
    "        x1 = self.embedding(decoder_input)\n",
    "        \n",
    "        # output => (batch_size, sequence_len, rnn_unit)\n",
    "        x2, rnn_state = self.rnn_model(x1) \n",
    "        output = self.dropout(x2)\n",
    "\n",
    "        return output, rnn_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, units=256, combine_layer=\"concat\", vocab_size=3000, batch_size=32):\n",
    "\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.combine_layer = combine_layer  # how to use context_vector [\"add\", \"concat\"]\n",
    "\n",
    "        # =====================================================\n",
    "\n",
    "        self._init_combine_layer()\n",
    "        \n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.dropout = Dropout(0.2)\n",
    "\n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\")  # same size as vocab\n",
    "\n",
    "    def _init_combine_layer(self):\n",
    "\n",
    "        if self.combine_layer == \"add\":\n",
    "            self.combine = tf.keras.layers.Add()\n",
    "\n",
    "        else:\n",
    "            self.combine = tf.keras.layers.Concatenate(axis=-1)\n",
    "\n",
    "    def _format_context_vector(self, context_vector):\n",
    "\n",
    "        context_vector = tf.expand_dims(context_vector, 1)\n",
    "        context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "    def call(self, decoder_input, context_vector):\n",
    "        \"\"\" \n",
    "        decoder_input  : \n",
    "        context_vector : image's vector      => (batch_size, img_context_size)\n",
    "        \"\"\"\n",
    "\n",
    "        context_vector = self._format_context_vector(context_vector)\n",
    "\n",
    "        # x1 (concat) => (batch_size, input_sentence_len, embedding_dim + image_context_size)\n",
    "        # x1 (add) => (batch_size, embedding_dim)\n",
    "        x1 = self.combine([context_vector, decoder_input])\n",
    "\n",
    "        # ============================================\n",
    "        # TODO: add another attention layer ?\n",
    "        # ============================================\n",
    "\n",
    "        # x2 => (batch_size, sequence_len, rnn_units)\n",
    "        x2 = self.fc1(x1)   # how important is every sequence\n",
    "        x2 = self.batchnorm(x2)\n",
    "        x2 = self.leakyrelu(x2)\n",
    "        x2 = self.dropout(x2)\n",
    "\n",
    "        # x3 => (batch_size, sequence_len * rnn_units) || was (batch_size * sequence_len=1, rnn_units)\n",
    "        x3 = tf.reshape(x2, (x2.shape[0], -1))\n",
    "\n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x3)\n",
    "\n",
    "        return word_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rnn_units': 256,\n",
       " 'rnn_type': 'BiLSTM',\n",
       " 'tokenizer': 'BERT',\n",
       " 'word_embedding': 'BERT',\n",
       " 'vocab_size': 3000,\n",
       " 'combine_strategy': 'merge',\n",
       " 'combine_layer': 'concat',\n",
       " 'image_context_size': 256,\n",
       " 'word_embedding_dim': 256,\n",
       " 'batch_size': 32,\n",
       " 'data_size': 5000,\n",
       " 'use_mapping': True,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_caption_length': 25,\n",
       " 'image_feature_extractor': 'xception',\n",
       " 'use_sequence': True,\n",
       " 'epoch': 20,\n",
       " 'use_mask': True,\n",
       " 'version': 'v5 : positional embedding'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 14:45:55.567105 139858796947264 configuration_utils.py:256] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/m13516112/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0402 14:45:55.568863 139858796947264 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0402 14:45:56.712431 139858796947264 modeling_tf_utils.py:333] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5 from cache at /home/m13516112/.cache/torch/transformers/d667df51ec24c20190f01fb4c20a21debc4c4fc12f7e2f5441ac0a99690e3ee9.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5\n",
      "I0402 14:45:57.998819 139858796947264 modeling_tf_utils.py:375] Layers from pretrained model not used in TFBertModel: ['nsp___cls', 'mlm___cls']\n"
     ]
    }
   ],
   "source": [
    "cnn_encoder = CNN_Encoder(\n",
    "    output_dim=PARAMS[\"image_context_size\"]\n",
    ")\n",
    "\n",
    "attention = BahdanauAttention(\n",
    "    units=PARAMS[\"rnn_units\"]\n",
    ")\n",
    "\n",
    "rnn_encoder = RNN_Encoder(\n",
    "    rnn_type=PARAMS[\"rnn_type\"], \n",
    "    rnn_units=PARAMS[\"rnn_units\"],\n",
    "    embedding_type=PARAMS[\"word_embedding\"], \n",
    "    embedding_dim=PARAMS[\"word_embedding_dim\"],  \n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    "    batch_size=PARAMS[\"batch_size\"]\n",
    ")\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    units=PARAMS[\"rnn_units\"],\n",
    "    combine_layer=PARAMS[\"combine_layer\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    "    batch_size=PARAMS[\"batch_size\"]\n",
    ")\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Requirements\n",
    "\n",
    "# combine_strategy = \"inject_init\" : IMAGE_CONTEXT_SIZE == UNITS\n",
    "# combine_strategy = \"inject_pre\"  : IMAGE_CONTEXT_SIZE == WORD_EMBEDDING_DIM\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default feed forward function\n",
    "\n",
    "@tf.function\n",
    "def feed_forward(img_tensor, caption):\n",
    "    \"\"\"\n",
    "    img_tensor => (batch_size, image_feature_size, 2048)\n",
    "    caption => (batch_size, max_caption_length)\n",
    "    \"\"\"\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    # note : used to be decoder hidden state, \n",
    "    # but changed into current word embedding for paralel training\n",
    "    \n",
    "    # hidden => (batch_size, embedding_size)\n",
    "    text_features, hidden = rnn_encoder(caption)\n",
    "\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    img_features = cnn_encoder(img_tensor)\n",
    "\n",
    "    # context_vector => (batch_size, image_context_size)\n",
    "    context_vector, _ = attention(img_features, hidden)\n",
    "\n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions = decoder(text_features, context_vector)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, caption, target):\n",
    "    \"\"\"\n",
    "    target => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training model\n",
    "    with tf.GradientTape() as gradient_tape:\n",
    "        \n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions = feed_forward(img_tensor, caption)\n",
    "        \n",
    "        # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "        loss = loss_function(target, predictions)\n",
    "\n",
    "        \n",
    "    # Apply gradient\n",
    "    trainable_variables = cnn_encoder.trainable_variables + \\\n",
    "                          rnn_encoder.trainable_variables + \\\n",
    "                          decoder.trainable_variables + \\\n",
    "                          attention.trainable_variables\n",
    "    \n",
    "    gradients = gradient_tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def eval_step(img_tensor, caption, target):\n",
    "    \n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions = feed_forward(img_tensor, caption)\n",
    "    \n",
    "    # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "    loss = loss_function(target, predictions)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def choose_predicted_id(predictions, strategy=\"max\", sampling_k=10):\n",
    "    \"\"\"\n",
    "    predictions : encoder word prediction => (batch_size, vocab_size)\n",
    "    strategy    : how to choose word [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sampling method (categorical dist)\n",
    "    if strategy == \"sample\":\n",
    "        \n",
    "        # sampled_proba & sampled_ids => (batch_size, sampling_k)\n",
    "        sampled_proba, sampled_ids = tf.math.top_k(predictions, sampling_k)\n",
    "        \n",
    "        # chosen_sampled_col => (batch_size, )\n",
    "        chosen_sampled_col = tf.squeeze(tf.random.categorical(sampled_proba, 1))\n",
    "        \n",
    "        # create row idx to zip with chosen_sampled_col\n",
    "        row_idx = tf.range(predictions.shape[0], dtype=chosen_sampled_col.dtype)\n",
    "        row_col_idx = tf.stack([row_idx, chosen_sampled_col], axis=1)\n",
    "        \n",
    "        # predicted_ids => (batch_size, )\n",
    "        predicted_ids = tf.gather_nd(sampled_ids, row_col_idx)\n",
    "\n",
    "    # Max index method\n",
    "    else:\n",
    "        predicted_ids = tf.argmax(predictions, 1)\n",
    "    \n",
    "    # predicted_ids => (batch_size, )\n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(images_paths):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [load_image(x)[0] for x in images_paths]\n",
    "    \n",
    "    # x => (batch_size, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, 10, 10, 2048)\n",
    "    x = extractor(x)\n",
    "    \n",
    "    # x  => (batch_size, img_feature_size, 2048)\n",
    "    x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = cnn_encoder(x)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def get_supporting_features(images_paths, strategy=\"mean\"):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, img_count, 1)\n",
    "    strategy : strategy to aggregate multiple supporting image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [[load_image(x)[0] for x in images_set] for images_set in images_paths]\n",
    "    \n",
    "    # x => (batch_size, img_count, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, img_count, 10, 10, 2048)\n",
    "    x = [extractor(image_set) for image_set in x]\n",
    "    \n",
    "    # features => (batch_size, img_count, img_feature_size, image_context_size)\n",
    "    features = cnn_encoder(x)\n",
    "    \n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    if strategy == \"logsumexp\":\n",
    "        features = tf.reduce_logsumexp(features, 1)\n",
    "    elif strategy == \"max\":\n",
    "        features = tf.reduce_max(features, 1)\n",
    "    elif strategy == \"min\":\n",
    "        features = tf.reduce_min(features, 1)\n",
    "    else:\n",
    "        features = tf.reduce_mean(features, 1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_indices(support_text):\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(0, len(support_text)):\n",
    "        context_token = target_tokenizer.tokenize(support_text[i])\n",
    "        context_token_id = target_tokenizer.convert_tokens_to_ids(context_token)\n",
    "        context_token_id = set(context_token_id)\n",
    "        context_token_id.discard(0)\n",
    "        for x in sorted(context_token_id):\n",
    "            indices.append([i, x])\n",
    "\n",
    "    # return => (word_count, 2)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_supporting_text_vector(support_text, vocab_size):\n",
    "    \"\"\"\n",
    "    support_text : list of text describing main image context => (batch_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(support_text)\n",
    "    \n",
    "    # indices => ( sum(batch_size * ?word_count), 2)\n",
    "    indices = get_one_hot_indices(support_text)\n",
    "    values = tf.ones(len(indices))\n",
    "    sparse_one_hot = tf.sparse.SparseTensor(indices, values, dense_shape=[batch_size, vocab_size])\n",
    "    \n",
    "    # sparse_one_hot => (batch_size, vocab_size)\n",
    "    return sparse_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluate(images_paths,\n",
    "                    support_text=None,\n",
    "                    support_imgs=None, \n",
    "                    support_aggregate_strategy=\"mean\",\n",
    "                    pplm_iteration=3,\n",
    "                    pplm_weight=0.03,\n",
    "                    pplm_gm_weight=0.8,\n",
    "                    choose_word_strategy=\"sample\",\n",
    "                   ):\n",
    "    \n",
    "    \"\"\"\n",
    "    images_paths : list of image_path                           => (batch_size, 1)\n",
    "    support_text : list of text describing main image context   => (batch_size)\n",
    "    support_imgs : list of list of image_path                   => (batch_size, image_count, 1)\n",
    "    support_aggregate_strategy : how to aggregate support image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    pplm_iteration : number of pplm step done for every decoding step\n",
    "    pplm_weight    : weight of pplm loss\n",
    "    pplm_gm_weight : geometric mean fusion weight (0 means use only original prediction, 1 means use only pplm prediction)\n",
    "    choose_word_strategy : how to choose word from prediction distribution [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(images_paths)\n",
    "    \n",
    "    # initialize captions placeholder\n",
    "    start_token = caption_tokenizer.convert_tokens_to_ids(['[CLS]']) # use bert id's not custom id\n",
    "    result_captions = tf.tile(tf.expand_dims(start_token, 1), [batch_size, 1])\n",
    "    attention_plot = tf.reshape([], shape=(batch_size, 0, IMAGE_FEATURE_SHAPE[0]))\n",
    "    \n",
    "    # Extract features from main images\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = get_image_features(images_paths)\n",
    "    \n",
    "    \n",
    "    if support_text is not None:\n",
    "        # support_text_vector => (batch_size, vocab_size)\n",
    "        support_text_vector = get_supporting_text_vector(support_text, decoder.vocab_size) \n",
    "    else:\n",
    "        # set all pplm related variable to 0\n",
    "        pplm_iteration = 0\n",
    "        pplm_weight = 0\n",
    "        pplm_gm_weight = 0\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(MAX_CAPTION_LENGTH)):\n",
    "        \n",
    "        # decoder_input => (batch_size, ~MAX_CAPTION_LENGTH)\n",
    "        caption = pad_sequences(result_captions, maxlen=MAX_CAPTION_LENGTH, padding=\"post\")\n",
    "        \n",
    "        # hidden => (batch_size, embedding_size)\n",
    "        text_features, hidden = rnn_encoder(caption)\n",
    "\n",
    "        # context_vector => (batch_size, image_context_size)\n",
    "        # attention_weights => (batch_size, img_feature_size, 1)\n",
    "        context_vector, attention_weights = attention(features, hidden)\n",
    "\n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions = decoder(text_features, context_vector)  \n",
    "        \n",
    "        \n",
    "        # ======================== PPLM section ========================\n",
    "        ori_prediction = predictions\n",
    "        \n",
    "        curr_pertubation = tf.Variable(tf.zeros((batch_size, rnn_encoder.embedding_dim)), name=\"curr_pertubation\", trainable=True)\n",
    "        \n",
    "        \n",
    "        for j in range(pplm_iteration):\n",
    "            \n",
    "            with tf.GradientTape() as pplm_tape: \n",
    "                \n",
    "                hidden += curr_pertubation\n",
    "                \n",
    "                context_vector, attention_weights = attention(features, hidden)\n",
    "                predictions = decoder(text_features, context_vector)  \n",
    "                pplm_loss = pplm_loss_function(support_text_vector, predictions, pplm_weight=pplm_weight)\n",
    "                \n",
    "            \"\"\"\n",
    "            most impactfull layer to train = last dense layer\n",
    "            \"\"\"\n",
    "            \n",
    "            trainable_variables = [curr_pertubation]\n",
    "            gradients = pplm_tape.gradient(pplm_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "            \n",
    "            predictions = decoder(text_features, context_vector)  \n",
    "        \n",
    "        # fuse final pplm_prediction and original prediction\n",
    "        fused_predictions = (predictions * pplm_gm_weight) + (ori_prediction * (1 - pplm_gm_weight)) \n",
    "        \n",
    "        # predicted_ids => (batch_size,)\n",
    "        predicted_ids = choose_predicted_id(fused_predictions, strategy=choose_word_strategy)\n",
    "        \n",
    "        # convert custom id mapping to bert's id\n",
    "        # predicted_bert_ids => (batch_size, 1)\n",
    "        predicted_bert_ids = target_tokenizer._convert_custom_id_to_bert_id(predicted_ids.numpy())\n",
    "        predicted_bert_ids = tf.expand_dims(predicted_bert_ids, 1)\n",
    "        \n",
    "        # store result\n",
    "        result_captions = tf.concat([result_captions, predicted_bert_ids], axis=1)\n",
    "        \n",
    "        # attention_weights => (batch_size, 1, img_feature_size)\n",
    "        attention_weights = tf.reshape(attention_weights, shape=(batch_size, 1, -1))\n",
    "        \n",
    "        # assign attention weights to respective generated word\n",
    "        # attention_plot => (batch_size, ~max_caption_len, feature_size)\n",
    "        attention_plot = tf.concat([attention_plot, attention_weights], axis=1)\n",
    "        \n",
    "    # remove start token & revert to tokens\n",
    "    result_captions = [caption_tokenizer.convert_ids_to_tokens(x[1:]) for x in result_captions]\n",
    "    \n",
    "    return result_captions, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy, MeanSquaredError\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def pplm_loss_function(real, pred, pplm_weight=0.03):\n",
    "    \"\"\"\n",
    "    real  => (batch_size, vocab_size)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "\n",
    "#     mm = tf.sparse.sparse_dense_matmul(real, tf.transpose(pred))\n",
    "#     mm = tf.reduce_sum(tf.abs(mm), 1)\n",
    "#     loss = tf.reduce_sum(mm, 0)\n",
    "    \n",
    "    real = tf.sparse.to_dense(real, default_value=0)\n",
    "    pplm_loss = CategoricalCrossentropy(from_logits=True)\n",
    "    loss = pplm_loss(real, pred, pplm_weight)\n",
    "    \n",
    "#     print(loss)\n",
    "    return loss\n",
    "\n",
    "    \"\"\"\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1395it [02:17, 10.16it/s]\n",
      "1395it [01:15, 18.48it/s]\n",
      "1395it [01:14, 18.69it/s]\n",
      "1395it [01:15, 18.60it/s]\n",
      "1395it [01:38, 14.14it/s]\n",
      "1395it [01:58, 11.78it/s]\n",
      "1395it [02:00, 11.58it/s]\n",
      "1395it [02:01, 11.45it/s]\n",
      "1395it [02:03, 11.32it/s]\n",
      "1395it [02:03, 11.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f30168583c8>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    for (img_tensor, captions, target, _) in tqdm(train_dataset):\n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss.append(batch_loss.numpy())\n",
    "        \n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3016768048>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f30166cb748>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.553197629570961"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-500:]) / len(loss[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7105905788809062"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-5000:]) / len(loss[-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:00<00:00, 671403.37it/s]\n"
     ]
    }
   ],
   "source": [
    "true_captions = {}\n",
    "for annot in tqdm(annotations[\"annotations\"]):\n",
    "    image_id = annot[\"image_id\"]\n",
    "\n",
    "    if image_id not in true_captions:\n",
    "        true_captions[image_id] = []\n",
    "    \n",
    "    true_captions[image_id].append(annot['caption'])\n",
    "    \n",
    "    \n",
    "def get_true_captions(image_paths):\n",
    "    return [true_captions[int(x[-10:-4])] for x in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:06<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "on a station and towels with sprays in and sink and toilet on it . is with\n",
      "\n",
      "true caps : \n",
      "A bathroom with a toilet and sink, \n",
      "View of a white toilet next to a vanity with a red sink.\n",
      "A white remote and sink in a room.\n",
      "A white toilet sitting next to a bathroom sink.\n",
      "A bathroom with a toilet and a red sink.\n",
      "=====================================\n",
      "predicted : \n",
      "people standing with kitchen with many cut in toilet in a and shower in next in white\n",
      "\n",
      "true caps : \n",
      "A small bathroom with sink, mirror, toilet and tub\n",
      "A white sink sitting under a bathroom mirror.\n",
      "A bathroom mirror and sink in a bathroom \n",
      "this bathroom is all beige and has a white sink\n",
      "A mirror reflection above the sink shows the toilet and bathtub.\n",
      "=====================================\n",
      "predicted : \n",
      "two bright luggage bills and a empty are vegetables are moving behind\n",
      "\n",
      "true caps : \n",
      "a yellow and grey passenger train parked next to a platform\n",
      "A train traveling down a set of tracks near a train station.\n",
      "Train arriving or departing at an empty station.\n",
      "A train is coming down the tracks near a platform.\n",
      "A subway train driving through a subway station. \n",
      "=====================================\n",
      "predicted : \n",
      "a woman holds a couple , at the table , man . front area in .\n",
      "\n",
      "true caps : \n",
      "A group of people sitting at the table smiling. \n",
      "A group of elderly people having dinner at a restaurant. \n",
      "a number of older people seated around a table\n",
      "A group of people are seated at a restaurant dining table.\n",
      "A group of people sitting around a restaurant table.\n",
      "=====================================\n",
      "predicted : \n",
      "red and picture people with red a with children in white in it riding the field with a man and black . game the .\n",
      "\n",
      "true caps : \n",
      "Two men standing close together examining some red wine.\n",
      "One man holding a wine glass with some wine while his friend looks on\n",
      "Two men looking up while toasting with wine glasses. \n",
      "A couple of men standing next to each other holding wine glasses.\n",
      "Two people are posing for the camera with their win glasses. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[50:55]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images)\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:06<00:00,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "a man standing on the back of the back of four people .\n",
      "\n",
      "true caps : \n",
      "some people are standing near some public toilets\n",
      "A group of people are standing out in front of portable toilets.\n",
      "people are standing around some porta potties and mess\n",
      "a couple of people stand in front of some toilets \n",
      "groups of people standing around the toilet area\n",
      "=====================================\n",
      "predicted : \n",
      "a hand holding a black and black cell phone\n",
      "\n",
      "true caps : \n",
      "A person is holding onto a cellphone somewhere.\n",
      "there is someone holding a small phone with a small screen\n",
      "A picture of a cell phone showing the time as 8:56 PM.\n",
      "a hand is holding a black and silver cellphone\n",
      "This is a verizon LG Envy 2 cell phone.\n",
      "=====================================\n",
      "predicted : \n",
      "people in the laptop in front of a dining room\n",
      "\n",
      "true caps : \n",
      "People working on laptop computers in a student dining room\n",
      "People sitting in a room at tables with laptops \n",
      "A group of people that are sitting in front of laptops.\n",
      "Many people have signed up for the computer seminar.\n",
      "A group of people at tables with laptops having conversation.\n",
      "=====================================\n",
      "predicted : \n",
      "a gray and black photo of a photo in a photo\n",
      "\n",
      "true caps : \n",
      "two birds near one another in a field \n",
      "Two black colored birds sitting in a field together. \n",
      "Two birds that are sitting on the ground.\n",
      "2 birds standing on the ground in a field \n",
      "Two birds that are standing next to each other on the grass. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[5:9]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_strategy=\"max\")\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:06<00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "white sink has covered ic . sitting on\n",
      "\n",
      "true caps : \n",
      "A bathroom with a toilet and sink, \n",
      "View of a white toilet next to a vanity with a red sink.\n",
      "A white remote and sink in a room.\n",
      "A white toilet sitting next to a bathroom sink.\n",
      "A bathroom with a toilet and a red sink.\n",
      "=====================================\n",
      "predicted : \n",
      "small sink standing at toilet , by white under area area with area area\n",
      "\n",
      "true caps : \n",
      "A small bathroom with sink, mirror, toilet and tub\n",
      "A white sink sitting under a bathroom mirror.\n",
      "A bathroom mirror and sink in a bathroom \n",
      "this bathroom is all beige and has a white sink\n",
      "A mirror reflection above the sink shows the toilet and bathtub.\n",
      "=====================================\n",
      "predicted : \n",
      "several trains at a open field are parked behind a platform behind behind birds . with in an room a player . at hydra while\n",
      "\n",
      "true caps : \n",
      "a yellow and grey passenger train parked next to a platform\n",
      "A train traveling down a set of tracks near a train station.\n",
      "Train arriving or departing at an empty station.\n",
      "A train is coming down the tracks near a platform.\n",
      "A subway train driving through a subway station. \n",
      "=====================================\n",
      "predicted : \n",
      "a couple are and people a playing something\n",
      "\n",
      "true caps : \n",
      "A group of people sitting at the table smiling. \n",
      "A group of elderly people having dinner at a restaurant. \n",
      "a number of older people seated around a table\n",
      "A group of people are seated at a restaurant dining table.\n",
      "A group of people sitting around a restaurant table.\n",
      "=====================================\n",
      "predicted : \n",
      "small boy wearing stand another two up another on a green\n",
      "\n",
      "true caps : \n",
      "Two men standing close together examining some red wine.\n",
      "One man holding a wine glass with some wine while his friend looks on\n",
      "Two men looking up while toasting with wine glasses. \n",
      "A couple of men standing next to each other holding wine glasses.\n",
      "Two people are posing for the camera with their win glasses. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[50:55]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images)\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "stop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-5981adc29e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: stop"
     ]
    }
   ],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train/{}\".format(str(datetime.now())[:-10])\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           attention=attention,\n",
    "                           optimizer=optimizer\n",
    "                          )\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------\")\n",
    "print(checkpoint_path)\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(checkpoint_path)\n",
    "with open(checkpoint_path + \"/config.txt\", \"w\") as f:\n",
    "    f.write(str(PARAMS))\n",
    "    \n",
    "log_file = open(checkpoint_path + \"/log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, PARAMS[\"epoch\"]):\n",
    "    \n",
    "    start = time.time()\n",
    "    loss = 0\n",
    "    batch = 1\n",
    "\n",
    "    for img_tensor, captions, target in tqdm(train_dataset):\n",
    "        \n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "\n",
    "            with open(checkpoint_path + \"/log.txt\", \"a\") as f:\n",
    "                log_message = ' {} Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                    checkpoint_path, epoch + 1, batch, batch_loss.numpy())\n",
    "                f.write(str(log_message + \"\\n\"))\n",
    "                print(log_message)   \n",
    "\n",
    "        batch += 1\n",
    "        \n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(batch_loss.numpy())\n",
    "\n",
    "#     ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = []\n",
    "\n",
    "# for (img_tensor, captions, target) in tqdm(train_dataset):\n",
    "#     batch_loss = train_step(img_tensor, captions, target)\n",
    "#     loss.append(batch_loss.numpy())\n",
    "    \n",
    "# plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PPLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "# text = [\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "# ]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images, support_text=text, pplm_iteration=5, pplm_weight=1)\n",
    "# result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
