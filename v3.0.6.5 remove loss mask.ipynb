{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git add \"Keras.ipynb\"\n",
    "!git commit -m \"test pred using pplm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"rnn_units\": 256,\n",
    "    \"rnn_type\": \"BiLSTM\",\n",
    "    \"tokenizer\": \"BERT\",\n",
    "    \"word_embedding\": \"BERT\",\n",
    "    \"vocab_size\": 3000,\n",
    "    \"combine_strategy\": \"merge\",\n",
    "    \"combine_layer\": \"concat\",\n",
    "    \"image_context_size\": 256,\n",
    "    \"word_embedding_dim\": 256,\n",
    "    \"batch_size\": 32,\n",
    "    \"data_size\": 5000,\n",
    "    \"use_mapping\": True,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_caption_length\": 25, # use <int> or None\n",
    "    \"image_feature_extractor\": \"xception\",\n",
    "    \"use_sequence\": True,\n",
    "    \"epoch\": 20,\n",
    "    \"use_mask\": True,\n",
    "    \n",
    "    \"beam_search_k\": 5,\n",
    "    \"version\": \"v5 : positional embedding\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "\n",
    "# for TFBertModel\n",
    "PROXIES = {\n",
    "  \"http\": \"http://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "  \"https\": \"https://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"[CLS] \" + annot['caption'] + \" [SEP]\"\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "\n",
    "stopper = -1 if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]\n",
    "train_captions = all_captions[:stopper]\n",
    "train_img_paths = all_img_paths[:stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 5000\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = len(train_captions) if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature_extractor(model_type=\"xception\"):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.compat.v1.keras.applications.xception\n",
    "        cnn_model = tf.compat.v1.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"CNN encoder model not supported yet\")\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "def get_image_feature_shape(model_type):\n",
    "    \n",
    "    if model_type == \"xception\":\n",
    "        return (100, 2048)\n",
    "    elif model_type == \"inception_v3\":\n",
    "        return (64, 2048)\n",
    "    else:\n",
    "        raise Exception (\"model unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = extractor_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FEATURE_SHAPE = get_image_feature_shape(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extractor, extractor_preprocessor = get_image_feature_extractor(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image feature\n",
    "\n",
    "this step mostly skipped since the results are cached already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 157.25\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = PARAMS[\"batch_size\"]\n",
    "\n",
    "estimated_batch_count = DATA_SIZE / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique images\n",
    "# unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# # Prepare dataset\n",
    "# image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "# image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "# image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessed image (batch)\n",
    "\n",
    "# for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "#     # get context vector of batch images\n",
    "#     batch_features = extractor(batch_imgs)\n",
    "    \n",
    "#     # flatten 2D cnn result into 1D for RNN decoder input\n",
    "#     # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "#     # image_feature = 100 (Xception)\n",
    "#     # image_feature = 64 (Inception V3)\n",
    "#     batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "#     # Cache preprocessed image\n",
    "#     for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "#         image_path = image_path.numpy().decode(\"utf-8\")\n",
    "#         np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Caption dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0401 11:18:27.723833 140502276835136 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0401 11:18:27.725107 140502276835136 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = PARAMS[\"tokenizer\"]\n",
    "VOCAB_SIZE = PARAMS[\"vocab_size\"]  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "    \n",
    "    def use_custom_mapping(self, use_mapping=True, vocab_size=3000):\n",
    "        \n",
    "        self.use_mapping = use_mapping\n",
    "        self.cust_vocab_size = vocab_size\n",
    "        self.mapping_initialized = False\n",
    "\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into custom indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        bert_ids = [self.convert_tokens_to_ids(self.tokenize(x)) for x in tqdm(texts)]\n",
    "        \n",
    "        if not self.use_mapping:\n",
    "            return bert_ids\n",
    "        \n",
    "        if not self.mapping_initialized:\n",
    "            self._initialize_custom_mapping(bert_ids)\n",
    "            return [self._convert_bert_id_to_custom_id(x) for x in tqdm(bert_ids)]\n",
    "        \n",
    "        return bert_ids\n",
    "    \n",
    "        \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \n",
    "        bert_ids = super().convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            return self._convert_bert_id_to_custom_id(bert_ids)\n",
    "        else:\n",
    "            return bert_ids\n",
    "        \n",
    "        \n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            bert_ids = self._convert_custom_id_to_bert_id(token_ids)\n",
    "        else:\n",
    "            bert_ids = token_ids\n",
    "            \n",
    "        bert_tokens = super().convert_ids_to_tokens(bert_ids)\n",
    "        return bert_tokens\n",
    "    \n",
    "    \n",
    "    def _initialize_custom_mapping(self, corpus_bert_ids):\n",
    "        \n",
    "        print(\"    > constructing custom mapping < \\n\")\n",
    "        self._build_occurence_table(corpus_bert_ids)\n",
    "        self._build_custom_mapping_table()\n",
    "        self.mapping_initialized = True\n",
    "        \n",
    "        \n",
    "    def _build_occurence_table(self, tokenized_captions):\n",
    "        \"\"\"\n",
    "        build dict of token frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        self.occurence_table = {}\n",
    "        for caption in tqdm(tokenized_captions):\n",
    "            for token in caption:\n",
    "                if token not in self.occurence_table:\n",
    "                    self.occurence_table[token] = 0\n",
    "                self.occurence_table[token] += 1\n",
    "                \n",
    "    \n",
    "    def _build_custom_mapping_table(self):\n",
    "        \n",
    "        _special_token = ['[UNK]', '[PAD]']\n",
    "        _actual_vocab_size = self.cust_vocab_size - len(_special_token)\n",
    "        \n",
    "        sorted_occurence = {k: v for k, v in sorted(\n",
    "            self.occurence_table.items(), reverse=True, key=lambda item: item[1]\n",
    "        )}\n",
    "        \n",
    "        used_tokens = sorted(list(sorted_occurence)[:_actual_vocab_size])\n",
    "        mapping_size = min(len(used_tokens), _actual_vocab_size)\n",
    "        \n",
    "        _bert_pad = 0\n",
    "        _bert_oov = 100\n",
    "        self._custom_pad = 0\n",
    "        self._custom_oov = mapping_size + 1\n",
    "        \n",
    "        self.bert_id_to_custom_id = {\n",
    "            _bert_pad: self._custom_pad, \n",
    "            _bert_oov: self._custom_oov\n",
    "        }\n",
    "        self.custom_id_to_bert_id = {\n",
    "            self._custom_pad: _bert_pad, \n",
    "            self._custom_oov: _bert_oov\n",
    "        }\n",
    "        \n",
    "        for i in range(0, mapping_size):\n",
    "            bert_token = used_tokens[i]\n",
    "            self.bert_id_to_custom_id[bert_token] = i + 1    \n",
    "            self.custom_id_to_bert_id[i + 1] = bert_token\n",
    "            \n",
    "        print(\"Vocab contains {0} / {1} unique tokens ({2:.2f} %)\".format(\n",
    "            len(used_tokens) + 2,\\\n",
    "            len(sorted_occurence),\\\n",
    "            (len(used_tokens) / len(sorted_occurence) * 100)\n",
    "        ))\n",
    "        \n",
    "        sorted_occurence_count = list(sorted_occurence.values())\n",
    "        used_tokens_count = sum(sorted_occurence_count[:_actual_vocab_size])\n",
    "        total_tokens_count = sum(sorted_occurence_count)\n",
    "        \n",
    "        print(\"Using {0} / {1} tokens available ({2:.2f} %)\".format(\n",
    "            used_tokens_count,\\\n",
    "            total_tokens_count,\\\n",
    "            (used_tokens_count / total_tokens_count * 100)\n",
    "        ))        \n",
    "        \n",
    "    def _convert_bert_id_to_custom_id(self, token_ids):\n",
    "        return [self.bert_id_to_custom_id[x] if x in self.bert_id_to_custom_id else self._custom_oov for x in token_ids]\n",
    "    \n",
    "    def _convert_custom_id_to_bert_id(self, token_ids):\n",
    "        return [self.custom_id_to_bert_id[x] for x in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_type, use_mapping, vocab_size):\n",
    "    \n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    if tokenizer_type == \"BERT\" :\n",
    "        tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "        tokenizer.use_custom_mapping(use_mapping, vocab_size)\n",
    "\n",
    "    # use default keras tokenizer\n",
    "    else : \n",
    "        tokenizer = TokenizerWrapper(num_words=vocab_size, oov_token=\"[UNK]\")\n",
    "        tokenizer.fit_on_texts(train_captions)    \n",
    "        tokenizer.word_index['[PAD]'] = 0\n",
    "        tokenizer.index_word[0] = '[PAD]'\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0401 11:18:29.806101 140502276835136 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0401 11:18:31.032635 140502276835136 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "caption_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=False,\n",
    "    vocab_size=0\n",
    ")\n",
    "\n",
    "target_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=PARAMS[\"use_mapping\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 2724.44it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2796.29it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 265036.21it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 285171.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > constructing custom mapping < \n",
      "\n",
      "Vocab contains 3000 / 3584 unique tokens (83.65 %)\n",
      "Using 68179 / 68765 tokens available (99.15 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "caption_tokens = caption_tokenizer.texts_to_sequences(train_captions)\n",
    "target_tokens = target_tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parallel dataset\n",
    "\n",
    "prepare captions and target to support parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parallel_dataset(img_paths, caption_tokens, target_tokens):\n",
    "    \n",
    "    dataset_img_paths = []\n",
    "    dataset_captions = []\n",
    "    dataset_target = []\n",
    "    dataset_target_position = []\n",
    "\n",
    "    for i in tqdm(range(0, len(train_img_paths))):\n",
    "        img = img_paths[i]\n",
    "        cap = caption_tokens[i]\n",
    "        tar = target_tokens[i]\n",
    "\n",
    "        for j in range(1, len(cap)):\n",
    "            dataset_img_paths.append(img)\n",
    "            dataset_captions.append(cap[:j])\n",
    "            dataset_target.append(tar[j])\n",
    "            dataset_target_position.append(j)\n",
    "\n",
    "    dataset_captions = pad_sequences(dataset_captions, maxlen=PARAMS[\"max_caption_length\"], padding='post')\n",
    "    \n",
    "    return dataset_img_paths, dataset_captions, dataset_target, dataset_target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 17296.78it/s]\n"
     ]
    }
   ],
   "source": [
    "parallel_img_paths, parallel_captions, parallel_target, parallel_target_position = preprocess_parallel_dataset(train_img_paths, caption_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = parallel_captions.shape[0]\n",
    "MAX_CAPTION_LENGTH = parallel_captions.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_target_position = tf.one_hot(parallel_target_position, depth=MAX_CAPTION_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "\n",
    "def load_dataset(img_name, caption, target, target_pos):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, caption, target, target_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((parallel_img_paths, parallel_captions, parallel_target, parallel_target_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "\n",
    "dataset = dataset.map(lambda item1, item2, item3, item4: tf.numpy_function(\n",
    "          load_dataset, [item1, item2, item3, item4], [tf.float32, tf.int32, tf.int32, tf.float32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(DATA_SIZE / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 16, image_feature = 100, 2048)\n",
    "# caption => (batch_size = 16, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1395 batches, (total : 44640)\n",
      "eval : 298 batches, (total : 9536)\n",
      "test : 300 batches, (total : 9600 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captioning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=PARAMS[\"learning_rate\"])\n",
    "# loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    real  => (batch_size,)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate loss\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "#     # create mask to filter out padding token \n",
    "#     mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "#     mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "#     # Ignore loss_ if real token is padding\n",
    "#     loss_ *= mask\n",
    "    \n",
    "    # Get mean of curren batch's loss (somewhat batch norm)\n",
    "    result_loss = tf.reduce_mean(loss_)\n",
    "    \n",
    "    return result_loss\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    loss_  => (batch_size, 1)\n",
    "    mask   => (batch_size, 1)  : indicate is padding or not\n",
    "\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim)\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # x => (batch_size, 100, 2048)\n",
    "        x = self.fc(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, image_feature_size, image_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, img_feature_size, image_context_size)\n",
    "        hidden                        => (batch_size, embedding_size)\n",
    "        \n",
    "        note : \n",
    "        img_feature_size ==  64 for Inception V3,\n",
    "        img_feature_size == 100 for Xception,\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        _w1 = self.W1(features)\n",
    "        _w2 = self.W2(hidden_with_time_axis)\n",
    "        score = tf.nn.tanh(_w1 + _w2)\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_with_time_axis      => (batch_size, 1, embedding_size)\n",
    "        score                      => (batch_size, img_feature_size, units)\n",
    "        attention_weights          => (batch_size, img_feature_size, 1)\n",
    "        context_vector (after sum) => (batch_size, img_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=256, \n",
    "                 embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 combine_strategy=\"merge\", combine_layer=\"concat\",\n",
    "                 vocab_size=3000, batch_size=32):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # when to use context_vector [\"inject_init\", \"inject_pre\", \"inject_par\", \"merge\"]\n",
    "        self.combine_strategy = combine_strategy\n",
    "        \n",
    "        # how to use context_vector [\"add\", \"concat\"]\n",
    "        self.combine_layer = combine_layer\n",
    "        \n",
    "        # =====================================================\n",
    "        \n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "        \n",
    "#         self.mask = Masking(mask_value=0, dtype=\"int32\")\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "        self.positional_embedding = Embedding(\n",
    "            input_dim=MAX_CAPTION_LENGTH,\n",
    "            output_dim=16,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "        \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\") # same size as vocab\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "           \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\") # same size as vocab\n",
    "        \n",
    "        \n",
    "    def _init_embedding(self):\n",
    "        \n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.bert_embedding = TFBertModel.from_pretrained('bert-base-uncased', proxies=PROXIES)\n",
    "            self.bert_embedding.trainable = False\n",
    "            self.embedding_dim = self.bert_embedding.config.hidden_size\n",
    "            \n",
    "        else:\n",
    "            self.default_embedding = Embedding(\n",
    "                input_dim=self.vocab_size, \n",
    "                output_dim=self.embedding_dim, \n",
    "                mask_zero=True,\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def _init_rnn(self):\n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector'\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            self.bilstm = Bidirectional(LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        \n",
    "    def embedding(self, tokens, as_sentence=False):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embedding_type == \"BERT\": \n",
    "            embedding, sentence_embedding = self._bert_embedding(tokens)\n",
    "        \n",
    "        else:\n",
    "            embedding = self._default_embedding(tokens)\n",
    "            sentence_embedding = tf.reduce_mean(embedding, 1)\n",
    "            \n",
    "\n",
    "        if as_sentence:\n",
    "            # embedding => (batch_size, embedding_dim)\n",
    "            return sentence_embedding\n",
    "        \n",
    "        else:\n",
    "            # embedding => (batch_size, tokens_length, embedding_dim)\n",
    "            return embedding\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        embedding (tokens)   => (batch_size, tokens_length, embedding_dim)\n",
    "        embedding (sentence) => (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def _bert_embedding(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        # mask out attention from padding\n",
    "        # mask => (batch_size, sequence_length)\n",
    "        attention_mask = x == 0\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, attention_mask=attention_mask)\n",
    "        \"\"\"\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        token_type_ids = tf.cast((x == 0), tf.int32)\n",
    "        \n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, token_type_ids=token_type_ids)\n",
    "        return hidden_states, sentence_embedding\n",
    "    \n",
    "        \"\"\"\n",
    "        hidden states contains hidden state for each word\n",
    "        sentence_embedding is general embedding for whole sentences\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def _default_embedding(self, x):\n",
    "        \n",
    "        # embedding => (batch_size, sequence_length, embedding_size)\n",
    "        embedding = self.default_embedding(x)\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "    def apply_strategy(self, x, context_vector, curr_iter=-1):\n",
    "        \"\"\"\n",
    "        context_vector : image's vector\n",
    "        x              : rnn input (word embedding)\n",
    "        strategy       : \n",
    "        curr_iter      : current iteration number\n",
    "        \n",
    "        No longer support inject_init & inject_pre, since training become fully parallel\n",
    "        \"\"\"\n",
    "        \n",
    "#         if self.combine_strategy == \"inject_init\":\n",
    "#             initial_state = tf.squeeze(context_vector) if curr_iter == 1 else None\n",
    "#             output, state = self.rnn_model(x, initial_state=initial_state)  \n",
    "            \n",
    "#         elif self.combine_strategy == \"inject_pre\":\n",
    "#             x = context_vector if curr_iter == 1 else x\n",
    "#             output, state = self.rnn_model(x)  \n",
    "            \n",
    "        if self.combine_strategy == \"inject_par\":\n",
    "            \n",
    "            context_vector = tf.expand_dims(context_vector, 1)\n",
    "            context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "                        \n",
    "            x = self.custom_combine_layer(context_vector, x)\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output, state = self.rnn_model(x)              \n",
    "            \n",
    "        else: # merge (as default)\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output, state = self.rnn_model(x) \n",
    "            \n",
    "            context_vector = tf.expand_dims(context_vector, 1)\n",
    "            context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output = self.custom_combine_layer(context_vector, output)\n",
    "            \n",
    "        return output, state\n",
    "    \n",
    "    \n",
    "    def rnn_model(self, x, initial_state=None):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            \n",
    "            # adjust initial state, LSTM has 2 hidden states (h and c)\n",
    "            if initial_state is not None:\n",
    "                init_h = initial_state\n",
    "                init_c = tf.zeros(initial_state.shape)\n",
    "                initial_state = [init_h, init_c]\n",
    "            \n",
    "            output, h_state, c_state = self.lstm(x, initial_state=initial_state)\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            output, _, h_state, _, _ = self.bilstm(x)\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, h_state = self.gru(x, initial_state=initial_state)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, h_state\n",
    "    \n",
    "        \"\"\"\n",
    "        output => (batch_size, rnn_size)\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def custom_combine_layer(self, x, y):\n",
    "        if self.combine_layer == \"add\":\n",
    "            return self._add_layer(x, y)\n",
    "        else:\n",
    "            return self._concat_layer(x, y)\n",
    "\n",
    "        \n",
    "    def _add_layer(self, x, y):\n",
    "        \n",
    "        if x.shape[1] != y.shape[1] :\n",
    "            exception = \"Cannot combine using 'add' strategy, both tensor has different shape {} & {}\"\n",
    "            raise Exception(exception.format(x.shape, y.shape))\n",
    "            \n",
    "        return tf.keras.layers.add([x, y])\n",
    "            \n",
    "        \n",
    "    def _concat_layer(self, x, y):\n",
    "        return tf.concat([x, y], axis=-1)\n",
    "        \n",
    "    \n",
    "    def call(self, decoder_input, context_vector, position):\n",
    "        \"\"\" \n",
    "        decoder_input  : last predicted word => (batch_size, sequence len)\n",
    "        context_vector : image's vector      => (batch_size, img_context_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Mask out padding (0)\n",
    "#         decoder_input = self.mask(decoder_input)\n",
    "        \n",
    "        # x1 => (batch_size, input_sentence_len, embedding_dim + positional_dim)\n",
    "        x1 = self.embedding(decoder_input)\n",
    "#         x1_1 = self.embedding(decoder_input)\n",
    "#         x1_2 = self.positional_embedding(position)\n",
    "        \n",
    "#         x1_2 = tf.reduce_mean(x1_2, axis=1)\n",
    "#         x1_2 = tf.expand_dims(x1_2, axis=1)\n",
    "#         x1_2 = tf.tile(x1_2, multiples=[1, x1_1.shape[1], 1])\n",
    "        \n",
    "#         x1 = tf.concat([x1_1, x1_2], axis=-1)\n",
    "        \n",
    "        # x2 (concat) => (batch_size, input_sentence_len, embedding_dim + image_context_size)\n",
    "        # x2 (add) => (batch_size, embedding_dim)\n",
    "        x2, rnn_state = self.apply_strategy(x1, context_vector)\n",
    "        \n",
    "        ## ============================================\n",
    "        ## TODO: add another attention layer ? \n",
    "        ## ============================================\n",
    "        \n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        # x3 => (batch_size, sequence_len, rnn_units)\n",
    "        x3 = self.fc1(x2)   # how important is every sequence \n",
    "        x3 = self.batchnorm(x3)\n",
    "        x3 = self.leakyrelu(x3)\n",
    "        \n",
    "        # x4 => (batch_size, sequence_len * rnn_units) || was (batch_size * sequence_len=1, rnn_units)\n",
    "        x4 = tf.reshape(x3, (x3.shape[0], -1))\n",
    "\n",
    "        x4 = self.dropout(x4)\n",
    "        \n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x4)\n",
    "        \n",
    "        return word_predictions, rnn_state\n",
    "    \n",
    "\n",
    "    def reset_state(self, batch_size=None):\n",
    "        \n",
    "        if batch_size is not None:\n",
    "            return tf.zeros((batch_size, self.rnn_units))\n",
    "        \n",
    "        return tf.zeros((self.batch_size, self.rnn_units))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rnn_units': 256,\n",
       " 'rnn_type': 'BiLSTM',\n",
       " 'tokenizer': 'BERT',\n",
       " 'word_embedding': 'BERT',\n",
       " 'vocab_size': 3000,\n",
       " 'combine_strategy': 'merge',\n",
       " 'combine_layer': 'concat',\n",
       " 'image_context_size': 256,\n",
       " 'word_embedding_dim': 256,\n",
       " 'batch_size': 32,\n",
       " 'data_size': 5000,\n",
       " 'use_mapping': True,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_caption_length': 25,\n",
       " 'image_feature_extractor': 'xception',\n",
       " 'use_sequence': True,\n",
       " 'epoch': 20,\n",
       " 'use_mask': True,\n",
       " 'beam_search_k': 5,\n",
       " 'version': 'v5 : positional embedding'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0401 11:18:39.711151 140502276835136 configuration_utils.py:256] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/m13516112/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0401 11:18:39.712848 140502276835136 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0401 11:18:40.922376 140502276835136 modeling_tf_utils.py:333] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5 from cache at /home/m13516112/.cache/torch/transformers/d667df51ec24c20190f01fb4c20a21debc4c4fc12f7e2f5441ac0a99690e3ee9.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5\n",
      "I0401 11:18:46.212471 140502276835136 modeling_tf_utils.py:375] Layers from pretrained model not used in TFBertModel: ['mlm___cls', 'nsp___cls']\n"
     ]
    }
   ],
   "source": [
    "encoder = CNN_Encoder(\n",
    "    output_dim=PARAMS[\"image_context_size\"]\n",
    ")\n",
    "\n",
    "attention = BahdanauAttention(\n",
    "    units=PARAMS[\"rnn_units\"]\n",
    ")\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    rnn_type=PARAMS[\"rnn_type\"], \n",
    "    rnn_units=PARAMS[\"rnn_units\"],\n",
    "    embedding_type=PARAMS[\"word_embedding\"], \n",
    "    embedding_dim=PARAMS[\"word_embedding_dim\"],  \n",
    "    combine_strategy=PARAMS[\"combine_strategy\"], \n",
    "    combine_layer=PARAMS[\"combine_layer\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    "    batch_size=PARAMS[\"batch_size\"]\n",
    ")\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Requirements\n",
    "\n",
    "# combine_strategy = \"inject_init\" : IMAGE_CONTEXT_SIZE == UNITS\n",
    "# combine_strategy = \"inject_pre\"  : IMAGE_CONTEXT_SIZE == WORD_EMBEDDING_DIM\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default feed forward function\n",
    "\n",
    "@tf.function\n",
    "def feed_forward(img_tensor, caption, position):\n",
    "    \"\"\"\n",
    "    img_tensor => (batch_size, image_feature_size, 2048)\n",
    "    caption => (batch_size, max_caption_length)\n",
    "    \"\"\"\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    # note : used to be decoder hidden state, \n",
    "    # but changed into current word embedding for paralel training\n",
    "    \n",
    "    # hidden => (batch_size, embedding_size)\n",
    "    hidden = decoder.embedding(caption, as_sentence=True)\n",
    "\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = encoder(img_tensor)\n",
    "\n",
    "    # context_vector => (batch_size, image_context_size)\n",
    "    context_vector, _ = attention(features, hidden)\n",
    "\n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions, _ = decoder(caption, context_vector, position)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, caption, target, position):\n",
    "    \"\"\"\n",
    "    target => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training model\n",
    "    with tf.GradientTape() as gradient_tape:\n",
    "        \n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions = feed_forward(img_tensor, caption, position)\n",
    "        \n",
    "        # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "        loss = loss_function(target, predictions)\n",
    "\n",
    "        \n",
    "    # Apply gradient\n",
    "    trainable_variables = encoder.trainable_variables + \\\n",
    "                          decoder.trainable_variables + \\\n",
    "                          attention.trainable_variables\n",
    "    \n",
    "    gradients = gradient_tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def eval_step(img_tensor, caption, target):\n",
    "    \n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions = feed_forward(img_tensor, caption)\n",
    "    \n",
    "    # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "    loss = loss_function(target, predictions)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def choose_predicted_id(predictions, strategy=\"max\", sampling_k=10):\n",
    "    \"\"\"\n",
    "    predictions : encoder word prediction => (batch_size, vocab_size)\n",
    "    strategy    : how to choose word [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sampling method (categorical dist)\n",
    "    if strategy == \"sample\":\n",
    "        \n",
    "        # sampled_proba & sampled_ids => (batch_size, sampling_k)\n",
    "        sampled_proba, sampled_ids = tf.math.top_k(predictions, sampling_k)\n",
    "        \n",
    "        # chosen_sampled_col => (batch_size, )\n",
    "        chosen_sampled_col = tf.squeeze(tf.random.categorical(sampled_proba, 1))\n",
    "        \n",
    "        # create row idx to zip with chosen_sampled_col\n",
    "        row_idx = tf.range(predictions.shape[0], dtype=chosen_sampled_col.dtype)\n",
    "        row_col_idx = tf.stack([row_idx, chosen_sampled_col], axis=1)\n",
    "        \n",
    "        # predicted_ids => (batch_size, )\n",
    "        predicted_ids = tf.gather_nd(sampled_ids, row_col_idx)\n",
    "\n",
    "    # Max index method\n",
    "    else:\n",
    "        predicted_ids = tf.argmax(predictions, 1)\n",
    "    \n",
    "    # predicted_ids => (batch_size, )\n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(images_paths):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [load_image(x)[0] for x in images_paths]\n",
    "    \n",
    "    # x => (batch_size, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, 10, 10, 2048)\n",
    "    x = extractor(x)\n",
    "    \n",
    "    # x  => (batch_size, img_feature_size, 2048)\n",
    "    x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = encoder(x)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def get_supporting_features(images_paths, strategy=\"mean\"):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, img_count, 1)\n",
    "    strategy : strategy to aggregate multiple supporting image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [[load_image(x)[0] for x in images_set] for images_set in images_paths]\n",
    "    \n",
    "    # x => (batch_size, img_count, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, img_count, 10, 10, 2048)\n",
    "    x = [extractor(image_set) for image_set in x]\n",
    "    \n",
    "    # features => (batch_size, img_count, img_feature_size, image_context_size)\n",
    "    features = encoder(x)\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    if strategy == \"logsumexp\":\n",
    "        features = tf.reduce_logsumexp(features, 1)\n",
    "    elif strategy == \"max\":\n",
    "        features = tf.reduce_max(features, 1)\n",
    "    elif strategy == \"min\":\n",
    "        features = tf.reduce_min(features, 1)\n",
    "    else:\n",
    "        features = tf.reduce_mean(features, 1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_indices(support_text):\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(0, len(support_text)):\n",
    "        context_token = target_tokenizer.tokenize(support_text[i])\n",
    "        context_token_id = target_tokenizer.convert_tokens_to_ids(context_token)\n",
    "        context_token_id = set(context_token_id)\n",
    "        context_token_id.discard(0)\n",
    "        for x in sorted(context_token_id):\n",
    "            indices.append([i, x])\n",
    "\n",
    "    # return => (word_count, 2)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_supporting_text_vector(support_text, vocab_size):\n",
    "    \"\"\"\n",
    "    support_text : list of text describing main image context => (batch_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(support_text)\n",
    "    \n",
    "    # indices => ( sum(batch_size * ?word_count), 2)\n",
    "    indices = get_one_hot_indices(support_text)\n",
    "    values = tf.ones(len(indices))\n",
    "    sparse_one_hot = tf.sparse.SparseTensor(indices, values, dense_shape=[batch_size, vocab_size])\n",
    "    \n",
    "    # sparse_one_hot => (batch_size, vocab_size)\n",
    "    return sparse_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluate(images_paths,\n",
    "                    support_text=None,\n",
    "                    support_imgs=None, \n",
    "                    support_aggregate_strategy=\"mean\",\n",
    "                    pplm_iteration=3,\n",
    "                    pplm_weight=0.03,\n",
    "                    pplm_gm_weight=0.8,\n",
    "                    choose_word_strategy=\"sample\",\n",
    "                    choose_word_sample_k=5,\n",
    "                   ):\n",
    "    \n",
    "    \"\"\"\n",
    "    images_paths : list of image_path                           => (batch_size, 1)\n",
    "    support_text : list of text describing main image context   => (batch_size)\n",
    "    support_imgs : list of list of image_path                   => (batch_size, image_count, 1)\n",
    "    support_aggregate_strategy : how to aggregate support image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    pplm_iteration : number of pplm step done for every decoding step\n",
    "    pplm_weight    : weight of pplm loss\n",
    "    pplm_gm_weight : geometric mean fusion weight (0 means use only original prediction, 1 means use only pplm prediction)\n",
    "    choose_word_strategy : how to choose word from prediction distribution [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(images_paths)\n",
    "    \n",
    "    # initialize captions placeholder\n",
    "    start_token = caption_tokenizer.convert_tokens_to_ids(['[CLS]']) # use bert id's not custom id\n",
    "    result_captions = tf.tile(tf.expand_dims(start_token, 1), [batch_size, 1])\n",
    "    attention_plot = tf.reshape([], shape=(batch_size, 0, IMAGE_FEATURE_SHAPE[0]))\n",
    "    \n",
    "    # Extract features from main images\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = get_image_features(images_paths)\n",
    "    \n",
    "    \n",
    "    if support_text is not None:\n",
    "        # support_text_vector => (batch_size, vocab_size)\n",
    "        support_text_vector = get_supporting_text_vector(support_text, decoder.vocab_size) \n",
    "    else:\n",
    "        # set all pplm related variable to 0\n",
    "        pplm_iteration = 0\n",
    "        pplm_weight = 0\n",
    "        pplm_gm_weight = 0\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(MAX_CAPTION_LENGTH)):\n",
    "        \n",
    "        # decoder_input => (batch_size, ~MAX_CAPTION_LENGTH)\n",
    "        caption = pad_sequences(result_captions, maxlen=MAX_CAPTION_LENGTH, padding=\"post\")\n",
    "        \n",
    "        # hidden => (batch_size, embedding_size)\n",
    "        hidden = decoder.embedding(caption, as_sentence=True)  ## BOTTLE\n",
    "\n",
    "        # context_vector => (batch_size, image_context_size)\n",
    "        # attention_weights => (batch_size, img_feature_size, 1)\n",
    "        context_vector, attention_weights = attention(features, hidden)\n",
    "\n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions, _ = decoder(caption, context_vector, position=None)  \n",
    "        \n",
    "        # ======================== PPLM section ========================\n",
    "        ori_prediction = predictions\n",
    "        \n",
    "        curr_pertubation = tf.Variable(tf.zeros((batch_size, decoder.embedding_dim)), name=\"curr_pertubation\", trainable=True)\n",
    "        \n",
    "        \n",
    "        for j in range(pplm_iteration):\n",
    "            \n",
    "            with tf.GradientTape() as pplm_tape: \n",
    "                \n",
    "                hidden += curr_pertubation\n",
    "                \n",
    "                context_vector, attention_weights = attention(features, hidden)\n",
    "                predictions, _ = decoder(caption, context_vector)\n",
    "                pplm_loss = pplm_loss_function(support_text_vector, predictions, pplm_weight=pplm_weight)\n",
    "                \n",
    "            \"\"\"\n",
    "            most impactfull layer to train = last dense layer\n",
    "            \"\"\"\n",
    "            \n",
    "            trainable_variables = [curr_pertubation]\n",
    "            gradients = pplm_tape.gradient(pplm_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "            \n",
    "            predictions, _ = decoder(caption, context_vector)\n",
    "        \n",
    "        # fuse final pplm_prediction and original prediction\n",
    "        fused_predictions = (predictions * pplm_gm_weight) + (ori_prediction * (1 - pplm_gm_weight)) \n",
    "        \n",
    "        # predicted_ids => (batch_size,)\n",
    "        predicted_ids = choose_predicted_id(fused_predictions, strategy=choose_word_strategy, sampling_k=choose_word_sample_k)\n",
    "        \n",
    "        # convert custom id mapping to bert's id\n",
    "        # predicted_bert_ids => (batch_size, 1)\n",
    "        predicted_bert_ids = target_tokenizer._convert_custom_id_to_bert_id(predicted_ids.numpy())\n",
    "        predicted_bert_ids = tf.expand_dims(predicted_bert_ids, 1)\n",
    "        \n",
    "        # store result\n",
    "        result_captions = tf.concat([result_captions, predicted_bert_ids], axis=1)\n",
    "        \n",
    "        # attention_weights => (batch_size, 1, img_feature_size)\n",
    "        attention_weights = tf.reshape(attention_weights, shape=(batch_size, 1, -1))\n",
    "        \n",
    "        # assign attention weights to respective generated word\n",
    "        # attention_plot => (batch_size, ~max_caption_len, feature_size)\n",
    "        attention_plot = tf.concat([attention_plot, attention_weights], axis=1)\n",
    "        \n",
    "    # remove start token & revert to tokens\n",
    "    result_captions = [caption_tokenizer.convert_ids_to_tokens(x[1:]) for x in result_captions]\n",
    "    \n",
    "    return result_captions, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy, MeanSquaredError\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def pplm_loss_function(real, pred, pplm_weight=0.03):\n",
    "    \"\"\"\n",
    "    real  => (batch_size, vocab_size)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "\n",
    "#     mm = tf.sparse.sparse_dense_matmul(real, tf.transpose(pred))\n",
    "#     mm = tf.reduce_sum(tf.abs(mm), 1)\n",
    "#     loss = tf.reduce_sum(mm, 0)\n",
    "    \n",
    "    real = tf.sparse.to_dense(real, default_value=0)\n",
    "    pplm_loss = CategoricalCrossentropy(from_logits=True)\n",
    "    loss = pplm_loss(real, pred, pplm_weight)\n",
    "    \n",
    "#     print(loss)\n",
    "    return loss\n",
    "\n",
    "    \"\"\"\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1395it [07:54,  2.94it/s]\n",
      "1395it [07:26,  3.13it/s]\n",
      "1395it [07:13,  3.22it/s]\n",
      "1395it [07:03,  3.29it/s]\n",
      "1395it [05:17,  4.40it/s]\n",
      "1395it [06:01,  3.86it/s]\n",
      "1395it [07:33,  3.08it/s]\n",
      "1395it [08:41,  2.67it/s]\n",
      "1395it [09:39,  2.41it/s]\n",
      "1395it [09:42,  2.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc5e1c09ba8>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    for (img_tensor, captions, target, target_position) in tqdm(train_dataset):\n",
    "        batch_loss = train_step(img_tensor, captions, target, target_position)\n",
    "        loss.append(batch_loss.numpy())\n",
    "        \n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc5e1b90748>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5145476198643446"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-500:]) / len(loss[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6932637613281608"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-5000:]) / len(loss[-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:00<00:00, 496015.21it/s]\n"
     ]
    }
   ],
   "source": [
    "true_captions = {}\n",
    "for annot in tqdm(annotations[\"annotations\"]):\n",
    "    image_id = annot[\"image_id\"]\n",
    "\n",
    "    if image_id not in true_captions:\n",
    "        true_captions[image_id] = []\n",
    "    \n",
    "    true_captions[image_id].append(annot['caption'])\n",
    "    \n",
    "    \n",
    "def get_true_captions(image_paths):\n",
    "    return [true_captions[int(x[-10:-4])] for x in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = all_img_paths[155:159]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_sample_k=10)\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = all_img_paths[155:159]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_strategy=\"max\")\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = all_img_paths[155:159]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_sample_k=5)\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train/{}\".format(str(datetime.now())[:-10])\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           attention=attention,\n",
    "                           optimizer=optimizer\n",
    "                          )\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------\")\n",
    "print(checkpoint_path)\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(checkpoint_path)\n",
    "with open(checkpoint_path + \"/config.txt\", \"w\") as f:\n",
    "    f.write(str(PARAMS))\n",
    "    \n",
    "log_file = open(checkpoint_path + \"/log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, PARAMS[\"epoch\"]):\n",
    "    \n",
    "    start = time.time()\n",
    "    loss = 0\n",
    "    batch = 1\n",
    "\n",
    "    for img_tensor, captions, target in tqdm(train_dataset):\n",
    "        \n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "\n",
    "            with open(checkpoint_path + \"/log.txt\", \"a\") as f:\n",
    "                log_message = ' {} Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                    checkpoint_path, epoch + 1, batch, batch_loss.numpy())\n",
    "                f.write(str(log_message + \"\\n\"))\n",
    "                print(log_message)   \n",
    "\n",
    "        batch += 1\n",
    "        \n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(batch_loss.numpy())\n",
    "\n",
    "#     ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = []\n",
    "\n",
    "# for (img_tensor, captions, target) in tqdm(train_dataset):\n",
    "#     batch_loss = train_step(img_tensor, captions, target)\n",
    "#     loss.append(batch_loss.numpy())\n",
    "    \n",
    "# plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PPLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "# text = [\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "# ]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images, support_text=text, pplm_iteration=5, pplm_weight=1)\n",
    "# result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
