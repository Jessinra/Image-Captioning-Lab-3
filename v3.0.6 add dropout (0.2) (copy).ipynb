{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git add \"Keras.ipynb\"\n",
    "!git commit -m \"test pred using pplm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"rnn_units\": 256,\n",
    "    \"rnn_type\": \"BiLSTM\",\n",
    "    \"tokenizer\": \"BERT\",\n",
    "    \"word_embedding\": \"BERT\",\n",
    "    \"vocab_size\": 3000,\n",
    "    \"combine_strategy\": \"merge\",\n",
    "    \"combine_layer\": \"concat\",\n",
    "    \"image_context_size\": 256,\n",
    "    \"word_embedding_dim\": 256,\n",
    "    \"batch_size\": 32,\n",
    "    \"data_size\": 5000,\n",
    "    \"use_mapping\": True,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_caption_length\": 25, # use <int> or None\n",
    "    \"image_feature_extractor\": \"xception\",\n",
    "    \"use_sequence\": True,\n",
    "    \"epoch\": 20,\n",
    "    \"use_mask\": True,\n",
    "    \n",
    "    \"beam_search_k\": 5,\n",
    "    \"version\": \"v5 : positional embedding\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "\n",
    "# for TFBertModel\n",
    "PROXIES = {\n",
    "  \"http\": \"http://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "  \"https\": \"https://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"[CLS] \" + annot['caption'] + \" [SEP]\"\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "\n",
    "stopper = -1 if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]\n",
    "train_captions = all_captions[:stopper]\n",
    "train_img_paths = all_img_paths[:stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 5000\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = len(train_captions) if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature_extractor(model_type=\"xception\"):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.compat.v1.keras.applications.xception\n",
    "        cnn_model = tf.compat.v1.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"CNN encoder model not supported yet\")\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "def get_image_feature_shape(model_type):\n",
    "    \n",
    "    if model_type == \"xception\":\n",
    "        return (100, 2048)\n",
    "    elif model_type == \"inception_v3\":\n",
    "        return (64, 2048)\n",
    "    else:\n",
    "        raise Exception (\"model unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = extractor_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FEATURE_SHAPE = get_image_feature_shape(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extractor, extractor_preprocessor = get_image_feature_extractor(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image feature\n",
    "\n",
    "this step mostly skipped since the results are cached already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 157.25\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = PARAMS[\"batch_size\"]\n",
    "\n",
    "estimated_batch_count = DATA_SIZE / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique images\n",
    "# unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# # Prepare dataset\n",
    "# image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "# image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "# image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessed image (batch)\n",
    "\n",
    "# for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "#     # get context vector of batch images\n",
    "#     batch_features = extractor(batch_imgs)\n",
    "    \n",
    "#     # flatten 2D cnn result into 1D for RNN decoder input\n",
    "#     # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "#     # image_feature = 100 (Xception)\n",
    "#     # image_feature = 64 (Inception V3)\n",
    "#     batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "#     # Cache preprocessed image\n",
    "#     for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "#         image_path = image_path.numpy().decode(\"utf-8\")\n",
    "#         np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Caption dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0401 09:47:23.483365 140433758590784 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0401 09:47:23.484695 140433758590784 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = PARAMS[\"tokenizer\"]\n",
    "VOCAB_SIZE = PARAMS[\"vocab_size\"]  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "    \n",
    "    def use_custom_mapping(self, use_mapping=True, vocab_size=3000):\n",
    "        \n",
    "        self.use_mapping = use_mapping\n",
    "        self.cust_vocab_size = vocab_size\n",
    "        self.mapping_initialized = False\n",
    "\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into custom indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        bert_ids = [self.convert_tokens_to_ids(self.tokenize(x)) for x in tqdm(texts)]\n",
    "        \n",
    "        if not self.use_mapping:\n",
    "            return bert_ids\n",
    "        \n",
    "        if not self.mapping_initialized:\n",
    "            self._initialize_custom_mapping(bert_ids)\n",
    "            return [self._convert_bert_id_to_custom_id(x) for x in tqdm(bert_ids)]\n",
    "        \n",
    "        return bert_ids\n",
    "    \n",
    "        \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \n",
    "        bert_ids = super().convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            return self._convert_bert_id_to_custom_id(bert_ids)\n",
    "        else:\n",
    "            return bert_ids\n",
    "        \n",
    "        \n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            bert_ids = self._convert_custom_id_to_bert_id(token_ids)\n",
    "        else:\n",
    "            bert_ids = token_ids\n",
    "            \n",
    "        bert_tokens = super().convert_ids_to_tokens(bert_ids)\n",
    "        return bert_tokens\n",
    "    \n",
    "    \n",
    "    def _initialize_custom_mapping(self, corpus_bert_ids):\n",
    "        \n",
    "        print(\"    > constructing custom mapping < \\n\")\n",
    "        self._build_occurence_table(corpus_bert_ids)\n",
    "        self._build_custom_mapping_table()\n",
    "        self.mapping_initialized = True\n",
    "        \n",
    "        \n",
    "    def _build_occurence_table(self, tokenized_captions):\n",
    "        \"\"\"\n",
    "        build dict of token frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        self.occurence_table = {}\n",
    "        for caption in tqdm(tokenized_captions):\n",
    "            for token in caption:\n",
    "                if token not in self.occurence_table:\n",
    "                    self.occurence_table[token] = 0\n",
    "                self.occurence_table[token] += 1\n",
    "                \n",
    "    \n",
    "    def _build_custom_mapping_table(self):\n",
    "        \n",
    "        _special_token = ['[UNK]', '[PAD]']\n",
    "        _actual_vocab_size = self.cust_vocab_size - len(_special_token)\n",
    "        \n",
    "        sorted_occurence = {k: v for k, v in sorted(\n",
    "            self.occurence_table.items(), reverse=True, key=lambda item: item[1]\n",
    "        )}\n",
    "        \n",
    "        used_tokens = sorted(list(sorted_occurence)[:_actual_vocab_size])\n",
    "        mapping_size = min(len(used_tokens), _actual_vocab_size)\n",
    "        \n",
    "        _bert_pad = 0\n",
    "        _bert_oov = 100\n",
    "        self._custom_pad = 0\n",
    "        self._custom_oov = mapping_size + 1\n",
    "        \n",
    "        self.bert_id_to_custom_id = {\n",
    "            _bert_pad: self._custom_pad, \n",
    "            _bert_oov: self._custom_oov\n",
    "        }\n",
    "        self.custom_id_to_bert_id = {\n",
    "            self._custom_pad: _bert_pad, \n",
    "            self._custom_oov: _bert_oov\n",
    "        }\n",
    "        \n",
    "        for i in range(0, mapping_size):\n",
    "            bert_token = used_tokens[i]\n",
    "            self.bert_id_to_custom_id[bert_token] = i + 1    \n",
    "            self.custom_id_to_bert_id[i + 1] = bert_token\n",
    "            \n",
    "        print(\"Vocab contains {0} / {1} unique tokens ({2:.2f} %)\".format(\n",
    "            len(used_tokens) + 2,\\\n",
    "            len(sorted_occurence),\\\n",
    "            (len(used_tokens) / len(sorted_occurence) * 100)\n",
    "        ))\n",
    "        \n",
    "        sorted_occurence_count = list(sorted_occurence.values())\n",
    "        used_tokens_count = sum(sorted_occurence_count[:_actual_vocab_size])\n",
    "        total_tokens_count = sum(sorted_occurence_count)\n",
    "        \n",
    "        print(\"Using {0} / {1} tokens available ({2:.2f} %)\".format(\n",
    "            used_tokens_count,\\\n",
    "            total_tokens_count,\\\n",
    "            (used_tokens_count / total_tokens_count * 100)\n",
    "        ))        \n",
    "        \n",
    "    def _convert_bert_id_to_custom_id(self, token_ids):\n",
    "        return [self.bert_id_to_custom_id[x] if x in self.bert_id_to_custom_id else self._custom_oov for x in token_ids]\n",
    "    \n",
    "    def _convert_custom_id_to_bert_id(self, token_ids):\n",
    "        return [self.custom_id_to_bert_id[x] for x in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_type, use_mapping, vocab_size):\n",
    "    \n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    if tokenizer_type == \"BERT\" :\n",
    "        tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "        tokenizer.use_custom_mapping(use_mapping, vocab_size)\n",
    "\n",
    "    # use default keras tokenizer\n",
    "    else : \n",
    "        tokenizer = TokenizerWrapper(num_words=vocab_size, oov_token=\"[UNK]\")\n",
    "        tokenizer.fit_on_texts(train_captions)    \n",
    "        tokenizer.word_index['[PAD]'] = 0\n",
    "        tokenizer.index_word[0] = '[PAD]'\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0401 09:47:29.283800 140433758590784 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0401 09:47:30.755156 140433758590784 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "caption_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=False,\n",
    "    vocab_size=0\n",
    ")\n",
    "\n",
    "target_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=PARAMS[\"use_mapping\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 2561.16it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2663.45it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 205946.38it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 210424.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > constructing custom mapping < \n",
      "\n",
      "Vocab contains 3000 / 3584 unique tokens (83.65 %)\n",
      "Using 68179 / 68765 tokens available (99.15 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "caption_tokens = caption_tokenizer.texts_to_sequences(train_captions)\n",
    "target_tokens = target_tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parallel dataset\n",
    "\n",
    "prepare captions and target to support parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parallel_dataset(img_paths, caption_tokens, target_tokens):\n",
    "    \n",
    "    dataset_img_paths = []\n",
    "    dataset_captions = []\n",
    "    dataset_target = []\n",
    "    dataset_target_position = []\n",
    "\n",
    "    for i in tqdm(range(0, len(train_img_paths))):\n",
    "        img = img_paths[i]\n",
    "        cap = caption_tokens[i]\n",
    "        tar = target_tokens[i]\n",
    "\n",
    "        for j in range(1, len(cap)):\n",
    "            dataset_img_paths.append(img)\n",
    "            dataset_captions.append(cap[:j])\n",
    "            dataset_target.append(tar[j])\n",
    "            dataset_target_position.append(j)\n",
    "\n",
    "    dataset_captions = pad_sequences(dataset_captions, maxlen=PARAMS[\"max_caption_length\"], padding='post')\n",
    "    \n",
    "    return dataset_img_paths, dataset_captions, dataset_target, dataset_target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 19266.56it/s]\n"
     ]
    }
   ],
   "source": [
    "parallel_img_paths, parallel_captions, parallel_target, parallel_target_position = preprocess_parallel_dataset(train_img_paths, caption_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = parallel_captions.shape[0]\n",
    "MAX_CAPTION_LENGTH = parallel_captions.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_target_position = tf.one_hot(parallel_target_position, depth=MAX_CAPTION_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "\n",
    "def load_dataset(img_name, caption, target, target_pos):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, caption, target, target_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((parallel_img_paths, parallel_captions, parallel_target, parallel_target_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "\n",
    "dataset = dataset.map(lambda item1, item2, item3, item4: tf.numpy_function(\n",
    "          load_dataset, [item1, item2, item3, item4], [tf.float32, tf.int32, tf.int32, tf.float32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(DATA_SIZE / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 16, image_feature = 100, 2048)\n",
    "# caption => (batch_size = 16, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1395 batches, (total : 44640)\n",
      "eval : 298 batches, (total : 9536)\n",
      "test : 300 batches, (total : 9600 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captioning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=PARAMS[\"learning_rate\"])\n",
    "# loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    real  => (batch_size,)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate loss\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # create mask to filter out padding token \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    # Ignore loss_ if real token is padding\n",
    "    loss_ *= mask\n",
    "    \n",
    "    # Get mean of curren batch's loss (somewhat batch norm)\n",
    "    result_loss = tf.reduce_mean(loss_)\n",
    "    \n",
    "    return result_loss\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    loss_  => (batch_size, 1)\n",
    "    mask   => (batch_size, 1)  : indicate is padding or not\n",
    "\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim)\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # x => (batch_size, 100, 2048)\n",
    "        x = self.fc(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, image_feature_size, image_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, img_feature_size, image_context_size)\n",
    "        hidden                        => (batch_size, embedding_size)\n",
    "        \n",
    "        note : \n",
    "        img_feature_size ==  64 for Inception V3,\n",
    "        img_feature_size == 100 for Xception,\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        _w1 = self.W1(features)\n",
    "        _w2 = self.W2(hidden_with_time_axis)\n",
    "        score = tf.nn.tanh(_w1 + _w2)\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_with_time_axis      => (batch_size, 1, embedding_size)\n",
    "        score                      => (batch_size, img_feature_size, units)\n",
    "        attention_weights          => (batch_size, img_feature_size, 1)\n",
    "        context_vector (after sum) => (batch_size, img_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=256, \n",
    "                 embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 combine_strategy=\"merge\", combine_layer=\"concat\",\n",
    "                 vocab_size=3000, batch_size=32):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # when to use context_vector [\"inject_init\", \"inject_pre\", \"inject_par\", \"merge\"]\n",
    "        self.combine_strategy = combine_strategy\n",
    "        \n",
    "        # how to use context_vector [\"add\", \"concat\"]\n",
    "        self.combine_layer = combine_layer\n",
    "        \n",
    "        # =====================================================\n",
    "        \n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "        \n",
    "#         self.mask = Masking(mask_value=0, dtype=\"int32\")\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "        self.positional_embedding = Embedding(\n",
    "            input_dim=MAX_CAPTION_LENGTH,\n",
    "            output_dim=16,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "        \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\") # same size as vocab\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "           \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\") # same size as vocab\n",
    "        \n",
    "        \n",
    "    def _init_embedding(self):\n",
    "        \n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.bert_embedding = TFBertModel.from_pretrained('bert-base-uncased', proxies=PROXIES)\n",
    "            self.bert_embedding.trainable = False\n",
    "            self.embedding_dim = self.bert_embedding.config.hidden_size\n",
    "            \n",
    "        else:\n",
    "            self.default_embedding = Embedding(\n",
    "                input_dim=self.vocab_size, \n",
    "                output_dim=self.embedding_dim, \n",
    "                mask_zero=True,\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def _init_rnn(self):\n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector'\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            self.bilstm = Bidirectional(LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        \n",
    "    def embedding(self, tokens, as_sentence=False):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embedding_type == \"BERT\": \n",
    "            embedding, sentence_embedding = self._bert_embedding(tokens)\n",
    "        \n",
    "        else:\n",
    "            embedding = self._default_embedding(tokens)\n",
    "            sentence_embedding = tf.reduce_mean(embedding, 1)\n",
    "            \n",
    "\n",
    "        if as_sentence:\n",
    "            # embedding => (batch_size, embedding_dim)\n",
    "            return sentence_embedding\n",
    "        \n",
    "        else:\n",
    "            # embedding => (batch_size, tokens_length, embedding_dim)\n",
    "            return embedding\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        embedding (tokens)   => (batch_size, tokens_length, embedding_dim)\n",
    "        embedding (sentence) => (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def _bert_embedding(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        # mask out attention from padding\n",
    "        # mask => (batch_size, sequence_length)\n",
    "        attention_mask = x == 0\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, attention_mask=attention_mask)\n",
    "        \"\"\"\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        token_type_ids = tf.cast((x == 0), tf.int32)\n",
    "        \n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, token_type_ids=token_type_ids)\n",
    "        return hidden_states, sentence_embedding\n",
    "    \n",
    "        \"\"\"\n",
    "        hidden states contains hidden state for each word\n",
    "        sentence_embedding is general embedding for whole sentences\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def _default_embedding(self, x):\n",
    "        \n",
    "        # embedding => (batch_size, sequence_length, embedding_size)\n",
    "        embedding = self.default_embedding(x)\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "    def apply_strategy(self, x, context_vector, curr_iter=-1):\n",
    "        \"\"\"\n",
    "        context_vector : image's vector\n",
    "        x              : rnn input (word embedding)\n",
    "        strategy       : \n",
    "        curr_iter      : current iteration number\n",
    "        \n",
    "        No longer support inject_init & inject_pre, since training become fully parallel\n",
    "        \"\"\"\n",
    "        \n",
    "#         if self.combine_strategy == \"inject_init\":\n",
    "#             initial_state = tf.squeeze(context_vector) if curr_iter == 1 else None\n",
    "#             output, state = self.rnn_model(x, initial_state=initial_state)  \n",
    "            \n",
    "#         elif self.combine_strategy == \"inject_pre\":\n",
    "#             x = context_vector if curr_iter == 1 else x\n",
    "#             output, state = self.rnn_model(x)  \n",
    "            \n",
    "        if self.combine_strategy == \"inject_par\":\n",
    "            \n",
    "            context_vector = tf.expand_dims(context_vector, 1)\n",
    "            context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "                        \n",
    "            x = self.custom_combine_layer(context_vector, x)\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output, state = self.rnn_model(x)              \n",
    "            \n",
    "        else: # merge (as default)\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output, state = self.rnn_model(x) \n",
    "            \n",
    "            context_vector = tf.expand_dims(context_vector, 1)\n",
    "            context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output = self.custom_combine_layer(context_vector, output)\n",
    "            \n",
    "        return output, state\n",
    "    \n",
    "    \n",
    "    def rnn_model(self, x, initial_state=None):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            \n",
    "            # adjust initial state, LSTM has 2 hidden states (h and c)\n",
    "            if initial_state is not None:\n",
    "                init_h = initial_state\n",
    "                init_c = tf.zeros(initial_state.shape)\n",
    "                initial_state = [init_h, init_c]\n",
    "            \n",
    "            output, h_state, c_state = self.lstm(x, initial_state=initial_state)\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            output, _, h_state, _, _ = self.bilstm(x)\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, h_state = self.gru(x, initial_state=initial_state)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, h_state\n",
    "    \n",
    "        \"\"\"\n",
    "        output => (batch_size, rnn_size)\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def custom_combine_layer(self, x, y):\n",
    "        if self.combine_layer == \"add\":\n",
    "            return self._add_layer(x, y)\n",
    "        else:\n",
    "            return self._concat_layer(x, y)\n",
    "\n",
    "        \n",
    "    def _add_layer(self, x, y):\n",
    "        \n",
    "        if x.shape[1] != y.shape[1] :\n",
    "            exception = \"Cannot combine using 'add' strategy, both tensor has different shape {} & {}\"\n",
    "            raise Exception(exception.format(x.shape, y.shape))\n",
    "            \n",
    "        return tf.keras.layers.add([x, y])\n",
    "            \n",
    "        \n",
    "    def _concat_layer(self, x, y):\n",
    "        return tf.concat([x, y], axis=-1)\n",
    "        \n",
    "    \n",
    "    def call(self, decoder_input, context_vector, position):\n",
    "        \"\"\" \n",
    "        decoder_input  : last predicted word => (batch_size, sequence len)\n",
    "        context_vector : image's vector      => (batch_size, img_context_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Mask out padding (0)\n",
    "#         decoder_input = self.mask(decoder_input)\n",
    "        \n",
    "        # x1 => (batch_size, input_sentence_len, embedding_dim + positional_dim)\n",
    "        x1 = self.embedding(decoder_input)\n",
    "#         x1_1 = self.embedding(decoder_input)\n",
    "#         x1_2 = self.positional_embedding(position)\n",
    "        \n",
    "#         x1_2 = tf.reduce_mean(x1_2, axis=1)\n",
    "#         x1_2 = tf.expand_dims(x1_2, axis=1)\n",
    "#         x1_2 = tf.tile(x1_2, multiples=[1, x1_1.shape[1], 1])\n",
    "        \n",
    "#         x1 = tf.concat([x1_1, x1_2], axis=-1)\n",
    "        \n",
    "        # x2 (concat) => (batch_size, input_sentence_len, embedding_dim + image_context_size)\n",
    "        # x2 (add) => (batch_size, embedding_dim)\n",
    "        x2, rnn_state = self.apply_strategy(x1, context_vector)\n",
    "        \n",
    "        ## ============================================\n",
    "        ## TODO: add another attention layer ? \n",
    "        ## ============================================\n",
    "        \n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        # x3 => (batch_size, sequence_len, rnn_units)\n",
    "        x3 = self.fc1(x2)   # how important is every sequence \n",
    "        x3 = self.batchnorm(x3)\n",
    "        x3 = self.leakyrelu(x3)\n",
    "        \n",
    "        # x4 => (batch_size, sequence_len * rnn_units) || was (batch_size * sequence_len=1, rnn_units)\n",
    "        x4 = tf.reshape(x3, (x3.shape[0], -1))\n",
    "\n",
    "        x4 = self.dropout(x4)\n",
    "        \n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x4)\n",
    "        \n",
    "        return word_predictions, rnn_state\n",
    "    \n",
    "\n",
    "    def reset_state(self, batch_size=None):\n",
    "        \n",
    "        if batch_size is not None:\n",
    "            return tf.zeros((batch_size, self.rnn_units))\n",
    "        \n",
    "        return tf.zeros((self.batch_size, self.rnn_units))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rnn_units': 256,\n",
       " 'rnn_type': 'BiLSTM',\n",
       " 'tokenizer': 'BERT',\n",
       " 'word_embedding': 'BERT',\n",
       " 'vocab_size': 3000,\n",
       " 'combine_strategy': 'merge',\n",
       " 'combine_layer': 'concat',\n",
       " 'image_context_size': 256,\n",
       " 'word_embedding_dim': 256,\n",
       " 'batch_size': 32,\n",
       " 'data_size': 5000,\n",
       " 'use_mapping': True,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_caption_length': 25,\n",
       " 'image_feature_extractor': 'xception',\n",
       " 'use_sequence': True,\n",
       " 'epoch': 20,\n",
       " 'use_mask': True,\n",
       " 'beam_search_k': 5,\n",
       " 'version': 'v5 : positional embedding'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0401 09:47:39.747110 140433758590784 configuration_utils.py:256] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/m13516112/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0401 09:47:39.749205 140433758590784 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0401 09:47:40.919034 140433758590784 modeling_tf_utils.py:333] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5 from cache at /home/m13516112/.cache/torch/transformers/d667df51ec24c20190f01fb4c20a21debc4c4fc12f7e2f5441ac0a99690e3ee9.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5\n",
      "I0401 09:47:52.988706 140433758590784 modeling_tf_utils.py:375] Layers from pretrained model not used in TFBertModel: ['nsp___cls', 'mlm___cls']\n"
     ]
    }
   ],
   "source": [
    "encoder = CNN_Encoder(\n",
    "    output_dim=PARAMS[\"image_context_size\"]\n",
    ")\n",
    "\n",
    "attention = BahdanauAttention(\n",
    "    units=PARAMS[\"rnn_units\"]\n",
    ")\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    rnn_type=PARAMS[\"rnn_type\"], \n",
    "    rnn_units=PARAMS[\"rnn_units\"],\n",
    "    embedding_type=PARAMS[\"word_embedding\"], \n",
    "    embedding_dim=PARAMS[\"word_embedding_dim\"],  \n",
    "    combine_strategy=PARAMS[\"combine_strategy\"], \n",
    "    combine_layer=PARAMS[\"combine_layer\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    "    batch_size=PARAMS[\"batch_size\"]\n",
    ")\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Requirements\n",
    "\n",
    "# combine_strategy = \"inject_init\" : IMAGE_CONTEXT_SIZE == UNITS\n",
    "# combine_strategy = \"inject_pre\"  : IMAGE_CONTEXT_SIZE == WORD_EMBEDDING_DIM\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default feed forward function\n",
    "\n",
    "@tf.function\n",
    "def feed_forward(img_tensor, caption, position):\n",
    "    \"\"\"\n",
    "    img_tensor => (batch_size, image_feature_size, 2048)\n",
    "    caption => (batch_size, max_caption_length)\n",
    "    \"\"\"\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    # note : used to be decoder hidden state, \n",
    "    # but changed into current word embedding for paralel training\n",
    "    \n",
    "    # hidden => (batch_size, embedding_size)\n",
    "    hidden = decoder.embedding(caption, as_sentence=True)\n",
    "\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = encoder(img_tensor)\n",
    "\n",
    "    # context_vector => (batch_size, image_context_size)\n",
    "    context_vector, _ = attention(features, hidden)\n",
    "\n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions, _ = decoder(caption, context_vector, position)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, caption, target, position):\n",
    "    \"\"\"\n",
    "    target => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training model\n",
    "    with tf.GradientTape() as gradient_tape:\n",
    "        \n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions = feed_forward(img_tensor, caption, position)\n",
    "        \n",
    "        # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "        loss = loss_function(target, predictions)\n",
    "\n",
    "        \n",
    "    # Apply gradient\n",
    "    trainable_variables = encoder.trainable_variables + \\\n",
    "                          decoder.trainable_variables + \\\n",
    "                          attention.trainable_variables\n",
    "    \n",
    "    gradients = gradient_tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def eval_step(img_tensor, caption, target):\n",
    "    \n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions = feed_forward(img_tensor, caption)\n",
    "    \n",
    "    # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "    loss = loss_function(target, predictions)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def choose_predicted_id(predictions, strategy=\"max\", sampling_k=10):\n",
    "    \"\"\"\n",
    "    predictions : encoder word prediction => (batch_size, vocab_size)\n",
    "    strategy    : how to choose word [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sampling method (categorical dist)\n",
    "    if strategy == \"sample\":\n",
    "        \n",
    "        # sampled_proba & sampled_ids => (batch_size, sampling_k)\n",
    "        sampled_proba, sampled_ids = tf.math.top_k(predictions, sampling_k)\n",
    "        \n",
    "        # chosen_sampled_col => (batch_size, )\n",
    "        chosen_sampled_col = tf.squeeze(tf.random.categorical(sampled_proba, 1))\n",
    "        \n",
    "        # create row idx to zip with chosen_sampled_col\n",
    "        row_idx = tf.range(predictions.shape[0], dtype=chosen_sampled_col.dtype)\n",
    "        row_col_idx = tf.stack([row_idx, chosen_sampled_col], axis=1)\n",
    "        \n",
    "        # predicted_ids => (batch_size, )\n",
    "        predicted_ids = tf.gather_nd(sampled_ids, row_col_idx)\n",
    "\n",
    "    # Max index method\n",
    "    else:\n",
    "        predicted_ids = tf.argmax(predictions, 1)\n",
    "    \n",
    "    # predicted_ids => (batch_size, )\n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(images_paths):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [load_image(x)[0] for x in images_paths]\n",
    "    \n",
    "    # x => (batch_size, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, 10, 10, 2048)\n",
    "    x = extractor(x)\n",
    "    \n",
    "    # x  => (batch_size, img_feature_size, 2048)\n",
    "    x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = encoder(x)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def get_supporting_features(images_paths, strategy=\"mean\"):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, img_count, 1)\n",
    "    strategy : strategy to aggregate multiple supporting image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [[load_image(x)[0] for x in images_set] for images_set in images_paths]\n",
    "    \n",
    "    # x => (batch_size, img_count, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, img_count, 10, 10, 2048)\n",
    "    x = [extractor(image_set) for image_set in x]\n",
    "    \n",
    "    # features => (batch_size, img_count, img_feature_size, image_context_size)\n",
    "    features = encoder(x)\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    if strategy == \"logsumexp\":\n",
    "        features = tf.reduce_logsumexp(features, 1)\n",
    "    elif strategy == \"max\":\n",
    "        features = tf.reduce_max(features, 1)\n",
    "    elif strategy == \"min\":\n",
    "        features = tf.reduce_min(features, 1)\n",
    "    else:\n",
    "        features = tf.reduce_mean(features, 1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_indices(support_text):\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(0, len(support_text)):\n",
    "        context_token = target_tokenizer.tokenize(support_text[i])\n",
    "        context_token_id = target_tokenizer.convert_tokens_to_ids(context_token)\n",
    "        context_token_id = set(context_token_id)\n",
    "        context_token_id.discard(0)\n",
    "        for x in sorted(context_token_id):\n",
    "            indices.append([i, x])\n",
    "\n",
    "    # return => (word_count, 2)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_supporting_text_vector(support_text, vocab_size):\n",
    "    \"\"\"\n",
    "    support_text : list of text describing main image context => (batch_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(support_text)\n",
    "    \n",
    "    # indices => ( sum(batch_size * ?word_count), 2)\n",
    "    indices = get_one_hot_indices(support_text)\n",
    "    values = tf.ones(len(indices))\n",
    "    sparse_one_hot = tf.sparse.SparseTensor(indices, values, dense_shape=[batch_size, vocab_size])\n",
    "    \n",
    "    # sparse_one_hot => (batch_size, vocab_size)\n",
    "    return sparse_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluate(images_paths,\n",
    "                    support_text=None,\n",
    "                    support_imgs=None, \n",
    "                    support_aggregate_strategy=\"mean\",\n",
    "                    pplm_iteration=3,\n",
    "                    pplm_weight=0.03,\n",
    "                    pplm_gm_weight=0.8,\n",
    "                    choose_word_strategy=\"sample\",\n",
    "                    choose_word_sample_k=5,\n",
    "                   ):\n",
    "    \n",
    "    \"\"\"\n",
    "    images_paths : list of image_path                           => (batch_size, 1)\n",
    "    support_text : list of text describing main image context   => (batch_size)\n",
    "    support_imgs : list of list of image_path                   => (batch_size, image_count, 1)\n",
    "    support_aggregate_strategy : how to aggregate support image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    pplm_iteration : number of pplm step done for every decoding step\n",
    "    pplm_weight    : weight of pplm loss\n",
    "    pplm_gm_weight : geometric mean fusion weight (0 means use only original prediction, 1 means use only pplm prediction)\n",
    "    choose_word_strategy : how to choose word from prediction distribution [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(images_paths)\n",
    "    \n",
    "    # initialize captions placeholder\n",
    "    start_token = caption_tokenizer.convert_tokens_to_ids(['[CLS]']) # use bert id's not custom id\n",
    "    result_captions = tf.tile(tf.expand_dims(start_token, 1), [batch_size, 1])\n",
    "    attention_plot = tf.reshape([], shape=(batch_size, 0, IMAGE_FEATURE_SHAPE[0]))\n",
    "    \n",
    "    # Extract features from main images\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = get_image_features(images_paths)\n",
    "    \n",
    "    \n",
    "    if support_text is not None:\n",
    "        # support_text_vector => (batch_size, vocab_size)\n",
    "        support_text_vector = get_supporting_text_vector(support_text, decoder.vocab_size) \n",
    "    else:\n",
    "        # set all pplm related variable to 0\n",
    "        pplm_iteration = 0\n",
    "        pplm_weight = 0\n",
    "        pplm_gm_weight = 0\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(MAX_CAPTION_LENGTH)):\n",
    "        \n",
    "        # decoder_input => (batch_size, ~MAX_CAPTION_LENGTH)\n",
    "        caption = pad_sequences(result_captions, maxlen=MAX_CAPTION_LENGTH, padding=\"post\")\n",
    "        \n",
    "        # hidden => (batch_size, embedding_size)\n",
    "        hidden = decoder.embedding(caption, as_sentence=True)  ## BOTTLE\n",
    "\n",
    "        # context_vector => (batch_size, image_context_size)\n",
    "        # attention_weights => (batch_size, img_feature_size, 1)\n",
    "        context_vector, attention_weights = attention(features, hidden)\n",
    "\n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions, _ = decoder(caption, context_vector, position=None)  \n",
    "        \n",
    "        # ======================== PPLM section ========================\n",
    "        ori_prediction = predictions\n",
    "        \n",
    "        curr_pertubation = tf.Variable(tf.zeros((batch_size, decoder.embedding_dim)), name=\"curr_pertubation\", trainable=True)\n",
    "        \n",
    "        \n",
    "        for j in range(pplm_iteration):\n",
    "            \n",
    "            with tf.GradientTape() as pplm_tape: \n",
    "                \n",
    "                hidden += curr_pertubation\n",
    "                \n",
    "                context_vector, attention_weights = attention(features, hidden)\n",
    "                predictions, _ = decoder(caption, context_vector)\n",
    "                pplm_loss = pplm_loss_function(support_text_vector, predictions, pplm_weight=pplm_weight)\n",
    "                \n",
    "            \"\"\"\n",
    "            most impactfull layer to train = last dense layer\n",
    "            \"\"\"\n",
    "            \n",
    "            trainable_variables = [curr_pertubation]\n",
    "            gradients = pplm_tape.gradient(pplm_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "            \n",
    "            predictions, _ = decoder(caption, context_vector)\n",
    "        \n",
    "        # fuse final pplm_prediction and original prediction\n",
    "        fused_predictions = (predictions * pplm_gm_weight) + (ori_prediction * (1 - pplm_gm_weight)) \n",
    "        \n",
    "        # predicted_ids => (batch_size,)\n",
    "        predicted_ids = choose_predicted_id(fused_predictions, strategy=choose_word_strategy, sampling_k=choose_word_sample_k)\n",
    "        \n",
    "        # convert custom id mapping to bert's id\n",
    "        # predicted_bert_ids => (batch_size, 1)\n",
    "        predicted_bert_ids = target_tokenizer._convert_custom_id_to_bert_id(predicted_ids.numpy())\n",
    "        predicted_bert_ids = tf.expand_dims(predicted_bert_ids, 1)\n",
    "        \n",
    "        # store result\n",
    "        result_captions = tf.concat([result_captions, predicted_bert_ids], axis=1)\n",
    "        \n",
    "        # attention_weights => (batch_size, 1, img_feature_size)\n",
    "        attention_weights = tf.reshape(attention_weights, shape=(batch_size, 1, -1))\n",
    "        \n",
    "        # assign attention weights to respective generated word\n",
    "        # attention_plot => (batch_size, ~max_caption_len, feature_size)\n",
    "        attention_plot = tf.concat([attention_plot, attention_weights], axis=1)\n",
    "        \n",
    "    # remove start token & revert to tokens\n",
    "    result_captions = [caption_tokenizer.convert_ids_to_tokens(x[1:]) for x in result_captions]\n",
    "    \n",
    "    return result_captions, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy, MeanSquaredError\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def pplm_loss_function(real, pred, pplm_weight=0.03):\n",
    "    \"\"\"\n",
    "    real  => (batch_size, vocab_size)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "\n",
    "#     mm = tf.sparse.sparse_dense_matmul(real, tf.transpose(pred))\n",
    "#     mm = tf.reduce_sum(tf.abs(mm), 1)\n",
    "#     loss = tf.reduce_sum(mm, 0)\n",
    "    \n",
    "    real = tf.sparse.to_dense(real, default_value=0)\n",
    "    pplm_loss = CategoricalCrossentropy(from_logits=True)\n",
    "    loss = pplm_loss(real, pred, pplm_weight)\n",
    "    \n",
    "#     print(loss)\n",
    "    return loss\n",
    "\n",
    "    \"\"\"\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1395it [05:41,  4.09it/s]\n",
      "1395it [05:15,  4.42it/s]\n",
      "1395it [05:16,  4.41it/s]\n",
      "1395it [05:12,  4.46it/s]\n",
      "1395it [05:16,  4.40it/s]\n",
      "1395it [05:57,  3.90it/s]\n",
      "1395it [07:33,  3.08it/s]\n",
      "1395it [07:32,  3.09it/s]\n",
      "1395it [07:37,  3.05it/s]\n",
      "1395it [07:37,  3.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb5cbc0d6d8>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    for (img_tensor, captions, target, target_position) in tqdm(train_dataset):\n",
    "        batch_loss = train_step(img_tensor, captions, target, target_position)\n",
    "        loss.append(batch_loss.numpy())\n",
    "        \n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb5cbb94828>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5492827113568782"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-500:]) / len(loss[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7017668343871832"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-5000:]) / len(loss[-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:00<00:00, 502688.17it/s]\n"
     ]
    }
   ],
   "source": [
    "true_captions = {}\n",
    "for annot in tqdm(annotations[\"annotations\"]):\n",
    "    image_id = annot[\"image_id\"]\n",
    "\n",
    "    if image_id not in true_captions:\n",
    "        true_captions[image_id] = []\n",
    "    \n",
    "    true_captions[image_id].append(annot['caption'])\n",
    "    \n",
    "    \n",
    "def get_true_captions(image_paths):\n",
    "    return [true_captions[int(x[-10:-4])] for x in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:22<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "there of a [UNK] a room in a man road\n",
      "\n",
      "true caps : \n",
      "A white toilet hanging off the side of a wall.\n",
      "a fancy bathroom with a lot of cabinet space around the toilet \n",
      "A small, clean, unoccupied bathroom on public transportation.\n",
      "The interior of a bathroom on an airplane\n",
      "A compact style bathroom with the toilet and sink in the wall along with cabinets on the wall.\n",
      "=====================================\n",
      "predicted : \n",
      "the cars shows a stop beds sign sign being the party st off the jet . it rockefeller canada cars are motor police scene large\n",
      "\n",
      "true caps : \n",
      "A picture of a stoplight in the nighttime.\n",
      "Light traffic on a city street at dusk\n",
      "A red streetlight at a rainy city intersection\n",
      "Cars stopped at a stop light during the evening.\n",
      "a city street with cars and street lights at dusk\n",
      "=====================================\n",
      "predicted : \n",
      "the cars sitting by by this people parked with people in street . is fighters\n",
      "\n",
      "true caps : \n",
      "A white truck is passing by a large crowd.\n",
      "The truck is riding slowing down the street in the parade. \n",
      "A Parks and Recreation truck driving along the road as onlookers watch\n",
      "A white utility truck driving down a street.\n",
      "A white utility truck passing by building with people out front.\n",
      "=====================================\n",
      "predicted : \n",
      "small on white bathroom filled preparing with pizzaing in the . [UNK] and a table in the area behind the field and the\n",
      "\n",
      "true caps : \n",
      "A refrigerator cluttered with magnets,papers, and clippings. \n",
      "A white refrigerator freezer sitting on top of a kitchen floor.\n",
      "The refrigerator has a lot of things on it.\n",
      "White refrigerator with papers stuck to the side of it. \n",
      "A refrigerator has magnets and papers and food on top of it in the corner of a yellow tiled kitchen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[155:159]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_sample_k=10)\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:21<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "a small bathroom has a counter and the bar [UNK] [UNK] and a [UNK] bottle\n",
      "\n",
      "true caps : \n",
      "A white toilet hanging off the side of a wall.\n",
      "a fancy bathroom with a lot of cabinet space around the toilet \n",
      "A small, clean, unoccupied bathroom on public transportation.\n",
      "The interior of a bathroom on an airplane\n",
      "A compact style bathroom with the toilet and sink in the wall along with cabinets on the wall.\n",
      "=====================================\n",
      "predicted : \n",
      "a street sign lite up being the the the the the\n",
      "\n",
      "true caps : \n",
      "A picture of a stoplight in the nighttime.\n",
      "Light traffic on a city street at dusk\n",
      "A red streetlight at a rainy city intersection\n",
      "Cars stopped at a stop light during the evening.\n",
      "a city street with cars and street lights at dusk\n",
      "=====================================\n",
      "predicted : \n",
      "a close up of people hanging from a plateet from an meal .\n",
      "\n",
      "true caps : \n",
      "A white truck is passing by a large crowd.\n",
      "The truck is riding slowing down the street in the parade. \n",
      "A Parks and Recreation truck driving along the road as onlookers watch\n",
      "A white utility truck driving down a street.\n",
      "A white utility truck passing by building with people out front.\n",
      "=====================================\n",
      "predicted : \n",
      "a man with white a white in white standing on white a a in a field\n",
      "\n",
      "true caps : \n",
      "A refrigerator cluttered with magnets,papers, and clippings. \n",
      "A white refrigerator freezer sitting on top of a kitchen floor.\n",
      "The refrigerator has a lot of things on it.\n",
      "White refrigerator with papers stuck to the side of it. \n",
      "A refrigerator has magnets and papers and food on top of it in the corner of a yellow tiled kitchen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[155:159]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_strategy=\"max\")\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:21<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "this small bird with a skate [UNK] brushing in red\n",
      "\n",
      "true caps : \n",
      "A white toilet hanging off the side of a wall.\n",
      "a fancy bathroom with a lot of cabinet space around the toilet \n",
      "A small, clean, unoccupied bathroom on public transportation.\n",
      "The interior of a bathroom on an airplane\n",
      "A compact style bathroom with the toilet and sink in the wall along with cabinets on the wall.\n",
      "=====================================\n",
      "predicted : \n",
      "a traffic on a red street near traffic by the street . the of a in the\n",
      "\n",
      "true caps : \n",
      "A picture of a stoplight in the nighttime.\n",
      "Light traffic on a city street at dusk\n",
      "A red streetlight at a rainy city intersection\n",
      "Cars stopped at a stop light during the evening.\n",
      "a city street with cars and street lights at dusk\n",
      "=====================================\n",
      "predicted : \n",
      "this white pole sitting behind on the roof\n",
      "\n",
      "true caps : \n",
      "A white truck is passing by a large crowd.\n",
      "The truck is riding slowing down the street in the parade. \n",
      "A Parks and Recreation truck driving along the road as onlookers watch\n",
      "A white utility truck driving down a street.\n",
      "A white utility truck passing by building with people out front.\n",
      "=====================================\n",
      "predicted : \n",
      "the refrigerator with the some sets of a dimly orange counter items in the kitchen and wood food clothing . in modern field on\n",
      "\n",
      "true caps : \n",
      "A refrigerator cluttered with magnets,papers, and clippings. \n",
      "A white refrigerator freezer sitting on top of a kitchen floor.\n",
      "The refrigerator has a lot of things on it.\n",
      "White refrigerator with papers stuck to the side of it. \n",
      "A refrigerator has magnets and papers and food on top of it in the corner of a yellow tiled kitchen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[155:159]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_sample_k=5)\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "stop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-5981adc29e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: stop"
     ]
    }
   ],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train/{}\".format(str(datetime.now())[:-10])\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           attention=attention,\n",
    "                           optimizer=optimizer\n",
    "                          )\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------\")\n",
    "print(checkpoint_path)\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(checkpoint_path)\n",
    "with open(checkpoint_path + \"/config.txt\", \"w\") as f:\n",
    "    f.write(str(PARAMS))\n",
    "    \n",
    "log_file = open(checkpoint_path + \"/log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, PARAMS[\"epoch\"]):\n",
    "    \n",
    "    start = time.time()\n",
    "    loss = 0\n",
    "    batch = 1\n",
    "\n",
    "    for img_tensor, captions, target in tqdm(train_dataset):\n",
    "        \n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "\n",
    "            with open(checkpoint_path + \"/log.txt\", \"a\") as f:\n",
    "                log_message = ' {} Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                    checkpoint_path, epoch + 1, batch, batch_loss.numpy())\n",
    "                f.write(str(log_message + \"\\n\"))\n",
    "                print(log_message)   \n",
    "\n",
    "        batch += 1\n",
    "        \n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(batch_loss.numpy())\n",
    "\n",
    "#     ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = []\n",
    "\n",
    "# for (img_tensor, captions, target) in tqdm(train_dataset):\n",
    "#     batch_loss = train_step(img_tensor, captions, target)\n",
    "#     loss.append(batch_loss.numpy())\n",
    "    \n",
    "# plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PPLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "# text = [\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "# ]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images, support_text=text, pplm_iteration=5, pplm_weight=1)\n",
    "# result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
