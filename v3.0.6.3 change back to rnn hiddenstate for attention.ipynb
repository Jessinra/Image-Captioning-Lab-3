{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git add \"Keras.ipynb\"\n",
    "!git commit -m \"test pred using pplm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"rnn_units\": 256,\n",
    "    \"rnn_type\": \"BiLSTM\",\n",
    "    \"tokenizer\": \"BERT\",\n",
    "    \"word_embedding\": \"BERT\",\n",
    "    \"vocab_size\": 3000,\n",
    "    \"combine_strategy\": \"merge\",\n",
    "    \"combine_layer\": \"concat\",\n",
    "    \"image_context_size\": 256,\n",
    "    \"word_embedding_dim\": 256,\n",
    "    \"batch_size\": 32,\n",
    "    \"data_size\": 5000,\n",
    "    \"use_mapping\": True,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_caption_length\": 25, # use <int> or None\n",
    "    \"image_feature_extractor\": \"xception\",\n",
    "    \"use_sequence\": True,\n",
    "    \"epoch\": 20,\n",
    "    \"use_mask\": True,\n",
    "    \"version\": \"v5 : positional embedding\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "\n",
    "# for TFBertModel\n",
    "PROXIES = {\n",
    "  \"http\": \"http://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "  \"https\": \"https://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"[CLS] \" + annot['caption'] + \" [SEP]\"\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "\n",
    "stopper = -1 if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]\n",
    "train_captions = all_captions[:stopper]\n",
    "train_img_paths = all_img_paths[:stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 5000\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = len(train_captions) if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature_extractor(model_type=\"xception\"):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.compat.v1.keras.applications.xception\n",
    "        cnn_model = tf.compat.v1.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"CNN encoder model not supported yet\")\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "def get_image_feature_shape(model_type):\n",
    "    \n",
    "    if model_type == \"xception\":\n",
    "        return (100, 2048)\n",
    "    elif model_type == \"inception_v3\":\n",
    "        return (64, 2048)\n",
    "    else:\n",
    "        raise Exception (\"model unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = extractor_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FEATURE_SHAPE = get_image_feature_shape(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extractor, extractor_preprocessor = get_image_feature_extractor(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image feature\n",
    "\n",
    "this step mostly skipped since the results are cached already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 157.25\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = PARAMS[\"batch_size\"]\n",
    "\n",
    "estimated_batch_count = DATA_SIZE / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique images\n",
    "# unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# # Prepare dataset\n",
    "# image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "# image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "# image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessed image (batch)\n",
    "\n",
    "# for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "#     # get context vector of batch images\n",
    "#     batch_features = extractor(batch_imgs)\n",
    "    \n",
    "#     # flatten 2D cnn result into 1D for RNN decoder input\n",
    "#     # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "#     # image_feature = 100 (Xception)\n",
    "#     # image_feature = 64 (Inception V3)\n",
    "#     batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "#     # Cache preprocessed image\n",
    "#     for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "#         image_path = image_path.numpy().decode(\"utf-8\")\n",
    "#         np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Caption dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0331 23:47:05.704699 139670776645440 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0331 23:47:05.706027 139670776645440 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = PARAMS[\"tokenizer\"]\n",
    "VOCAB_SIZE = PARAMS[\"vocab_size\"]  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "    \n",
    "    def use_custom_mapping(self, use_mapping=True, vocab_size=3000):\n",
    "        \n",
    "        self.use_mapping = use_mapping\n",
    "        self.cust_vocab_size = vocab_size\n",
    "        self.mapping_initialized = False\n",
    "\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into custom indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        bert_ids = [self.convert_tokens_to_ids(self.tokenize(x)) for x in tqdm(texts)]\n",
    "        \n",
    "        if not self.use_mapping:\n",
    "            return bert_ids\n",
    "        \n",
    "        if not self.mapping_initialized:\n",
    "            self._initialize_custom_mapping(bert_ids)\n",
    "            return [self._convert_bert_id_to_custom_id(x) for x in tqdm(bert_ids)]\n",
    "        \n",
    "        return bert_ids\n",
    "    \n",
    "        \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \n",
    "        bert_ids = super().convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            return self._convert_bert_id_to_custom_id(bert_ids)\n",
    "        else:\n",
    "            return bert_ids\n",
    "        \n",
    "        \n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            bert_ids = self._convert_custom_id_to_bert_id(token_ids)\n",
    "        else:\n",
    "            bert_ids = token_ids\n",
    "            \n",
    "        bert_tokens = super().convert_ids_to_tokens(bert_ids)\n",
    "        return bert_tokens\n",
    "    \n",
    "    \n",
    "    def _initialize_custom_mapping(self, corpus_bert_ids):\n",
    "        \n",
    "        print(\"    > constructing custom mapping < \\n\")\n",
    "        self._build_occurence_table(corpus_bert_ids)\n",
    "        self._build_custom_mapping_table()\n",
    "        self.mapping_initialized = True\n",
    "        \n",
    "        \n",
    "    def _build_occurence_table(self, tokenized_captions):\n",
    "        \"\"\"\n",
    "        build dict of token frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        self.occurence_table = {}\n",
    "        for caption in tqdm(tokenized_captions):\n",
    "            for token in caption:\n",
    "                if token not in self.occurence_table:\n",
    "                    self.occurence_table[token] = 0\n",
    "                self.occurence_table[token] += 1\n",
    "                \n",
    "    \n",
    "    def _build_custom_mapping_table(self):\n",
    "        \n",
    "        _special_token = ['[UNK]', '[PAD]']\n",
    "        _actual_vocab_size = self.cust_vocab_size - len(_special_token)\n",
    "        \n",
    "        sorted_occurence = {k: v for k, v in sorted(\n",
    "            self.occurence_table.items(), reverse=True, key=lambda item: item[1]\n",
    "        )}\n",
    "        \n",
    "        used_tokens = sorted(list(sorted_occurence)[:_actual_vocab_size])\n",
    "        mapping_size = min(len(used_tokens), _actual_vocab_size)\n",
    "        \n",
    "        _bert_pad = 0\n",
    "        _bert_oov = 100\n",
    "        self._custom_pad = 0\n",
    "        self._custom_oov = mapping_size + 1\n",
    "        \n",
    "        self.bert_id_to_custom_id = {\n",
    "            _bert_pad: self._custom_pad, \n",
    "            _bert_oov: self._custom_oov\n",
    "        }\n",
    "        self.custom_id_to_bert_id = {\n",
    "            self._custom_pad: _bert_pad, \n",
    "            self._custom_oov: _bert_oov\n",
    "        }\n",
    "        \n",
    "        for i in range(0, mapping_size):\n",
    "            bert_token = used_tokens[i]\n",
    "            self.bert_id_to_custom_id[bert_token] = i + 1    \n",
    "            self.custom_id_to_bert_id[i + 1] = bert_token\n",
    "            \n",
    "        print(\"Vocab contains {0} / {1} unique tokens ({2:.2f} %)\".format(\n",
    "            len(used_tokens) + 2,\\\n",
    "            len(sorted_occurence),\\\n",
    "            (len(used_tokens) / len(sorted_occurence) * 100)\n",
    "        ))\n",
    "        \n",
    "        sorted_occurence_count = list(sorted_occurence.values())\n",
    "        used_tokens_count = sum(sorted_occurence_count[:_actual_vocab_size])\n",
    "        total_tokens_count = sum(sorted_occurence_count)\n",
    "        \n",
    "        print(\"Using {0} / {1} tokens available ({2:.2f} %)\".format(\n",
    "            used_tokens_count,\\\n",
    "            total_tokens_count,\\\n",
    "            (used_tokens_count / total_tokens_count * 100)\n",
    "        ))        \n",
    "        \n",
    "    def _convert_bert_id_to_custom_id(self, token_ids):\n",
    "        return [self.bert_id_to_custom_id[x] if x in self.bert_id_to_custom_id else self._custom_oov for x in token_ids]\n",
    "    \n",
    "    def _convert_custom_id_to_bert_id(self, token_ids):\n",
    "        return [self.custom_id_to_bert_id[x] for x in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_type, use_mapping, vocab_size):\n",
    "    \n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    if tokenizer_type == \"BERT\" :\n",
    "        tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "        tokenizer.use_custom_mapping(use_mapping, vocab_size)\n",
    "\n",
    "    # use default keras tokenizer\n",
    "    else : \n",
    "        tokenizer = TokenizerWrapper(num_words=vocab_size, oov_token=\"[UNK]\")\n",
    "        tokenizer.fit_on_texts(train_captions)    \n",
    "        tokenizer.word_index['[PAD]'] = 0\n",
    "        tokenizer.index_word[0] = '[PAD]'\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0331 23:47:18.363003 139670776645440 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0331 23:47:19.713310 139670776645440 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "caption_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=False,\n",
    "    vocab_size=0\n",
    ")\n",
    "\n",
    "target_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=PARAMS[\"use_mapping\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 2695.53it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2579.59it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 216192.32it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 210650.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > constructing custom mapping < \n",
      "\n",
      "Vocab contains 3000 / 3584 unique tokens (83.65 %)\n",
      "Using 68179 / 68765 tokens available (99.15 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "caption_tokens = caption_tokenizer.texts_to_sequences(train_captions)\n",
    "target_tokens = target_tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parallel dataset\n",
    "\n",
    "prepare captions and target to support parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parallel_dataset(img_paths, caption_tokens, target_tokens):\n",
    "    \n",
    "    dataset_img_paths = []\n",
    "    dataset_captions = []\n",
    "    dataset_target = []\n",
    "    dataset_target_position = []\n",
    "\n",
    "    for i in tqdm(range(0, len(train_img_paths))):\n",
    "        img = img_paths[i]\n",
    "        cap = caption_tokens[i]\n",
    "        tar = target_tokens[i]\n",
    "\n",
    "        for j in range(1, len(cap)):\n",
    "            dataset_img_paths.append(img)\n",
    "            dataset_captions.append(cap[:j])\n",
    "            dataset_target.append(tar[j])\n",
    "            dataset_target_position.append(j)\n",
    "\n",
    "    dataset_captions = pad_sequences(dataset_captions, maxlen=PARAMS[\"max_caption_length\"], padding='post')\n",
    "    \n",
    "    return dataset_img_paths, dataset_captions, dataset_target, dataset_target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 21391.43it/s]\n"
     ]
    }
   ],
   "source": [
    "parallel_img_paths, parallel_captions, parallel_target, parallel_target_position = preprocess_parallel_dataset(train_img_paths, caption_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = parallel_captions.shape[0]\n",
    "MAX_CAPTION_LENGTH = parallel_captions.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_target_position = tf.one_hot(parallel_target_position, depth=MAX_CAPTION_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "\n",
    "def load_dataset(img_name, caption, target, target_pos):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, caption, target, target_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((parallel_img_paths, parallel_captions, parallel_target, parallel_target_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "\n",
    "dataset = dataset.map(lambda item1, item2, item3, item4: tf.numpy_function(\n",
    "          load_dataset, [item1, item2, item3, item4], [tf.float32, tf.int32, tf.int32, tf.float32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(DATA_SIZE / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 16, image_feature = 100, 2048)\n",
    "# caption => (batch_size = 16, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1395 batches, (total : 44640)\n",
      "eval : 298 batches, (total : 9536)\n",
      "test : 300 batches, (total : 9600 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captioning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=PARAMS[\"learning_rate\"])\n",
    "# loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    real  => (batch_size,)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate loss\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # create mask to filter out padding token \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    # Ignore loss_ if real token is padding\n",
    "    loss_ *= mask\n",
    "    \n",
    "    # Get mean of curren batch's loss (somewhat batch norm)\n",
    "    result_loss = tf.reduce_mean(loss_)\n",
    "    \n",
    "    return result_loss\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    loss_  => (batch_size, 1)\n",
    "    mask   => (batch_size, 1)  : indicate is padding or not\n",
    "\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim)\n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # x => (batch_size, 100, 2048)\n",
    "        x = self.fc(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, image_feature_size, image_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, img_feature_size, image_context_size)\n",
    "        hidden                        => (batch_size, embedding_size)\n",
    "        \n",
    "        note : \n",
    "        img_feature_size ==  64 for Inception V3,\n",
    "        img_feature_size == 100 for Xception,\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        _w1 = self.W1(features)\n",
    "        _w2 = self.W2(hidden_with_time_axis)\n",
    "        score = tf.nn.tanh(_w1 + _w2)\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_with_time_axis      => (batch_size, 1, embedding_size)\n",
    "        score                      => (batch_size, img_feature_size, units)\n",
    "        attention_weights          => (batch_size, img_feature_size, 1)\n",
    "        context_vector (after sum) => (batch_size, img_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rnn Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class RNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=256, \n",
    "                 embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 vocab_size=3000, batch_size=32):\n",
    "        \n",
    "        super(RNN_Encoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # =====================================================\n",
    "        \n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "        \n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "\n",
    "    def _init_embedding(self):\n",
    "        \n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.bert_embedding = TFBertModel.from_pretrained('bert-base-uncased', proxies=PROXIES)\n",
    "            self.bert_embedding.trainable = False\n",
    "            self.embedding_dim = self.bert_embedding.config.hidden_size\n",
    "            \n",
    "        else:\n",
    "            self.default_embedding = Embedding(\n",
    "                input_dim=self.vocab_size, \n",
    "                output_dim=self.embedding_dim, \n",
    "                mask_zero=True,\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def _init_rnn(self):\n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector'\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            self.bilstm = Bidirectional(LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        \n",
    "    def embedding(self, tokens, as_sentence=False):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embedding_type == \"BERT\": \n",
    "            embedding, sentence_embedding = self._bert_embedding(tokens)\n",
    "        \n",
    "        else:\n",
    "            embedding = self._default_embedding(tokens)\n",
    "            sentence_embedding = tf.reduce_mean(embedding, 1)\n",
    "            \n",
    "\n",
    "        if as_sentence:\n",
    "            # embedding => (batch_size, embedding_dim)\n",
    "            return sentence_embedding\n",
    "        \n",
    "        else:\n",
    "            # embedding => (batch_size, tokens_length, embedding_dim)\n",
    "            return embedding\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        embedding (tokens)   => (batch_size, tokens_length, embedding_dim)\n",
    "        embedding (sentence) => (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def _bert_embedding(self, x):\n",
    "        \n",
    "        token_type_ids = tf.cast((x == 0), tf.int32)\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, token_type_ids=token_type_ids)\n",
    "\n",
    "        return hidden_states, sentence_embedding\n",
    "    \n",
    "        \"\"\"\n",
    "        hidden states contains hidden state for each word\n",
    "        sentence_embedding is general embedding for whole sentences\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def _default_embedding(self, x):\n",
    "        \n",
    "        # embedding => (batch_size, sequence_length, embedding_size)\n",
    "        embedding = self.default_embedding(x)\n",
    "        return embedding\n",
    "\n",
    "    \n",
    "    def rnn_model(self, x):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            output, h_state, _ = self.lstm(x)\n",
    "            \n",
    "        elif self.rnn_type == \"BiLSTM\":\n",
    "            output, _, h_state, _, _ = self.bilstm(x)\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, h_state = self.gru(x)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, h_state\n",
    "    \n",
    "        \"\"\"\n",
    "        output => (batch_size, rnn_size)\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "    def call(self, decoder_input):\n",
    "        \"\"\" \n",
    "        decoder_input  : last predicted word => (batch_size, sequence len)\n",
    "        \"\"\"\n",
    "        \n",
    "        # x1 => (batch_size, input_sentence_len, embedding_dim + positional_dim)\n",
    "        x1 = self.embedding(decoder_input)\n",
    "        \n",
    "        # output => (batch_size, sequence_len, rnn_unit)\n",
    "        x2, rnn_state = self.rnn_model(x1) \n",
    "        output = self.dropout(x2)\n",
    "\n",
    "        return output, rnn_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU, Bidirectional, Dropout\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, units=256, combine_layer=\"concat\", vocab_size=3000, batch_size=32):\n",
    "\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.combine_layer = combine_layer  # how to use context_vector [\"add\", \"concat\"]\n",
    "\n",
    "        # =====================================================\n",
    "\n",
    "        self._init_combine_layer()\n",
    "        \n",
    "        self.leakyrelu = LeakyReLU(alpha=0.1)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.dropout = Dropout(0.2)\n",
    "\n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.units)\n",
    "        self.fc2 = Dense(self.vocab_size, activation=\"softmax\")  # same size as vocab\n",
    "\n",
    "    def _init_combine_layer(self):\n",
    "\n",
    "        if self.combine_layer == \"add\":\n",
    "            self.combine = tf.keras.layers.Add()\n",
    "\n",
    "        else:\n",
    "            self.combine = tf.keras.layers.Concatenate(axis=-1)\n",
    "\n",
    "    def _format_context_vector(self, context_vector):\n",
    "\n",
    "        context_vector = tf.expand_dims(context_vector, 1)\n",
    "        context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "    def call(self, decoder_input, context_vector):\n",
    "        \"\"\" \n",
    "        decoder_input  : \n",
    "        context_vector : image's vector      => (batch_size, img_context_size)\n",
    "        \"\"\"\n",
    "\n",
    "        context_vector = self._format_context_vector(context_vector)\n",
    "\n",
    "        # x1 (concat) => (batch_size, input_sentence_len, embedding_dim + image_context_size)\n",
    "        # x1 (add) => (batch_size, embedding_dim)\n",
    "        x1 = self.combine([context_vector, decoder_input])\n",
    "\n",
    "        # ============================================\n",
    "        # TODO: add another attention layer ?\n",
    "        # ============================================\n",
    "\n",
    "        # x2 => (batch_size, sequence_len, rnn_units)\n",
    "        x2 = self.fc1(x1)   # how important is every sequence\n",
    "        x2 = self.batchnorm(x2)\n",
    "        x2 = self.leakyrelu(x2)\n",
    "        x2 = self.dropout(x2)\n",
    "\n",
    "        # x3 => (batch_size, sequence_len * rnn_units) || was (batch_size * sequence_len=1, rnn_units)\n",
    "        x3 = tf.reshape(x2, (x2.shape[0], -1))\n",
    "\n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x3)\n",
    "\n",
    "        return word_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rnn_units': 256,\n",
       " 'rnn_type': 'BiLSTM',\n",
       " 'tokenizer': 'BERT',\n",
       " 'word_embedding': 'BERT',\n",
       " 'vocab_size': 3000,\n",
       " 'combine_strategy': 'merge',\n",
       " 'combine_layer': 'concat',\n",
       " 'image_context_size': 256,\n",
       " 'word_embedding_dim': 256,\n",
       " 'batch_size': 32,\n",
       " 'data_size': 5000,\n",
       " 'use_mapping': True,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_caption_length': 25,\n",
       " 'image_feature_extractor': 'xception',\n",
       " 'use_sequence': True,\n",
       " 'epoch': 20,\n",
       " 'use_mask': True,\n",
       " 'version': 'v5 : positional embedding'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0331 23:47:30.048012 139670776645440 configuration_utils.py:256] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/m13516112/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0331 23:47:30.049706 139670776645440 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0331 23:47:31.254603 139670776645440 modeling_tf_utils.py:333] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5 from cache at /home/m13516112/.cache/torch/transformers/d667df51ec24c20190f01fb4c20a21debc4c4fc12f7e2f5441ac0a99690e3ee9.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5\n",
      "I0331 23:47:46.944269 139670776645440 modeling_tf_utils.py:375] Layers from pretrained model not used in TFBertModel: ['mlm___cls', 'nsp___cls']\n"
     ]
    }
   ],
   "source": [
    "cnn_encoder = CNN_Encoder(\n",
    "    output_dim=PARAMS[\"image_context_size\"]\n",
    ")\n",
    "\n",
    "attention = BahdanauAttention(\n",
    "    units=PARAMS[\"rnn_units\"]\n",
    ")\n",
    "\n",
    "rnn_encoder = RNN_Encoder(\n",
    "    rnn_type=PARAMS[\"rnn_type\"], \n",
    "    rnn_units=PARAMS[\"rnn_units\"],\n",
    "    embedding_type=PARAMS[\"word_embedding\"], \n",
    "    embedding_dim=PARAMS[\"word_embedding_dim\"],  \n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    "    batch_size=PARAMS[\"batch_size\"]\n",
    ")\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    units=PARAMS[\"rnn_units\"],\n",
    "    combine_layer=PARAMS[\"combine_layer\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    "    batch_size=PARAMS[\"batch_size\"]\n",
    ")\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Requirements\n",
    "\n",
    "# combine_strategy = \"inject_init\" : IMAGE_CONTEXT_SIZE == UNITS\n",
    "# combine_strategy = \"inject_pre\"  : IMAGE_CONTEXT_SIZE == WORD_EMBEDDING_DIM\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default feed forward function\n",
    "\n",
    "@tf.function\n",
    "def feed_forward(img_tensor, caption):\n",
    "    \"\"\"\n",
    "    img_tensor => (batch_size, image_feature_size, 2048)\n",
    "    caption => (batch_size, max_caption_length)\n",
    "    \"\"\"\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    # note : used to be decoder hidden state, \n",
    "    # but changed into current word embedding for paralel training\n",
    "    \n",
    "    # hidden => (batch_size, embedding_size)\n",
    "    text_features, hidden = rnn_encoder(caption)\n",
    "\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    img_features = cnn_encoder(img_tensor)\n",
    "\n",
    "    # context_vector => (batch_size, image_context_size)\n",
    "    context_vector, _ = attention(img_features, hidden)\n",
    "\n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions = decoder(text_features, context_vector)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, caption, target):\n",
    "    \"\"\"\n",
    "    target => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training model\n",
    "    with tf.GradientTape() as gradient_tape:\n",
    "        \n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions = feed_forward(img_tensor, caption)\n",
    "        \n",
    "        # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "        loss = loss_function(target, predictions)\n",
    "\n",
    "        \n",
    "    # Apply gradient\n",
    "    trainable_variables = cnn_encoder.trainable_variables + \\\n",
    "                          rnn_encoder.trainable_variables + \\\n",
    "                          decoder.trainable_variables + \\\n",
    "                          attention.trainable_variables\n",
    "    \n",
    "    gradients = gradient_tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def eval_step(img_tensor, caption, target):\n",
    "    \n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions = feed_forward(img_tensor, caption)\n",
    "    \n",
    "    # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "    loss = loss_function(target, predictions)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def choose_predicted_id(predictions, strategy=\"max\", sampling_k=10):\n",
    "    \"\"\"\n",
    "    predictions : encoder word prediction => (batch_size, vocab_size)\n",
    "    strategy    : how to choose word [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sampling method (categorical dist)\n",
    "    if strategy == \"sample\":\n",
    "        \n",
    "        # sampled_proba & sampled_ids => (batch_size, sampling_k)\n",
    "        sampled_proba, sampled_ids = tf.math.top_k(predictions, sampling_k)\n",
    "        \n",
    "        # chosen_sampled_col => (batch_size, )\n",
    "        chosen_sampled_col = tf.squeeze(tf.random.categorical(sampled_proba, 1))\n",
    "        \n",
    "        # create row idx to zip with chosen_sampled_col\n",
    "        row_idx = tf.range(predictions.shape[0], dtype=chosen_sampled_col.dtype)\n",
    "        row_col_idx = tf.stack([row_idx, chosen_sampled_col], axis=1)\n",
    "        \n",
    "        # predicted_ids => (batch_size, )\n",
    "        predicted_ids = tf.gather_nd(sampled_ids, row_col_idx)\n",
    "\n",
    "    # Max index method\n",
    "    else:\n",
    "        predicted_ids = tf.argmax(predictions, 1)\n",
    "    \n",
    "    # predicted_ids => (batch_size, )\n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(images_paths):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [load_image(x)[0] for x in images_paths]\n",
    "    \n",
    "    # x => (batch_size, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, 10, 10, 2048)\n",
    "    x = extractor(x)\n",
    "    \n",
    "    # x  => (batch_size, img_feature_size, 2048)\n",
    "    x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = cnn_encoder(x)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def get_supporting_features(images_paths, strategy=\"mean\"):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, img_count, 1)\n",
    "    strategy : strategy to aggregate multiple supporting image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [[load_image(x)[0] for x in images_set] for images_set in images_paths]\n",
    "    \n",
    "    # x => (batch_size, img_count, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, img_count, 10, 10, 2048)\n",
    "    x = [extractor(image_set) for image_set in x]\n",
    "    \n",
    "    # features => (batch_size, img_count, img_feature_size, image_context_size)\n",
    "    features = cnn_encoder(x)\n",
    "    \n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    if strategy == \"logsumexp\":\n",
    "        features = tf.reduce_logsumexp(features, 1)\n",
    "    elif strategy == \"max\":\n",
    "        features = tf.reduce_max(features, 1)\n",
    "    elif strategy == \"min\":\n",
    "        features = tf.reduce_min(features, 1)\n",
    "    else:\n",
    "        features = tf.reduce_mean(features, 1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_indices(support_text):\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(0, len(support_text)):\n",
    "        context_token = target_tokenizer.tokenize(support_text[i])\n",
    "        context_token_id = target_tokenizer.convert_tokens_to_ids(context_token)\n",
    "        context_token_id = set(context_token_id)\n",
    "        context_token_id.discard(0)\n",
    "        for x in sorted(context_token_id):\n",
    "            indices.append([i, x])\n",
    "\n",
    "    # return => (word_count, 2)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_supporting_text_vector(support_text, vocab_size):\n",
    "    \"\"\"\n",
    "    support_text : list of text describing main image context => (batch_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(support_text)\n",
    "    \n",
    "    # indices => ( sum(batch_size * ?word_count), 2)\n",
    "    indices = get_one_hot_indices(support_text)\n",
    "    values = tf.ones(len(indices))\n",
    "    sparse_one_hot = tf.sparse.SparseTensor(indices, values, dense_shape=[batch_size, vocab_size])\n",
    "    \n",
    "    # sparse_one_hot => (batch_size, vocab_size)\n",
    "    return sparse_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluate(images_paths,\n",
    "                    support_text=None,\n",
    "                    support_imgs=None, \n",
    "                    support_aggregate_strategy=\"mean\",\n",
    "                    pplm_iteration=3,\n",
    "                    pplm_weight=0.03,\n",
    "                    pplm_gm_weight=0.8,\n",
    "                    choose_word_strategy=\"sample\",\n",
    "                   ):\n",
    "    \n",
    "    \"\"\"\n",
    "    images_paths : list of image_path                           => (batch_size, 1)\n",
    "    support_text : list of text describing main image context   => (batch_size)\n",
    "    support_imgs : list of list of image_path                   => (batch_size, image_count, 1)\n",
    "    support_aggregate_strategy : how to aggregate support image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    pplm_iteration : number of pplm step done for every decoding step\n",
    "    pplm_weight    : weight of pplm loss\n",
    "    pplm_gm_weight : geometric mean fusion weight (0 means use only original prediction, 1 means use only pplm prediction)\n",
    "    choose_word_strategy : how to choose word from prediction distribution [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(images_paths)\n",
    "    \n",
    "    # initialize captions placeholder\n",
    "    start_token = caption_tokenizer.convert_tokens_to_ids(['[CLS]']) # use bert id's not custom id\n",
    "    result_captions = tf.tile(tf.expand_dims(start_token, 1), [batch_size, 1])\n",
    "    attention_plot = tf.reshape([], shape=(batch_size, 0, IMAGE_FEATURE_SHAPE[0]))\n",
    "    \n",
    "    # Extract features from main images\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = get_image_features(images_paths)\n",
    "    \n",
    "    \n",
    "    if support_text is not None:\n",
    "        # support_text_vector => (batch_size, vocab_size)\n",
    "        support_text_vector = get_supporting_text_vector(support_text, decoder.vocab_size) \n",
    "    else:\n",
    "        # set all pplm related variable to 0\n",
    "        pplm_iteration = 0\n",
    "        pplm_weight = 0\n",
    "        pplm_gm_weight = 0\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(MAX_CAPTION_LENGTH)):\n",
    "        \n",
    "        # decoder_input => (batch_size, ~MAX_CAPTION_LENGTH)\n",
    "        caption = pad_sequences(result_captions, maxlen=MAX_CAPTION_LENGTH, padding=\"post\")\n",
    "        \n",
    "        # hidden => (batch_size, embedding_size)\n",
    "        text_features, hidden = rnn_encoder(caption)\n",
    "\n",
    "        # context_vector => (batch_size, image_context_size)\n",
    "        # attention_weights => (batch_size, img_feature_size, 1)\n",
    "        context_vector, attention_weights = attention(features, hidden)\n",
    "\n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions = decoder(text_features, context_vector)  \n",
    "        \n",
    "        \n",
    "        # ======================== PPLM section ========================\n",
    "        ori_prediction = predictions\n",
    "        \n",
    "        curr_pertubation = tf.Variable(tf.zeros((batch_size, rnn_encoder.embedding_dim)), name=\"curr_pertubation\", trainable=True)\n",
    "        \n",
    "        \n",
    "        for j in range(pplm_iteration):\n",
    "            \n",
    "            with tf.GradientTape() as pplm_tape: \n",
    "                \n",
    "                hidden += curr_pertubation\n",
    "                \n",
    "                context_vector, attention_weights = attention(features, hidden)\n",
    "                predictions = decoder(text_features, context_vector)  \n",
    "                pplm_loss = pplm_loss_function(support_text_vector, predictions, pplm_weight=pplm_weight)\n",
    "                \n",
    "            \"\"\"\n",
    "            most impactfull layer to train = last dense layer\n",
    "            \"\"\"\n",
    "            \n",
    "            trainable_variables = [curr_pertubation]\n",
    "            gradients = pplm_tape.gradient(pplm_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "            \n",
    "            predictions = decoder(text_features, context_vector)  \n",
    "        \n",
    "        # fuse final pplm_prediction and original prediction\n",
    "        fused_predictions = (predictions * pplm_gm_weight) + (ori_prediction * (1 - pplm_gm_weight)) \n",
    "        \n",
    "        # predicted_ids => (batch_size,)\n",
    "        predicted_ids = choose_predicted_id(fused_predictions, strategy=choose_word_strategy)\n",
    "        \n",
    "        # convert custom id mapping to bert's id\n",
    "        # predicted_bert_ids => (batch_size, 1)\n",
    "        predicted_bert_ids = target_tokenizer._convert_custom_id_to_bert_id(predicted_ids.numpy())\n",
    "        predicted_bert_ids = tf.expand_dims(predicted_bert_ids, 1)\n",
    "        \n",
    "        # store result\n",
    "        result_captions = tf.concat([result_captions, predicted_bert_ids], axis=1)\n",
    "        \n",
    "        # attention_weights => (batch_size, 1, img_feature_size)\n",
    "        attention_weights = tf.reshape(attention_weights, shape=(batch_size, 1, -1))\n",
    "        \n",
    "        # assign attention weights to respective generated word\n",
    "        # attention_plot => (batch_size, ~max_caption_len, feature_size)\n",
    "        attention_plot = tf.concat([attention_plot, attention_weights], axis=1)\n",
    "        \n",
    "    # remove start token & revert to tokens\n",
    "    result_captions = [caption_tokenizer.convert_ids_to_tokens(x[1:]) for x in result_captions]\n",
    "    \n",
    "    return result_captions, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy, MeanSquaredError\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def pplm_loss_function(real, pred, pplm_weight=0.03):\n",
    "    \"\"\"\n",
    "    real  => (batch_size, vocab_size)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "\n",
    "#     mm = tf.sparse.sparse_dense_matmul(real, tf.transpose(pred))\n",
    "#     mm = tf.reduce_sum(tf.abs(mm), 1)\n",
    "#     loss = tf.reduce_sum(mm, 0)\n",
    "    \n",
    "    real = tf.sparse.to_dense(real, default_value=0)\n",
    "    pplm_loss = CategoricalCrossentropy(from_logits=True)\n",
    "    loss = pplm_loss(real, pred, pplm_weight)\n",
    "    \n",
    "#     print(loss)\n",
    "    return loss\n",
    "\n",
    "    \"\"\"\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1395it [03:06,  7.49it/s]\n",
      "1395it [03:27,  6.72it/s]\n",
      "1395it [03:06,  7.47it/s]\n",
      "1395it [02:41,  8.62it/s]\n",
      "1395it [02:53,  8.03it/s]\n",
      "1395it [02:56,  7.90it/s]\n",
      "1395it [02:56,  7.89it/s]\n",
      "1395it [02:45,  8.45it/s]\n",
      "1395it [02:53,  8.06it/s]\n",
      "1395it [02:55,  7.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f04279d21d0>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    for (img_tensor, captions, target, _) in tqdm(train_dataset):\n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss.append(batch_loss.numpy())\n",
    "        \n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0427964160>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5110449254363775"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-500:]) / len(loss[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6694110459327698"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss[-5000:]) / len(loss[-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:02<00:00, 192942.52it/s]\n"
     ]
    }
   ],
   "source": [
    "true_captions = {}\n",
    "for annot in tqdm(annotations[\"annotations\"]):\n",
    "    image_id = annot[\"image_id\"]\n",
    "\n",
    "    if image_id not in true_captions:\n",
    "        true_captions[image_id] = []\n",
    "    \n",
    "    true_captions[image_id].append(annot['caption'])\n",
    "    \n",
    "    \n",
    "def get_true_captions(image_paths):\n",
    "    return [true_captions[int(x[-10:-4])] for x in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:17<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "white wall is sitting and on in the room . sitting flying . on out\n",
      "\n",
      "true caps : \n",
      "A bathroom with a toilet and sink, \n",
      "View of a white toilet next to a vanity with a red sink.\n",
      "A white remote and sink in a room.\n",
      "A white toilet sitting next to a bathroom sink.\n",
      "A bathroom with a toilet and a red sink.\n",
      "=====================================\n",
      "predicted : \n",
      "mirror room in whites of playing room with room .\n",
      "\n",
      "true caps : \n",
      "A small bathroom with sink, mirror, toilet and tub\n",
      "A white sink sitting under a bathroom mirror.\n",
      "A bathroom mirror and sink in a bathroom \n",
      "this bathroom is all beige and has a white sink\n",
      "A mirror reflection above the sink shows the toilet and bathtub.\n",
      "=====================================\n",
      "predicted : \n",
      "a station in train through through - up to it\n",
      "\n",
      "true caps : \n",
      "a yellow and grey passenger train parked next to a platform\n",
      "A train traveling down a set of tracks near a train station.\n",
      "Train arriving or departing at an empty station.\n",
      "A train is coming down the tracks near a platform.\n",
      "A subway train driving through a subway station. \n",
      "=====================================\n",
      "predicted : \n",
      "there two many people a small woman room .\n",
      "\n",
      "true caps : \n",
      "A group of people sitting at the table smiling. \n",
      "A group of elderly people having dinner at a restaurant. \n",
      "a number of older people seated around a table\n",
      "A group of people are seated at a restaurant dining table.\n",
      "A group of people sitting around a restaurant table.\n",
      "=====================================\n",
      "predicted : \n",
      "a man wearing jackets trying up a big bed to road to in some field of field sidewalk microphone to in black lot cars walking\n",
      "\n",
      "true caps : \n",
      "Two men standing close together examining some red wine.\n",
      "One man holding a wine glass with some wine while his friend looks on\n",
      "Two men looking up while toasting with wine glasses. \n",
      "A couple of men standing next to each other holding wine glasses.\n",
      "Two people are posing for the camera with their win glasses. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[50:55]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images)\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:16<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "predicted : \n",
      "a man that is standing on the back to the horse and the man\n",
      "\n",
      "true caps : \n",
      "some people are standing near some public toilets\n",
      "A group of people are standing out in front of portable toilets.\n",
      "people are standing around some porta potties and mess\n",
      "a couple of people stand in front of some toilets \n",
      "groups of people standing around the toilet area\n",
      "=====================================\n",
      "predicted : \n",
      "a small baby with a man standing holding the food .\n",
      "\n",
      "true caps : \n",
      "A person is holding onto a cellphone somewhere.\n",
      "there is someone holding a small phone with a small screen\n",
      "A picture of a cell phone showing the time as 8:56 PM.\n",
      "a hand is holding a black and silver cellphone\n",
      "This is a verizon LG Envy 2 cell phone.\n",
      "=====================================\n",
      "predicted : \n",
      "there is a room computers lot of people\n",
      "\n",
      "true caps : \n",
      "People working on laptop computers in a student dining room\n",
      "People sitting in a room at tables with laptops \n",
      "A group of people that are sitting in front of laptops.\n",
      "Many people have signed up for the computer seminar.\n",
      "A group of people at tables with laptops having conversation.\n",
      "=====================================\n",
      "predicted : \n",
      "a bird filled with lots in the stands with the sheep\n",
      "\n",
      "true caps : \n",
      "two birds near one another in a field \n",
      "Two black colored birds sitting in a field together. \n",
      "Two birds that are sitting on the ground.\n",
      "2 birds standing on the ground in a field \n",
      "Two birds that are standing next to each other on the grass. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = all_img_paths[5:9]\n",
    "\n",
    "result, attention_plot = custom_evaluate(images, choose_word_strategy=\"max\")\n",
    "truth_caption = get_true_captions(images)\n",
    "\n",
    "\n",
    "for x, y in zip(result, truth_caption):\n",
    "    print(\"=====================================\") \n",
    "    stop_idx = x.index('[SEP]') if '[SEP]' in x else None\n",
    "    pred = \" \".join(x[:stop_idx])\n",
    "    pred = pred.replace(\" ##\", \"\")\n",
    "    print(\"predicted : \\n{}\".format(pred))\n",
    "    print()\n",
    "    print(\"true caps : \\n{}\".format(\"\\n\".join(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "stop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-5981adc29e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: stop"
     ]
    }
   ],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train/{}\".format(str(datetime.now())[:-10])\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           attention=attention,\n",
    "                           optimizer=optimizer\n",
    "                          )\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------\")\n",
    "print(checkpoint_path)\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(checkpoint_path)\n",
    "with open(checkpoint_path + \"/config.txt\", \"w\") as f:\n",
    "    f.write(str(PARAMS))\n",
    "    \n",
    "log_file = open(checkpoint_path + \"/log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, PARAMS[\"epoch\"]):\n",
    "    \n",
    "    start = time.time()\n",
    "    loss = 0\n",
    "    batch = 1\n",
    "\n",
    "    for img_tensor, captions, target in tqdm(train_dataset):\n",
    "        \n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "\n",
    "            with open(checkpoint_path + \"/log.txt\", \"a\") as f:\n",
    "                log_message = ' {} Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                    checkpoint_path, epoch + 1, batch, batch_loss.numpy())\n",
    "                f.write(str(log_message + \"\\n\"))\n",
    "                print(log_message)   \n",
    "\n",
    "        batch += 1\n",
    "        \n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(batch_loss.numpy())\n",
    "\n",
    "#     ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = []\n",
    "\n",
    "# for (img_tensor, captions, target) in tqdm(train_dataset):\n",
    "#     batch_loss = train_step(img_tensor, captions, target)\n",
    "#     loss.append(batch_loss.numpy())\n",
    "    \n",
    "# plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PPLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "# text = [\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "# ]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images, support_text=text, pplm_iteration=5, pplm_weight=1)\n",
    "# result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
